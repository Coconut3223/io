<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Subword Segmentation &#8212; HomePage</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/def.css?v=5a9d86bd" />
    <script src="../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=6dbb43f8"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#NLP/subword" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="HomePage"
           class="md-header-nav__button md-logo">
          
            <i class="md-icon">&#xe869</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Cocobook</span>
          <span class="md-header-nav__topic"> Subword Segmentation </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">HomePage</a></li>
            
            <li class="md-tabs__item"><a href="../AI/index.html" class="md-tabs__link">AI</a></li>
            
            <li class="md-tabs__item"><a href="../python/index.html" class="md-tabs__link">Python</a></li>
            
            <li class="md-tabs__item"><a href="index.html" class="md-tabs__link">NLP</a></li>
            
            <li class="md-tabs__item"><a href="../SQL/main.html" class="md-tabs__link">SQL</a></li>
            
            <li class="md-tabs__item"><a href="../utils/index.html" class="md-tabs__link">Utils</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="HomePage" class="md-nav__button md-logo">
      
        <i class="md-icon">&#xe869</i>
      
    </a>
    <a href="../index.html"
       title="HomePage">Cocobook</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#nlp-subword--page-root" class="md-nav__link">Subword Segmentation</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#subword-segmentations-with-language-model" class="md-nav__link">Subword segmentations with language model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#byte-pair-encoding-bpe" class="md-nav__link">Byte-Pair-Encoding BPE</a>
        </li>
        <li class="md-nav__item"><a href="#unigram-language-model" class="md-nav__link">Unigram language model</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#questions" class="md-nav__link">Questions</a>
        </li>
        <li class="md-nav__item"><a href="#subword-regularization-improving-neural-network-translation-models-with-multiple-subword-candidates" class="md-nav__link">[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates]</a>
        </li>
        <li class="md-nav__item"><a href="#neural-machine-translation-of-rare-words-with-subword-units" class="md-nav__link">[Neural Machine Translation of Rare Words with Subword Units]</a>
        </li>
        <li class="md-nav__item"><a href="#neural-machine-translation-with-byte-level-subwords" class="md-nav__link">[Neural Machine Translation with Byte-Level Subwords]</a>
        </li>
        <li class="md-nav__item"><a href="#bbpe" class="md-nav__link">BBPE</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#encoding" class="md-nav__link">Encoding</a>
        </li>
        <li class="md-nav__item"><a href="#decoding" class="md-nav__link">decoding</a>
        </li>
        <li class="md-nav__item"><a href="#experiment" class="md-nav__link">Experiment</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="subword-segmentation">
<h1 id="nlp-subword--page-root">Subword Segmentation<a class="headerlink" href="#nlp-subword--page-root" title="Link to this heading">¶</a></h1>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-column sd-col-4 sd-col-xs-4 sd-col-sm-4 sd-col-md-4 sd-col-lg-4 docutils">
<div class="line-block">
<div class="line"><span class="defi">vocabulary size</span> 字典的大小</div>
<div class="line"><span class="defi">step size</span> 去编码一个句子需要的 tokens数量。the number of tokens required to encode the sentence。 挂钩  decoding efficiency</div>
</div>
</div>
<div class="sd-col sd-d-flex-column sd-col-8 sd-col-xs-8 sd-col-sm-8 sd-col-md-8 sd-col-lg-8 docutils">
<div class="admonition-example admonition hint">
<p class="admonition-title">Example</p>
<table>
<thead>
<tr class="row-odd"><th class="head"><p>size</p></th>
<th class="head"><p>vocabulary</p></th>
<th class="head"><p>step size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>6</p></td>
<td><p>今/天/吃/啥/至/不</p></td>
<td><p>4 &amp; 4</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>今天/吃/啥/至今/不</p></td>
<td><p>3 &amp; 3</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>今天/吃/啥/至今/不吃</p></td>
<td><p>3 &amp; 2</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>only with a small fixed size of vocabulary (<strong>usually 16k to 32k</strong>), the number of required symbols to encode a sentence will not significantly increase, which is an important feature for an efficient decoding.</p>
<div class="admonition-open-vocabulary-issue admonition danger">
<p class="admonition-title">open vocabulary issue</p>
<div class="line-block">
<div class="line">NLP 以前是使用 fixed word vocabularies, 训练和推断都很依赖 vocabulary size. 但是 limiting vocabulary size 会新增  <code class="docutils literal notranslate"><span class="pre">UNK</span></code>  的数量。导致一些场景下（an open vocabulary setting）（翻译、具有高效构词过程）表现不好。</div>
<div class="line">比起把这些高复合词编码成一个定长向量，更倾向编码成一个可分割的不定长的向量。</div>
</div>
</div>
<p>✏️ Break up rare words into subword units 把出现较少的词分割成更小的子词。</p>
<p>Neural Machine Translation with multiple subword segmentations</p>
<p><span class="defi">NMT training with on-the-fly 即时 subword sampling</span></p>
<div class="line-block">
<div class="line">Given a source sentence  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span>  and a target sentence  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span> , let  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>M</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = (x_1,...,x_M), y = (y_1,...,y_N)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>  be the corresponding subword sequences segmented with an underlying subword segmenter, e.g., BPE.</div>
<div class="line">NMT models the translation probability  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P (Y |X ) = P (y|x)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>  as a target language sequence model that generates target subword  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>  conditioning on the target history  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{&lt;n}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2274em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span></span></span></span></span>  and source input sequence  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span> :</div>
</div>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">y</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>n</mi></msub><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>n</mi></mrow></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P (\bold{y}|\bold{x};θ) = \prod_{i=1}^NP (y_n|\bold{x}, y_{&lt;n}; θ)

</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathbf" style="margin-right:0.01597em;">y</span><span class="mord">∣</span><span class="mord mathbf">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2274em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div><div class="line-block">
<div class="line">A common choice to predict the subword  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>  is to use a RNN architecture. However, note that subword regularization is not specific to this architecture and can be applicable to other NMT architectures without RNN, e.g., (Vaswani et al., 2017; Gehring et al., 2017).</div>
<div class="line">NMT is trained using the standard maximum likelihood estimation, i.e., maximizing the log- likelihood L(θ) of a given parallel corpus D =</div>
</div>
<p>Strictly speaking, wordpiece model (Schuster and Naka- jima, 2012) is different from BPE. We consider wordpiece as a variant of BPE, as it also uses an incremental vocabulary generation with a different loss function.</p>
<p>Wordpiece model uses a likelihood instead of frequency.</p>
<section id="subword-segmentations-with-language-model">
<h2 id="subword-segmentations-with-language-model">Subword segmentations with language model<a class="headerlink" href="#subword-segmentations-with-language-model" title="Link to this heading">¶</a></h2>
<table>
<thead>
<tr class="row-odd"><th class="head"><p>?</p></th>
<th class="head"><p>BPE</p></th>
<th class="head"><p>unigram</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>encoder</p></td>
<td><p>dictionary encoder</p></td>
<td><p>entropy encoder</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>frequency</p></td>
<td><p>probabilistic model</p></td>
</tr>
<tr class="row-even"><td><p>multiple segmentations</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
<div class="admonition-bpe-unigram-same admonition note">
<p class="admonition-title">BPE &amp; unigram 【SAME】</p>
<p>the same idea that they encode a text using fewer bits with a certain data compression principle (dictionary vs. entropy).</p>
</div>
<section id="byte-pair-encoding-bpe">
<h3 id="byte-pair-encoding-bpe">Byte-Pair-Encoding BPE<a class="headerlink" href="#byte-pair-encoding-bpe" title="Link to this heading">¶</a></h3>
<p>[Neural Machine Translation of Rare Words with Subword Units]</p>
<div class="line-block">
<div class="line">原始是 data compression literature。</div>
<div class="line">BPE is a variant of dictionary (substitution) encoder that incrementally finds a set of symbols such that the total number of symbols for encoding the text is minimized.</div>
</div>
<p><strong>Steps:</strong> 首先切成单个字符，再不断合并最频繁的，字典 size 一直变少到想要的 size。</p>
<ol class="arabic simple">
<li><p>splits the whole sentence into individual characters</p></li>
<li><p>The most <strong>frequent</strong> adjacent pairs of characters are then consecutively merged until reaching a desired vocabulary size</p></li>
</ol>
<p><strong>advantage：</strong></p>
<ul class="simple">
<li><p>有效平衡 vocabulary size &amp; step size (the number of tokens required to encode the sentence)</p></li>
<li><p>合并组成子词只和子词的频率相关  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em;"></span></span></span></span></span>  常见的词会被保留成一个单独的 symbol = 拥有自己的 ID。不常见的词会被分解成更小的单元（子字符串｜字符）。</p></li>
</ul>
<p><strong>disadvantage：</strong></p>
<ul class="simple">
<li><p>基于贪婪原则和确定的符号替代 deterministic symbol replacement  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em;"></span></span></span></span></span>  不能根据概率提供多重分割。</p></li>
</ul>
<p>[Neural Machine Translation of Rare Words with Subword Units]: <a class="reference external" href="https://arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p>
</section>
<section id="unigram-language-model">
<h3 id="unigram-language-model">Unigram language model<a class="headerlink" href="#unigram-language-model" title="Link to this heading">¶</a></h3>
<p>On the other hand, the unigram language model is reformulated as an entropy encoder that minimizes the total code length for the text.</p>
</section>
</section>
<section id="questions">
<h2 id="questions">Questions<a class="headerlink" href="#questions" title="Link to this heading">¶</a></h2>
<div class="admonition-multiple-segmentation-candidates admonition danger">
<p class="admonition-title">multiple segmentation candidates</p>
<div class="line-block">
<div class="line">即使是同样的字典，还是有不一样的分割表示方式。is a spurious ambiguity</div>
<div class="line">However, a sentence can be represented in multiple subword sequences even with the same vocabulary.</div>
</div>
<div class="admonition-multiple-subword-sequences-encoding-the-same-sentence-hello-world admonition hint">
<p class="admonition-title"><strong>Multiple subword sequences encoding the same sentence “Hello World”</strong></p>
<div class="line-block">
<div class="line">Subwords( meansspaces)  Vocabulary id sequence</div>
<div class="line">Hell/o/ world 13586 137 255</div>
<div class="line">H/ello/ world 320 7363 255</div>
<div class="line">He/llo/ world 579 10115 255</div>
<div class="line">/He/l/l/o/ world 7 18085 356 356 137 255</div>
<div class="line">H/el/l/o/ /world 320 585 356 137 7 12295</div>
</div>
</div>
</div>
</section>
<section id="subword-regularization-improving-neural-network-translation-models-with-multiple-subword-candidates">
<h2 id="subword-regularization-improving-neural-network-translation-models-with-multiple-subword-candidates">[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates]<a class="headerlink" href="#subword-regularization-improving-neural-network-translation-models-with-multiple-subword-candidates" title="Link to this heading">¶</a></h2>
<div class="admonition-abstract admonition hint">
<p class="admonition-title">Abstract</p>
<p><span class="defi">Subword units</span> are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, <span class="defi">subword segmentation</span> is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. <strong>The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, ==subword regularization==, which trains the model with multiple subword segmentations probabilistically sampled during training.</strong> In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.
把子词分割当作噪声进行优化，从而提高鲁棒性。</p>
</div>
<p>Subword regularization consists of the following two sub-contributions:</p>
<ul class="simple">
<li><dl class="simple">
<dt>A simple NMT training algorithm to integrate multiple segmentation candidates. Our approach is implemented as an on-the-fly data sampling, which is not specific to NMT architecture. Subword regularization can be applied to any NMT system without changing the model structure.</dt><dd><p>我们提出了一种简单的 NMT 训练算法来集成多个分割候选者。我们的方法被实现为动态数据采样，这不是 NMT 架构所特有的。子字正则化可以应用于任何NMT系统，而不改变模型结构。</p>
</dd>
</dl>
</li>
<li><p>A new subword segmentation algorithm based on a language model, which provides multiple segmentations with probabilities. The language model allows to emulate the noise generated during the segmentation of actual data.</p></li>
</ul>
<div class="line-block">
<div class="line">A common choice to predict the subword  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>  is to use a RNN architecture. However, note that subword regularization is not specific to RNN and can be applicable to other NMT architectures without RNN.</div>
<div class="line">NMT is trained using the standard maximum likelihood estimation, i.e., maximizing the log-likelihood L(θ) of a given parallel corpus D =</div>
</div>
<p>In this paper, we propose a new subword segmentation algorithm based on a unigram language model, which is capable of outputing multiple sub-word segmentations with probabilities. The unigram language model makes an assumption that each subword occurs independently, and consequently, the probability of a subword sequence</p>
<p>[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates]: <a class="reference external" href="https://arxiv.org/pdf/1804.10959.pdf">https://arxiv.org/pdf/1804.10959.pdf</a></p>
</section>
<section id="neural-machine-translation-of-rare-words-with-subword-units">
<h2 id="neural-machine-translation-of-rare-words-with-subword-units">[Neural Machine Translation of Rare Words with Subword Units]<a class="headerlink" href="#neural-machine-translation-of-rare-words-with-subword-units" title="Link to this heading">¶</a></h2>
<div class="admonition-abstract admonition hint">
<p class="admonition-title">Abstract</p>
<p>Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, <strong>making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of ==subword units==</strong>. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration ), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). 這是基於這樣的直覺：各種詞類都可以透過比單字更小的單位進行翻譯，例如名稱（透過字元複製或音譯轉寫）、化合物（透過組合翻譯）以及同源詞和外來詞（透過語音和形態轉換）。<strong>We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the :defi:`byte pair encoding` compression algorithm</strong>, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.</p>
</div>
<p>我们的主要目标是在 NMT 网络本身中对开放词汇翻译进行建模，而不需要稀有词的回退模型，前者更有效。Our main goal is to model open-vocabulary translation in the NMT network itself, without requiring a back-off model for rare words.</p>
<p>Byte Pair Encoding (BPE) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.</p>
<p>Firstly, we initialize the symbol vocabulary with the character vocabulary, and represent each word as a sequence of characters, plus a special end-of-word symbol ‘·’, which allows us to restore the original tokenization after translation. We iteratively count all symbol pairs and replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span><span class="o">,</span> <span class="nn">collections</span>

<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
    <span class="k">return</span> <span class="n">pairs</span>

<span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
    <span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">'(?&lt;!\S)'</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">'(?!\S)'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
        <span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">v_out</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'l o w &lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">'l o w e r &lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">'n e w e s t &lt;/w&gt;'</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span>
        <span class="s1">'w i d e s t &lt;/w&gt;'</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>
<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">pairs</span><span class="p">[</span><span class="n">best</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="go">('e', 's')</span>
<span class="go">('es', 't')</span>
<span class="go">('est', '&lt;/w&gt;')</span>
<span class="go">('l', 'o')</span>
<span class="go">('lo', 'w')</span>
<span class="go">('n', 'e')</span>
<span class="go">('ne', 'w')</span>
<span class="go">('new', 'est&lt;/w&gt;')</span>
<span class="go">('low', '&lt;/w&gt;')</span>
<span class="go">('w', 'i')</span>
<span class="go">vocab = {'low&lt;/w&gt;': 5, 'low e r &lt;/w&gt;': 2, 'newest&lt;/w&gt;': 6, 'wi d est&lt;/w&gt;': 3}</span>
<span class="go">pairs = {('low', 'e'): 2, ('e', 'r'): 2, ('r', '&lt;/w&gt;'): 2, ('w', 'i'): 3,</span>
<span class="go">('i', 'd'): 3, ('d', 'est&lt;/w&gt;'): 3}</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Each merge operation pro- duces a new symbol which represents a charac- ter n-gram. Frequent character n-grams (or whole words) are eventually merged into a single sym- bol, thus BPE requires no shortlist. The final sym- bol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations – the latter is the only hyperparameter of the algorithm.</div>
<div class="line">For efficiency, we do not consider pairs that cross word boundaries. The algorithm can thus be run on the dictionary extracted from a text, with each word being weighted by its frequency. A minimal Python implementation is shown in Al-</div>
</div>
</section>
<section id="neural-machine-translation-with-byte-level-subwords">
<h2 id="neural-machine-translation-with-byte-level-subwords">[Neural Machine Translation with Byte-Level Subwords]<a class="headerlink" href="#neural-machine-translation-with-byte-level-subwords" title="Link to this heading">¶</a></h2>
<div class="admonition-abstract admonition hint">
<p class="admonition-title">Abstract</p>
<div class="line-block">
<div class="line">Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words.</div>
<div class="line">Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness.</div>
<div class="line">Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice.</div>
<div class="line">In this paper, we <strong>investigate byte-level subwords, specifically ==byte-level BPE (BBPE)==, which is co**mpacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that **contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer</strong>.</div>
<div class="line">Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.</div>
</div>
<ul class="simple">
<li><p>character-level: 稀少的会占用词典大小，会导致OOV，limit compactness</p></li>
<li><p>byte-level: 高计算成本</p></li>
<li><p>byte-level subword：需要用 CNN｜RNN 来 contextualize BBPE embedding。</p></li>
</ul>
</div>
<p>[Neural Machine Translation with Byte-Level Subwords]:<a class="reference external" href="https://arxiv.org/abs/1909.03341">https://arxiv.org/abs/1909.03341</a></p>
</section>
<section id="bbpe">
<h2 id="bbpe">BBPE<a class="headerlink" href="#bbpe" title="Link to this heading">¶</a></h2>
<div class="admonition-abstract admonition hint">
<p class="admonition-title">Abstract</p>
<div class="line-block">
<div class="line">Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words.</div>
<div class="line">Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily <strong>take up vocabulary slots and limit its compactness</strong>.</div>
<div class="line">Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. <strong>High computational cost</strong> has however prevented it from being widely deployed or used in practice.</div>
<div class="line">In this paper, we investigate byte-level subwords, specifically <strong>==byte-level BPE (BBPE)==, which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is.</strong></div>
<div class="line"><strong>We claim that :defi:`contextualizing BBPE embeddings` is necessary, which can be implemented by a convolutional or recurrent layer.</strong> Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE.</div>
<div class="line">In the multilingual setting, BBPE maximizes <strong>vocabulary sharing</strong> across many languages and achieves better translation quality. Moreover, we show that BBPE enables <strong>transferring models between languages</strong> with non-overlapping character sets.</div>
</div>
</div>
<div class="line-block">
<div class="line">比 character-level 更 compacter, no out-of-vocabulary</div>
<div class="line">比 byte-level 更 efficient, smaller</div>
<div class="line">在 multi-lingual 上 能 vocabulary sharing &amp; transferring models between languages</div>
</div>
<p>原本：data compression = 》 &lt;kbd&gt;byte&lt;/kbd&gt; + &lt;kbd&gt;subword&lt;/kbd&gt;</p>
<section id="encoding">
<h3 id="encoding">Encoding<a class="headerlink" href="#encoding" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>UTF-8 encoding</p></li>
<li><p>learn (B)BPE vocabularies jointly on source and target sentences using SentencePiece</p></li>
</ul>
<div class="admonition-utf-8-encoding admonition note">
<p class="admonition-title">UTF-8 encoding</p>
<div class="line-block">
<div class="line">encodes each Unicode character into 1 to 4 bytes</div>
<div class="line">represent a sentence in any language as a sequence of UTF-8 bytes (248 out of 256 possible bytes).</div>
</div>
<img alt="../_images/utf8.jpg" src="../_images/utf8.jpg"/>
<p>The design of UTF-8 encoding ensures the uniqueness of this recovery process: for a character UTF-8 encoded with multiple bytes, its trailing bytes will not make a valid UTF-8 encoded character.</p>
<p>[搞搞字节，byte的小知识](<a class="reference external" href="https://zhuanlan.zhihu.com/p/449954688">https://zhuanlan.zhihu.com/p/449954688</a>)</p>
</div>
<div class="line-block">
<div class="line">BBPE symbols can be partial characters shared by different characters or the combination of complete and partial</div>
<div class="line">characters. This arbitrariness may necessitate incorporating</div>
<div class="line">a larger context surrounding each symbol for disambiguation and learning the character boundaries.</div>
</div>
<div class="admonition-defi-contextualized-dynamic-word-embedding admonition note">
<p class="admonition-title"><span class="defi">Contextualized（Dynamic）Word Embedding</span></p>
<p>在很多的NLP工作里面，一个单词可以表示成很多种意思（即一词多义），如何处理一词多义、考虑单词在上下文中的意思
[From Static Embedding to Contextualized Embedding](<a class="reference external" href="https://zhuanlan.zhihu.com/p/147938963">https://zhuanlan.zhihu.com/p/147938963</a>)</p>
</div>
<p>We propose to use either <strong>a depth-wise convolutional layer or a bidirectional recurrent layer with gated recurrent units</strong> to contextualize BBPE embeddings before feeding them into the model</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>c</mi><mi>t</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>e</mi><mi>m</mi><mi>b</mi></mrow></msub><mo>=</mo><mtext>DepthWiseConv</mtext><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>e</mi><mi>m</mi><mi>b</mi></mrow></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><msub><mi>x</mi><mrow><mi>c</mi><mi>t</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>e</mi><mi>m</mi><mi>b</mi></mrow></msub><mo>=</mo><mtext>BiGRU</mtext><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>e</mi><mi>m</mi><mi>b</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{ctx\_emb}=\text{DepthWiseConv}(X_{emb})\\
x_{ctx\_emb}=\text{BiGRU}(X_{emb})</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7976em;vertical-align:-0.367em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">x</span><span class="mord mtight" style="margin-right:0.02778em;">_</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">mb</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.367em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">DepthWiseConv</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">mb</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.7976em;vertical-align:-0.367em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">x</span><span class="mord mtight" style="margin-right:0.02778em;">_</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">mb</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.367em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">BiGRU</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">mb</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></section>
<section id="decoding">
<h3 id="decoding">decoding<a class="headerlink" href="#decoding" title="Link to this heading">¶</a></h3>
<p>Empirically, we find that invalid outputs from trained models are very rare.</p>
<div class="line-block">
<div class="line">And a common error pattern in halftrained models is redundant repeating bytes. In our system,</div>
<div class="line">we try to recover as many Unicode characters as possible</div>
<div class="line">from this error pattern efficiently in linear time.</div>
</div>
<p>The design of UTF-8 encoding ensures the uniqueness of this recovery process: for a character UTF-8 encoded with multiple bytes, its trailing bytes will not make a valid UTF-8 encoded character. Then the best selection in Eq. 1 is unique and so is the final solution.</p>
</section>
<section id="experiment">
<h3 id="experiment">Experiment<a class="headerlink" href="#experiment" title="Link to this heading">¶</a></h3>
<p>learn (B)BPE vocabularies jointly on source and target sentences using SentencePiece</p>
<img alt="../_images/NMT_1.png" src="../_images/NMT_1.png"/>
<div class="line-block">
<div class="line">learning rate schedule</div>
</div>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">/Users/yeyehaoye/gitproject/io/docs/source/NLP/subword.rst</span>, line 284)</p>
<p>Line block ends without a blank line.</p>
</aside>
<p><a href="#id1"><span class="problematic" id="id2">|</span></a>set attention and ReLU dropout to 0.1
| use 0.2 residual dropout for Tbase models in X-En
| use a kernel size of 5 and a padding of 2 on both sides for all convolutional layers.</p>
<aside class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">/Users/yeyehaoye/gitproject/io/docs/source/NLP/subword.rst</span>, line 284); <em><a href="#id2">backlink</a></em></p>
<p>Inline substitution_reference start-string without end-string.</p>
</aside>
<p>Inference and Evaluation</p>
<div class="line-block">
<div class="line">set beam width to 4 for EnDe and 5 for the other and</div>
<div class="line">use the best checkpoint by <strong>validation loss</strong> to generate the predictions.</div>
<div class="line">We calculate casesensitive tokenized BLEU (Papineni et al. 2002) as the metrics using <strong>sacreBLEU</strong> (Post 2018).</div>
</div>
</section>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>