# Transformer

## BERT related

**BERT**， **B**idirectional **E**ncoder **R**epresentation from **T**ransformer

### original

.. note:: ""
    | **Transformer** SA +
    | **Bidirectional** 完形填空 MLM

    .. table::

        +-----------+--------+
        |BERT       |v.s.    |
        +===========+========+
        |MLM，      |gpt，L2R|
        +-----------+--------+
        |Transformer|RNN     |
        +-----------+--------+

- advangtages

    BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]()
