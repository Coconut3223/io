<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Recurrent Neural Networks &#8212; cocobook  文档</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/def.css?v=5a9d86bd" />
    <script src="../../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/translations.js?v=beaddf03"></script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#AI/DL/rnn" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../index.html" title="cocobook  文档"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">cocobook  文档</span>
          <span class="md-header-nav__topic"> Recurrent Neural Networks </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../../"versions.json"",
        target_loc = "../../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../../index.html" class="md-tabs__link">cocobook  文档</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../index.html" title="cocobook 文档" class="md-nav__button md-logo">
      
        <img src="../../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../../index.html"
       title="cocobook 文档">cocobook  文档</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#ai-dl-rnn--page-root" class="md-nav__link">Recurrent Neural Networks</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#standard-rnn" class="md-nav__link">standard RNN</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#shortcoming" class="md-nav__link">shortcoming</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#lstm-long-short-term-memory-network" class="md-nav__link">LSTM Long Short-Term Memory Network</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#forget-gate" class="md-nav__link">forget gate 遗忘门</a>
        </li>
        <li class="md-nav__item"><a href="#input-gate" class="md-nav__link">input gate 输入门</a>
        </li>
        <li class="md-nav__item"><a href="#output-gate" class="md-nav__link">output gate 输出门</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#gnu-gated-recurrent-unit-gru" class="md-nav__link">GNU Gated Recurrent Unit-GRU</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#reset-gate" class="md-nav__link">Reset Gate</a>
        </li>
        <li class="md-nav__item"><a href="#update-gate" class="md-nav__link">Update Gate</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#ref" class="md-nav__link">Ref</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../../_sources/AI/DL/rnn.rst.txt">显示源代码</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="recurrent-neural-networks">
<h1 id="ai-dl-rnn--page-root">Recurrent Neural Networks<a class="headerlink" href="#ai-dl-rnn--page-root" title="Link to this heading">¶</a></h1>
<section id="standard-rnn">
<h2 id="standard-rnn">standard RNN<a class="headerlink" href="#standard-rnn" title="Link to this heading">¶</a></h2>
<p>&lt;kbd&gt;Variable-length Sequene&lt;/kbd&gt; &lt;kbd&gt;Sequential-Dependent&lt;/kbd&gt; &lt;kbd&gt;Time-Dependent&lt;/kbd&gt;</p>
<p>==recurrent==: <strong>revisit or reuse past states as inputs</strong> to predict the next or future states.</p>
<p><span class="defi">Memory</span> 拥有前一个阶段的输出作为未来输出的灵感的一部分。</p>
<p>!!! p “The biggest difference between RNN and traditional neural networks is that each time the previous output is taken to the next hidden layer and trained together.”</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_6.gif)&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_1.gif)&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;最后只要 o5 进行解码&lt;/p&gt;
&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_3.jpg){width=70%}&lt;p&gt;x1 进去产生 h1，h1 同时存到 m1, 此时 o1 完全是 h1 的解码&lt;br&gt; 为了统一，也可以自定义一个 m0 同样和 x1 进去产生 h1&lt;/p&gt;&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_4.webp){width=70%}&lt;p&gt;x2 &amp; m1 进去产生 h2, h2 同时被存到 m12， 此时 o2 完全是 h2 的解码&lt;/p&gt;&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_5.jpg){width=70%}&lt;p&gt;x3 &amp; m12 进去产生 h3, h3 同时被存到 m123， 此时 o3 完全是 h3 的解码 &lt;/p&gt;&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_6.jpg){width=70%}&lt;p&gt;memory (m1,m12,…) 并不是直接raw input 进去产生新一轮的 hidden state (h2,h3,…), 而是采用一个&lt;b&gt;权值矩阵 W 参数化&lt;/b&gt;记忆单元&lt;/p&gt;&lt;/figure&gt;
&lt;/div&gt;</p>
<p>$$s_t=text{tanh}(Ux_t+Ws_{t-1})$$</p>
<section id="shortcoming">
<h3 id="shortcoming">shortcoming<a class="headerlink" href="#shortcoming" title="Link to this heading">¶</a></h3>
<p>RNN 比较擅长解决 想要的信息 &amp; 相关的信息 距离较近。</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_8.png)&lt;p&gt;Good&lt;/p&gt;&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_9.png)&lt;p&gt;Bad&lt;/p&gt;&lt;/figure&gt;
&lt;/div&gt;</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_7.jpg)&lt;/figure&gt;
&lt;p&gt;short-term memory has a large impact (such as the orange region), but long-term memory effects are small (such as black and green regions), which is the &lt;b&gt;short-term memory problem&lt;/b&gt; of RNN.&lt;/p&gt;
&lt;/div&gt;
如果序列足夠長，它們將很難將資訊從較早的時間步驟傳遞到較晚的時間步驟。因此，如果您嘗試處理一段文字來進行預測，RNN 可能會從一開始就遺漏重要資訊。
1. RNN has short-term memory problems and cannot handle very long input sequences
2. Training RNN requires significant cost</p>
<p>![](./pics/RNN_2.jpg)</p>
<ul class="simple">
<li><p>&lt;u&gt;unfold&lt;/u&gt;</p></li>
<li><p><strong>数据是按照顺序进入</strong>，我们在处理序列化的数据时，往往会在用**滑动窗口**的办法来调整不同的结构。</p></li>
<li><p>m0 的设置：这个初始值可以作为一个参数进行反向传播，也可以将其简单的设置为零，表示前面没有任何信息。</p></li>
</ul>
<dl class="simple">
<dt>!!! p “在同一层的隐藏单元中进行传播的 权值矩阵W &amp; 输入到隐层的权值矩阵U &amp; 隐层到输出的权值矩阵V，为什么是相同的?”</dt><dd><p>就像是 CNN 用参数共享的卷积核来提取相同的特征，在RNN中，使用参数共享的 U,V 来**确保相同的输入产生的输出是一样**。参数共享的矩阵W <strong>确保了对于相同的上文，产生相同的下文</strong>。
&gt; 一段文本中，可能会出现大量的“小狗”。无论小狗出现在哪个位置（x?），参数共享使得神经网络在输入“小狗”的时候，在不考虑上下文的 memory，$xxrightarrow{完全编码}h$ 的结果是一样。类似地，在不考虑当前输入 x，$mxrightarrow{完全编码}h$ 的结果是一样。</p>
</dd>
</dl>
</section>
</section>
<section id="lstm-long-short-term-memory-network">
<h2 id="lstm-long-short-term-memory-network">LSTM Long Short-Term Memory Network<a class="headerlink" href="#lstm-long-short-term-memory-network" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>!!! p “motivation”</dt><dd><p>To solve short-term memory of RNN, LSTM can retain “important information” in longer sequence data, ignoring less important information.
LSTMs were designed to combat vanishing gradients through a gating mechanism.</p>
</dd>
</dl>
<p>![](./pics/LSTM_1.png){width=70%}</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/RNN_3.png)&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/LSTM_2.png)&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;All recurrent neural networks have chain repeating modules of neural networks. &lt;br&gt;&lt;mark&gt;standard RNN&lt;/mark&gt;: repeating module has a very simple structure, such as only a single tanh layer.&lt;br&gt;&lt;mark&gt;LSTM&lt;/mark&gt;: Not a single neural network layer, but four, and interacting in a very special way.&lt;/p&gt;</p>
<p><strong>pre-knowledge：</strong></p>
<ul class="simple">
<li><p><cite>tanh</cite>
tanh activation 用於幫助調節流經網絡的值。 tanh 函數將值壓縮為始終在 -1 和 1 之間。</p></li>
<li><p><cite>Sigmoid</cite> $sigma$</p></li>
</ul>
<p>&lt;div class=”grid” style=”grid-template-columns: repeat(3, 1fr) !important;” markdown&gt;
&lt;figure markdown=”span” style=”grid-column-start: 1; grid-column-end: 3;”&gt;![](./pics/LSTM_3.webp){width=500px}&lt;/figure&gt;
&lt;p style=”grid-column-start: 3; grid-column-end: 4;”&gt;圆形：Neuial Network layer，一层神经网络，也就是$w^Tx*b$的操作。区别在于使用的激活函数不同&lt;br&gt;这里存在 sigmoid（$sigma(w^Tx*b)$） &amp; tanh（$text{tanh}(w^Tx*b)$） 两种激活函数&lt;br&gt;方块：Matrix operation 矩阵操作，并且是 &lt;mark&gt;pointwise&lt;/mark&gt; 逐元素操作&lt;br&gt; verctor concatenation: 矩阵拼接，变成 $(H_1+H_2)*W$ &lt;/p&gt;
&lt;/div&gt;</p>
<dl>
<dt>!!! p “hidden state $h_t$ &amp; cell state $c_t$”</dt><dd><p>相比于原始的 RNN 的 hidden state， LSTM 增加了一个细胞状态 cell state。 尽管 LSTM 中的 cell state &amp; hidden output 都包含有关 LSTM 模型的信息，但它们的角色不同&lt;br&gt;
<span class="defi">hidden state</span> 可以被看作是**当前时刻**的 LSTM 的“理解”或“编码”信息，可以被传递到下一层的 LSTM 或者用于预测任务.&lt;br&gt;
<span class="defi">cell state</span> 是用于存储**先前的信息**和计算新的信息，一直在上面传递, internal memory</p>
</dd>
<dt>!!! p “为什么 LSTM 能携带 long-term memory？又是如何只记住重点的 memory？”</dt><dd><p>能携带 long-term memory 因为多了一个 cell state 細胞狀態有點像傳送帶。 它直接沿著整個鏈條執行，只有一些輕微的線性相互作用。 資訊很容易不變地沿著它流動，携带整个文本的信息
如何记住重点，则是与 <span class="defi">门 gate</span> 息息相关。
- 如何 忘记不重要的 &amp; 记住重要的。</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/DL/rnn.rst</span>, line 92)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p>$f_t=sigma(cdot)in[0,1]$, 任何值✖️0 都是0，任何值✖️1都是它本身，所以当信息被乘以0，那么就会被忘记，如果乘以1，就会记住。$f_t$ 的数值其实就是遗忘程度。</p>
</div></blockquote>
</dd>
</dl>
<p>&lt;div class=”grid” style=”grid-template-columns: repeat(5, 1fr) !important;” markdown&gt;
&lt;figure markdown=”span” style=”grid-column-start: 1; grid-column-end: 4;”&gt;![](./pics/LSTM_4.webp){width=80%}&lt;p&gt;at time t&lt;/p&gt;&lt;/figure&gt;
&lt;p style=”grid-column-start: 4; grid-column-end: 6;”&gt;Input:&lt;br&gt;– $C_{t-1}$ cell state at t-1&lt;br&gt;– $h_{t-1}$ hidden state at t-1&lt;br&gt;– $x_t$ 输入的向量 at t &lt;br&gt;Output:&lt;br&gt;– $C_{t}$&lt;br&gt;– $h_{t}$&lt;br&gt;细胞状态 $c_{t-1}$ 一直在上面的线传递，$h_t &amp; x_{t}$ 会对 $c$ 进行修改，输出 $c_t$ &lt;br&gt;$c_t$ 会参与 $h_t$ 的计算。&lt;br&gt; 其中的计算过程透过 &lt;mark&gt;门 gate&lt;/mark&gt; 来实现。&lt;/p&gt;4
&lt;/div&gt;</p>
<dl class="simple">
<dt>!!! p “size of vector”</dt><dd><p>dim(c) = dim(h)&lt;br&gt; shape of cell state = shape of hidden state
&gt; 以 遗忘门 举例：
&gt; $f_t^{d1times d2}:=sigma(W_f^{d1times(d1+d3)}[h_{t-1}, x_t]^{(d1+d3)times d2}+b_f)$
&gt; $c_{t-1}^{d1times d2}underline{text{pointwise multiplfy}}f_t^{d1times d2}$
&gt;
![](./pics/LSTM_5.webp){width=50%}</p>
</dd>
</dl>
<p><span class="defi">forget gate</span>  决定：用当前的判断 $f$：要记得多少过去的信息 $c_{t-1}$ &lt;br&gt;
<span class="defi">input gate</span> 决定：用当前的判断 $i$：要加入多少当前的信息 $tilde{c}_{t}$ &lt;br&gt;
<span class="defi">output gate</span> 决定：用迄今为止的判断 $o$：要向外面或者未来暴露多少信息 $c_t$ 。defines how much of the internal state you want to expose to the external network (higher layers and the next time step).&lt;br&gt;</p>
<section id="forget-gate">
<h3 id="forget-gate">forget gate 遗忘门<a class="headerlink" href="#forget-gate" title="Link to this heading">¶</a></h3>
<p>当前的信息 $[h_{t-1}, x_t]$  决定 过去的信息 $c_{t-1}$ 要忘记多少</p>
<p>$$f_t=sigma(W_f[h_{t-1}, x_t]+b_f)$$</p>
<p>![](./pics/LSTM_6.gif){width=80%}</p>
</section>
<section id="input-gate">
<h3 id="input-gate">input gate 输入门<a class="headerlink" href="#input-gate" title="Link to this heading">¶</a></h3>
<p>将当前的信息 $[h_{t-1}, x_t]$ 更新到过去的信息 $c$ 里：不仅要处理要流入的值，还要决定哪些值是重要的，所以 $[h_{t-1}, x_t]$ 同时经过 Sigmoid &amp; tanh。The sigmoid output will decide which information is important to keep from the tanh output.</p>
<p>$$i_t = sigma(W_i[h_{t-1}, x_t]+b_i)\tilde{c}_t=text{tanh}(W_c[h_{t-1}, x_t]+b_c)$$</p>
<p>![](./pics/LSTM_7.gif){width=80%}</p>
<p>然后进行 cell state 的 update：昨日的信息 $ f_t * c_{t-1}$ 有需要遗忘的，今天的信息 $i_t * tilde{c}_t $ 也同样有需要遗忘的</p>
<p>$$c_t=f_t*c_{t-1}+i_t* tilde{c}_t$$</p>
<p>![](./pics/LSTM_8.gif){width=80%}</p>
</section>
<section id="output-gate">
<h3 id="output-gate">output gate 输出门<a class="headerlink" href="#output-gate" title="Link to this heading">¶</a></h3>
<p>cell state $c_t$ 已更新，要过一遍 tanh 传递给下一轮的 hidden state &amp; output for prediction，同时 当前信息 $[h_{t-1}, x_t]$ 要过 sigmoid 决定新的 $c_t$ 里有哪些是需要遗忘的。</p>
<p>$$o_t=sigma(W_o[h_{t-1}, x_t]+b_o)\h_t=o_t*text{tanh}(c_t)$$</p>
<p>![](./pics/LSTM_9.gif){width=80%}</p>
<p>![](./pics/LSTM_10.png){width=80%}</p>
<p>t 时刻的 hidden state $h_t$ 既作为 hidden state 继续向前流动，又作为 t时刻的输出，来进行解码和完成任务。</p>
</section>
</section>
<section id="gnu-gated-recurrent-unit-gru">
<h2 id="gnu-gated-recurrent-unit-gru">GNU Gated Recurrent Unit-GRU<a class="headerlink" href="#gnu-gated-recurrent-unit-gru" title="Link to this heading">¶</a></h2>
<p>a variant of LSTM. He retains the characteristics of LSTM to focus and forget unimportant information, and it will not be lost during long-term propagation.</p>
<p>GRU 将 LSTM 的 forget gate &amp; input gate 整合到一个单独的 update gate， 还把 cell state 和 hidden state 合并成一个 hidden state，还有其他的一些小 changes。</p>
<p>&lt;figure markdown=”span”&gt;![](./pics/GRU_1.png)&lt;p&gt;&lt;/p&gt; GRU 只用 hidden state &amp; input，而且 only 2 gates: reset gate &amp; update gate&lt;/p&gt;&lt;/figure&gt;</p>
<dl class="simple">
<dt>!!! p “更少的 tensor operation 更快的训练速度。”</dt><dd><p>!!! danger “但性能上谁更好，不确定，还是要真正 train 之后才知道。”</p>
</dd>
</dl>
<p>![](./pics/GRU_2.webp){width=60%}</p>
<p>![](./pics/GRU_3.png)</p>
<p><span class="defi">reset gate</span> determines how to combine the new input with the previous memory&lt;br&gt;
<span class="defi">update gate</span> defines how much of the previous memory to keep around.</p>
<p>简单来说，把 reset 的参数都变成 1， update 的参数 都变成0，就是 standard RNN。</p>
<section id="reset-gate">
<h3 id="reset-gate">Reset Gate<a class="headerlink" href="#reset-gate" title="Link to this heading">¶</a></h3>
<p>决定忘记哪些过去信息</p>
</section>
<section id="update-gate">
<h3 id="update-gate">Update Gate<a class="headerlink" href="#update-gate" title="Link to this heading">¶</a></h3>
<p>把 LSTM 的 forget gate &amp; input gate 融了进来。&lt;br&gt;
what information to throw away and what new information to add.</p>
</section>
</section>
<section id="ref">
<h2 id="ref">Ref<a class="headerlink" href="#ref" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>[如何理解RNN？（理论篇）]</p></li>
<li><p>[Long short-term memory network-Long short-term memory | LSTM]</p></li>
<li><p>[Illustrated Guide to LSTM’s and GRU’s: A step by step explanation]</p></li>
<li><p>[大名鼎鼎的LSTM详解]</p></li>
<li><p>[图解LSTM实现cell state 和hidden state和output]</p></li>
<li><p>[Understanding LSTM Networks]</p></li>
<li><p>[Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU and LSTM RNN with Python and Theano]</p></li>
</ul>
<p>[如何理解RNN？（理论篇）]:<a class="reference external" href="https://easyai.tech/blog/rnn-understand/">https://easyai.tech/blog/rnn-understand/</a>
[Long short-term memory network-Long short-term memory | LSTM]:<a class="reference external" href="https://www.easyai.tech/en/ai-definition/lstm/">https://www.easyai.tech/en/ai-definition/lstm/</a>
[Illustrated Guide to LSTM’s and GRU’s: A step by step explanation]:<a class="reference external" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>
[大名鼎鼎的LSTM详解]: <a class="reference external" href="https://zhuanlan.zhihu.com/p/518848475">https://zhuanlan.zhihu.com/p/518848475</a>
[图解LSTM实现cell state 和hidden state和output]:<a class="reference external" href="https://blog.csdn.net/u010087338/article/details/129805575">https://blog.csdn.net/u010087338/article/details/129805575</a>
[Understanding LSTM Networks]:<a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
[Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU and LSTM RNN with Python and Theano]:<a class="reference external" href="https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/">https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/</a></p>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>