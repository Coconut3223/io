<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>LRs &#8212; HomePage</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/def.css?v=5a9d86bd" />
    <script src="../../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/translations.js?v=beaddf03"></script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#AI/ML/lr" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../index.html" title="HomePage"
           class="md-header-nav__button md-logo">
          
            <i class="md-icon">&#xe869</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Cocobook</span>
          <span class="md-header-nav__topic"> LRs </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../../"versions.json"",
        target_loc = "../../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../../index.html" class="md-tabs__link">HomePage</a></li>
            
            <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">AI</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../index.html" title="HomePage" class="md-nav__button md-logo">
      
        <i class="md-icon">&#xe869</i>
      
    </a>
    <a href="../../index.html"
       title="HomePage">Cocobook</a>
  </label>
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">AI</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../eva.html" class="md-nav__link">模型评估与选择</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../highd.html" class="md-nav__link">high dimentional DA</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../loss.html" class="md-nav__link">Loss</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../pre_1.html" class="md-nav__link">预处理</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#ai-ml-lr--page-root" class="md-nav__link">LRs</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#problem-setting" class="md-nav__link">Problem Setting</a>
        </li>
        <li class="md-nav__item"><a href="#linear-regression" class="md-nav__link">Linear Regression</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#graphical" class="md-nav__link">Graphical</a>
        </li>
        <li class="md-nav__item"><a href="#inference-of-lr-model" class="md-nav__link">inference of LR model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#confidence-interval-for-the-regression-function" class="md-nav__link">Confidence interval for the regression function</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#id1" class="md-nav__link">广义线性模型</a>
        </li>
        <li class="md-nav__item"><a href="#regularization" class="md-nav__link">正则化 Regularization</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#ridge-regression-l2-penalty" class="md-nav__link">Ridge Regression  - L2 penalty</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#background" class="md-nav__link">Background</a>
        </li>
        <li class="md-nav__item"><a href="#content" class="md-nav__link">Content</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#the-shrinkage-parameter-ridge" class="md-nav__link">λ := the shrinkage parameter - ridge</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#lasso-l1-penalty-least-absolute-shrinkage-and-selection-operator" class="md-nav__link">Lasso - L1 penalty, Least Absolute Shrinkage and Selection Operator</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#the-shrinkage-parameter-lasso" class="md-nav__link">λ := the shrinkage parameter -lasso</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#elastic-net-a-combination-of-lasso-and-ridge-regression" class="md-nav__link">elastic net: A combination of Lasso and Ridge regression</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#comparison" class="md-nav__link">Comparison</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#lse-lasso-ridge" class="md-nav__link">LSE & Lasso & ridge</a>
        </li>
        <li class="md-nav__item"><a href="#lasso-ridge" class="md-nav__link">Lasso & ridge</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#solution-path" class="md-nav__link">solution path</a>
        </li></ul>
            </nav>
        </li>
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="lrs">
<h1 id="ai-ml-lr--page-root">LRs<a class="headerlink" href="#ai-ml-lr--page-root" title="Link to this heading">¶</a></h1>
<p>==多重回归==.包含了多个变量的回归</p>
<section id="problem-setting">
<h2 id="problem-setting">Problem Setting<a class="headerlink" href="#problem-setting" title="Link to this heading">¶</a></h2>
<p>$$y =β_0 +β_1x_1 +…+β_px_p +ε=red{beta^Tx}+epsilon\quadbegin{cases}x_1,dots,x_p:&amp;text{Predictors, }red{text{independent}}text{ variable, covariates}\Y:&amp;text{Response, dependent variable
}\epsilon:&amp;text{noises, error}end{cases}$$</p>
<p>$$begin{cases}Y_1=beta_0+beta_1x_{1,1}+dots+beta_{p}x_{1,p}+epsilon_1\Y_2=beta_0+beta_1x_{2,1}+dots+beta_{p}x_{2,p}+epsilon_2\vdots\Y_n=beta_0+beta_1x_{n,1}+dots+beta_{p}x_{n,p}+epsilon_nend{cases}\iffbegin{bmatrix}Y_1\vdots\Y_nend{bmatrix}=begin{bmatrix}1&amp;x_{1,1}&amp;dots&amp;x_{1,p}\vdots\1&amp;x_{n,1}&amp;dots&amp;x_{n,p}end{bmatrix}begin{bmatrix}beta_0\beta_1\vdots\beta_pend{bmatrix}+begin{bmatrix}epsilon_1\vdots\epsilon_nend{bmatrix}\=begin{bmatrix}x_1^T&amp;dots&amp; x_n^T\end{bmatrix}beta+Xi,x_i=begin{bmatrix}1\x_{i1}\vdots\x_{ip}end{bmatrix},beta=begin{bmatrix}beta_0\beta_1\vdots\beta_pend{bmatrix},Xi=begin{bmatrix}epsilon_1\vdots\epsilon_nend{bmatrix}$$</p>
<p>$$iff Y=red{Xbeta}+Xi,YinR^n,XinR^{ntimes(p+1)},betainR^{p+1},XiinR^n$$</p>
<p><strong>Assumptions</strong> (for statistical inference)</p>
<ul class="simple">
<li><p><strong>predictor $x_i$ is usually assumed to be non-random.</strong></p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 19)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>已经被观察到了，所以是一个值，”the independent variable is an observed value (so its value is known)”，我们研究的是对一个已知的x，y的条件分布 Y｜X
- 关于误差的 assumption：</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 21)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><ul>
<li><p>$red{ε_1,dots,epsilon_n}$ <strong>are independent</strong></p></li>
<li><dl>
<dt>$red{Eε_1 =Eε_2 =…=Eε_n =0iff EXiinR^n=0}$</dt><dd><ul class="simple">
<li><p>the model has to be identifiable.</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 24)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>如果没有这个假设，那么不同的模型之间只是存在一个常数 C 级别上的不同，并没有本质区别，但是在在讨论的时候就容易存在差异，感觉有点像是规定了规定，才能更好的基于统一背景下讨论。</p>
</dd>
</dl>
</li>
<li><p>$red{Var(ε_1)=…=Var(ε_n)=σ^2iff Var(Xi)=sigma^2IinR^n}$</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 26)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>这个是不一定的，在heterogeneous数据里，这个没怎么听懂</p>
<dl class="simple">
<dt>!!! warning “期望和方差做 assumption 不等同于 指定分布 $mathbb{E}(epsilon)=0 + Var(epsilon)=sigma^2neqepsilon ~Ν(0,sigma)$”</dt><dd><p>有些书是assume $ε_1,ε_2,..ε_nin~N(0, σ), i.i.d$,是否指定分布取决于我们想干什么
线性回归残差满足正态分布只是一种假设，目的是使得后续对估计量的推断 inference 能够利用正态分布的性质。？因为在满足正态假设的前提下，hypothesis testing &amp;confidential interval 比较容易
做OLS并得出回归系数的无偏估计这一过程本身而言，并不需要分布假设，也就是说，即使Y的取值为 0、1（binary），我们仍然可以做OLS线性回归，得出回归系数的无偏估计，但此时，残差显然不服从正态分布，因此也就不能对回归系数做 wald test。</p>
</dd>
</dl>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 32)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<ul class="simple">
<li><p>$red{text{centered X}inR^{ntimes(p+1)}, mathbb{E}x=0,Var(X)=SigmainR^{(p+1)times (p+1)}}$</p></li>
<li><p>$red{mathbb{E}Y =0}$</p></li>
</ul>
<p>$$β^∗ =min_beta frac{1}{2}red{mathbb{E}}(Y − x^Tβ)^2, colorbox{aqua}{text{(Convex in β)}}$$</p>
<p><strong>常规思路：</strong>
$cfrac{partial}{partialbeta}f(beta)=mathbb{E}big(x(Y-x^Tbeta)big)xlongequal{SET}0\[1em]implies beta^*=mathbb{E}(x^Tx)^{-1}mathbb{E}(xY)=Sigma^{-1}Cov(x,Y)=Var(x)^{-1}Cov(x,Y)$</p>
</section>
<section id="linear-regression">
<h2 id="linear-regression">Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">¶</a></h2>
<p>$$y = w^Tx+b$$</p>
<p>!!! p “estimate parameter using ==Least Squares Estimation, LSE==”</p>
<p>$$
begin{align*}hat{beta}_{LSE}&amp;=minlimits_{beta}sumlimits_{i=1}^n(Y_i-hat{Y_i})^2\&amp;=minlimits_betasumlimits_{i=1}^n{Y_i-(beta_0+beta_1x_{i1}+dots+beta_px_{ip})}^2\
&amp;xlongequal{X(text{population level})}minlimits_betasumlimits_{i=1}^n{Y_i-X_i^Tbeta}^2\
&amp;xlongequal{mathbb X(text{empirical level})}minlimits_beta(Y-mathbb Xbeta)^T(Y-mathbb Xbeta)end{align*}\YinR^n,mathbb XinR^{ntimes(p+1)},betainR^{p+1}
$$</p>
<p>$$
cfrac{partial}{partialbeta}(Y-mathbb Xbeta)^T(Y-mathbb Xbeta)=mathbb X^T(Y-mathbb Xbeta)xlongequal{SET}0\[1em]hat{beta}_{LSE}=(mathbb X^Tmathbb X)^{-1}mathbb X^TY
$$</p>
<dl class="simple">
<dt>!!! p “如果 $mathbb X$ 有被 centered，那么用 LSE 对参数进行估计，<strong>本质上其实也是对样本方差 variance 和 correlation between features and response 的估计，这又回归到在高维空间里去. estimate is difficult</strong>”</dt><dd><p>$$hat{beta}_{LSE}=(mathbb X^Tmathbb X)^{-1}mathbb X^TY=(cfrac{mathbb X^Tmathbb X}{n})^{-1}cdot cfrac{mathbb X^T Y}{n}=Var(mathbb X)cdot Cov(mathbb X, Y)$$</p>
</dd>
</dl>
<p><strong>requirements:</strong></p>
<ul class="simple">
<li><p>X必须列满秩，$r(X)=p+1,n≥p+1$</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 63)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>$r(mathbb X)=r(mathbb X^Tmathbb X)=p+1 iff (mathbb X^Tmathbb X)^{-1} text{exists}$</p>
<p><strong>properties:</strong></p>
<ul class="simple">
<li><p>$hat{beta}_{LSE}$ is unbiased $mathbb E(hatbeta_{LSE})=beta$</p></li>
<li><p>$Var(hatbeta_{LSE})=sigma^2(mathbb X^Tmathbb X)^{-1}$</p></li>
<li><p>$hat{Y}$  is a linear function of Y.  $iff hat{Y}_{LSE}=Xhat{beta}_{LSE}=X(X^TX)^{-1}X^TY$.</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 70)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>从这角度来卡，LR 就是使用了所有的observations
- <span class="defi">residual sum of squares, RSS</span> $=sumlimits_{i=1}^n{Y_i-hat{Y}_i}^2$ RSS表示拟合模型后Y的变化仍然不能被X的变化解释的部分 $hat{sigma}^2=cfrac{RSS}{n-p-1}$</p>
<section id="graphical">
<h3 id="graphical">Graphical<a class="headerlink" href="#graphical" title="Link to this heading">¶</a></h3>
<p>Suppose $mathbb{X}^Tmathbb{X}$ is <strong>inverible</strong>.
$hat{Y}=mathbb{X}hat{beta}implies$ $hat{Y}$ is an <span class="defi">orthogonal projection image</span> of $hat{Y}$ on the image of $mathbb{X}$.
$hat{epsilon}=Y-hat{Y}$</p>
</section>
<section id="inference-of-lr-model">
<h3 id="inference-of-lr-model">inference of LR model<a class="headerlink" href="#inference-of-lr-model" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>!!! warning “更强的 assumption: $Xi～N(0,sigma^2I)(text{i.e. correct model got})$”</dt><dd><p>指定了 $Xi$ 的分布，我们才能在此基础上推 distribution of $hatbeta_{LSE}$ , 才能进行 hypothesis testing. 以下由此进行推断：</p>
</dd>
</dl>
<p>这里存在一点狭义：这里的 $beta$ 是从包括1，也就是$betainR^{p+1}$, 为了更对称和完整，就直接用对应的分量 k对k来看，也就是说 $hatbeta_0～N(1, c_{0,0}sigma^2)$, start form 0.</p>
<p>$$
begin{align*}&amp;hat{beta}_{LSE}～N(beta,(mathbb X^Tmathbb X)^{-1}sigma^2)&amp;tag{vector}\&amp;hatbeta_k～N(beta_k,c_{k,k}sigma^2)&amp;tag{分量}\&amp;hatbeta_k-beta_k～N(0, c_{k,k}sigma^2)end{align*}
$$</p>
<p>$qquadbegin{cases}c_{k, k}:=text{ the  (k,k) entry of } (mathbb X^T mathbb X)^{-1}\sqrt{c_{k,k}hat{sigma}^2}:=text{ the standard error of }beta_k\t_k=cfrac{hat{beta_k}}{sqrt{c_{k,k}hat{sigma}^2}}:=text{ the t-statistic for } beta_k\P(<a href="#id12"><span class="problematic" id="id13">|T|</span></a>&gt;|t_k|):=text{ the p-value for }beta_k,T～t(n-p-1)（自由度）end{cases}$</p>
<ul class="simple">
<li><p>$Xi～N(0,sigma^2I)(text{i.e. correct model got})implies hat{beta}～N(beta,c_{k+1,k+1}sigma^1)$</p></li>
</ul>
<p>&gt; At significant level 0.05 (when n is large), $H_0=beta_k=0$</p>
<p>![](./pics/LRs_1.png){width=60%}</p>
<section id="confidence-interval-for-the-regression-function">
<h4 id="confidence-interval-for-the-regression-function">Confidence interval for the regression function<a class="headerlink" href="#confidence-interval-for-the-regression-function" title="Link to this heading">¶</a></h4>
<p><strong>【Uncentralized】</strong> For a subject with predictor $X = (1, x_1, …, x_p)^T$and response Y. fitted linear regression model $Y= β_0 +β_1x_1 +…+β_px_p+ ε$. the regression $mathbb{E}Y=β_0 +β_1x_1 +…+β_px_p$ is a function of $X=(1,x_1,dots,x_p)^T$</p>
<p>The estimator of $mathbb{E}Y$ is $mathbb{E}hat{Y}=hat β_0+hat β_1x_1+…+hat β_px_p=mathbb X^That β$</p>
<p>$$
mathbb{E}Y～NBig(mathbb{E}hat{Y},sigma^2mathbb X^T(mathbb X^Tmathbb X)^{-1}mathbb XBig)
$$</p>
<p>The 95% confidence interval (CI) for EY is
$Big[mathbb{E}hat{Y}-1.96sigmasqrt{ X^T(mathbb X^Tmathbb X)^{-1} X}, mathbb{E}hat{Y}+1.96sigmasqrt{X^T(mathbb X^Tmathbb X)^{-1}X}Big]$
or approximately
$Big[mathbb{E}hat{Y}-1.96hatsigmasqrt{ X^T(mathbb X^Tmathbb X)^{-1}X}, mathbb{E}hat{Y}+1.96hatsigmasqrt{ X^T(mathbb X^Tmathbb X)^{-1} X}Big]$</p>
<p><strong>【Centralized】</strong> For general p ≥ 1, If x and Y are both centralized，$tilde x=x-overline x, tilde Y=Y-overline Y$</p>
<ul class="simple">
<li><p>$impliesbeta_0=0implies tilde X=(tilde x_1,dots,tilde x_p)inR^p$</p></li>
</ul>
<p>fitted linear regression model $tilde Y = β_1tilde x_1 +…+β_ptilde x_p+ tilde ε$</p>
<p>the regression $mathbb{E}(tilde Y)=β_1tilde x_1 +…+β_ptilde x_p$ is a function of $tilde X=(tilde x_1,dots,tilde x_p)^T$</p>
<p>The 95% confidence interval (CI) for EY is</p>
<p>$Big[mathbb{E}hat{tilde{Y}}-1.96tildesigmasqrt{tilde {X}^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde {X}}, mathbb{E}hat{tilde{Y}}+1.96tildesigmasqrt{tilde X^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde { X}}Big]$</p>
<p>To see the contribution of each predictor on $tilde Y$ , we can fix all the other variables at 0.
For example, the contribution of $tilde x_1xrightarrow{SET}hat{tilde Y}=beta_1tilde x_1+beta_2times0+dots+beta_ptimes 0$
The 95% confidence interval (CI) for $Etilde Y$ is
$Big[widehat{Etilde Y}-1.96tildesigmasqrt{tilde {chi}^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde {chi}}, space widehat{Etilde Y}+1.96tildesigmasqrt{tilde chi^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde { chi}}Big],chi=(tilde x_1,0,dots,0)^T$</p>
<p>Equivalence between hypothesis testing $H_0 : β_k = 0”$ and the band: After the data is centralized, we accept $H_0iff$
$$0inBig[hat{beta}_k-1.96hat{sigma}sqrt{c_{kk}}, hat{beta}_k+1.96hat{sigma}sqrt{c_{kk}}Big]$$</p>
<p>$$iff0inBig[widehat{Etilde Y}-1.96tildesigmasqrt{tilde {chi}^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde {chi}}, space widehat{Etilde Y}+1.96tildesigmasqrt{tilde chi^T(tilde {mathbb X}^Ttilde {mathbb X})^{-1}tilde { chi}}Big]$$</p>
<p>where $chi=(0,dots,1,dots,0)^T$. the kth element is 1 and the others 0.
This means 0 is included in the band at $x_k=1$. On the other hand, (0,0) is included in the band. Therefore, $H_0$ is accepted if and only if a horizontal line is included in the band.</p>
<p><span class="defi">the confidence band</span></p>
<p><strong>All the predicted expectations of Y and the bounds of the CI</strong> are functions of x.
We can draw these functions and get the confidence band. 就是所有的置信区间值都画出来</p>
<p>!!! warning “不同于 conformal prediction interval（这个是囊括百分之九十五的数据点）,  confidence band 是说预测的数据 fitted value 有百分之九十五的可能被囊括”</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](pics/LRs_2.png){width=80%}&lt;p&gt;uncentralized
$hat{EY}=hatbeta_0+hatbeta_1x$&lt;/p&gt;&lt;/figure&gt;</p>
<p>&lt;figure markdown=”span”&gt;![](pics/LRs_3.png){width=80%}&lt;p&gt;centralized
$hat{EY}=hatbeta_0+hatbeta_1x$&lt;/p&gt;&lt;/figure&gt;
&lt;/div&gt;</p>
<p>![](pics/LRs_4.png){width=80%}</p>
<p>==causal inference 因果推断==.是在一个较大系统内部确定指定现象的实际、独立效果的过程。因果推断和相关性推断的主要区别是前者分析结果变量在其原因变量变化时发生的回应。</p>
<p>==干扰因素 (Confounding)==. 又称为干扰因子、干扰变量、混淆变量、共变因等，在统计学和因果关系中是指会同时影响自变量和因变量，导致出现伪关系的一种变量。在不严谨的语境下，干扰因子也可以指所有未知变量，包括中介变因和对撞变因。干扰因子会造成伪关系，是相关不蕴涵因果的原因之一。
[Implementing Causal Inference: Trying to Understand the Question of Why]
[线性回归残差是否一定满足正态分布？ - 知乎]
[What does the assumption: “The independent variable is not random.” in OLS mean?]</p>
<p>[Implementing Causal Inference: Trying to Understand the Question of Why]: <a class="reference external" href="https://towardsdatascience.com/implementing-causal-inference-a-key-step-towards-agi-de2cde8ea599">https://towardsdatascience.com/implementing-causal-inference-a-key-step-towards-agi-de2cde8ea599</a>
[What does the assumption: “The independent variable is not random.” in OLS mean?]: <a class="reference external" href="https://stats.stackexchange.com/questions/462173/what-does-the-assumption-the-independent-variable-is-not-random-in-ols-mean">https://stats.stackexchange.com/questions/462173/what-does-the-assumption-the-independent-variable-is-not-random-in-ols-mean</a>
[线性回归残差是否一定满足正态分布？ - 知乎]: <a class="reference external" href="https://www.zhihu.com/question/489283459">https://www.zhihu.com/question/489283459</a></p>
<p>!!! p “Assumption: both X and Y are centralized and standardized(scaled) (接下来都是基于这个设定来统一一下)”</p>
</section>
</section>
</section>
<section id="id1">
<h2 id="id1">广义线性模型<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p>==generalized linear model 广义线性模型==。$g(*):=$联系函数 link function
$$y=g^{-1}(w^Tx+b)$$</p>
<p>&gt; 当 y 不是线性变化，而是在指数尺度上变化
&gt; $ln y=w^Tx+bLeftrightarrow g(*)=ln(*)$
&gt; 将指数曲线投影到一条直线上。</p>
</section>
<section id="regularization">
<h2 id="regularization">正则化 Regularization<a class="headerlink" href="#regularization" title="Link to this heading">¶</a></h2>
<p>==$L_p$ - Norm==。 $Vert wVert_p=(sumlimits_{i=1}^dx_i^p)^{1/p}$</p>
<p>!!! warning “针对的是 row”</p>
<p>避免过拟合的产生和减少网络误差
正则化的作用是选择**经验风险与模型复杂度同时较小**的模型。</p>
<section id="ridge-regression-l2-penalty">
<h3 id="ridge-regression-l2-penalty">Ridge Regression  - L2 penalty<a class="headerlink" href="#ridge-regression-l2-penalty" title="Link to this heading">¶</a></h3>
<section id="background">
<h4 id="background">Background<a class="headerlink" href="#background" title="Link to this heading">¶</a></h4>
<p><strong>Why we need Ridge Regression ↔ Understanding of Ridge Regression?</strong></p>
<p><span class="defi">the estimator $hat{beta}$ of LSE</span> $hatbeta_{LSE}= (mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y$ exists only if $(mathbb X^Tmathbb X)^{-1}$
exists.
However, $hatbeta_{LSE}$  does not exist $iff (mathbb X^Tmathbb X)$ is not invertible:</p>
<ol class="arabic simple">
<li><p>$mathbb XinR^{ntimes p}, p&gt;n$</p></li>
<li><p>the columns in $mathbb X$ are linearly dependent.</p></li>
</ol>
<p>&lt;u&gt;**idea 1: guarantee the invertibility**&lt;/u&gt;
A simple way to <strong>guarantee the invertibility</strong> is to add <strong>a diagonal matrix $IinR^{ptimes p}$</strong> to $mathbb X^Tmathbb X$。 (加单位矩阵 λI 这个不止用在 linear regression)
$implieshatbeta_{text{ridge}}= (mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb Y,quad lambda&gt;0$
这也能避免一个问题就是当 eigenvalue is too small 0.00001 的时候， inverse 就会变超大 100000，导致说这个 more in-stable，在类似 using Newton Method 需要顾及 variance inverse 的时候 converge more slowly。我加一个 small lambda = 0.01，整个就会 around 在100左右</p>
<p>&lt;u&gt;**idea 2: regularize the coefficients**&lt;/u&gt;
<strong>FOR:</strong> If the βs are unconstrained, they can explode and are susceptible to very high variance. 爆炸，容易受到非常高的方差影响 Thus, we need to regularize the coefficients.</p>
<p><strong>approach 1: penalize the value of beta:</strong>
Using penalty approach
==L2- Norm==. $Vert wVert_2=(sumlimits_{i=1}^dx_i^2)^{1/2}=sqrt{w^2}$
$beta^*(lambda):=minlimits_betasumlimits_{i=1}^n(Y_i-X_i^Tbeta)^2+lambda(beta_1^2+dots+beta_p^2),quadlambda&gt;0$
$iff beta^*(lambda):=minlimits_beta Vertmathbb Y-mathbb XbetaVert^2+lambdaVertbetaVert^2,quadlambda&gt;0$
$implieshatbeta_{text{ridge}}= (mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb Y,quad lambda&gt;0$</p>
<ol class="arabic simple">
<li><p>if $λ → ∞ implies hatβ^*(λ) → 0;$</p></li>
<li><p>if $λ = 0implieshat beta^*(λ) =hat β_{LSE} ;$</p></li>
</ol>
<p><strong>approach 2: constrain the value of beta:</strong>
$beta^*[t]:= minlimits_{beta}sumlimits_{i=1}^n (Y_i-X_i^Tbeta)^2,quad s.t.spacesumlimits_{j=1}^pbeta_j^2le t iff text{ L2 norm}$
$iff beta^*[t]:=minlimits_beta Vertmathbb Y-mathbb XbetaVert^2,quad s.t.spaceVertbetaVert^2le t$</p>
<ol class="arabic simple">
<li><p>if $t = 0implies hat β^*[t] = 0$;</p></li>
<li><p>if $t ≥ Verthat β_{LSE} Vert^2 = t_0 implieshatβ^* [t] = hatβ_{LSE};$</p></li>
</ol>
<p><strong>The Penalty approach and the Constraint approach are equivalent and t and λ have a kind of inverse relationship.</strong> $t=Verthatbeta^*(lambda)Vert^2impliesbeta^*(λ)=beta^*[λ]$</p>
<p><strong>approach 3: Data augmentation approach:</strong>
$beta^*(lambda):=minlimits_betasumlimits_{i=1}^n(Y_i-X_i^Tbeta)^2+lambda(beta_1^2+dots+beta_p^2),quadlambda&gt;0$
Rewrite it:
$minlimits_betasumlimits_{i=1}^n(Y_i-X_i^Tbeta)^2+(0-tilde X_{n+1}^Tbeta)^2+dots+(0-tilde X_{n+p}^Tbeta)^2=Vert  tilde{ mathbb Y}-tilde{mathbb X}betaVert^2\[1ex]qquadtilde X_{n+1}=(sqrtlambda,0,dots,0)^T,dots,tilde X_{n+p}=(0,dots,0,sqrtlambda)^T$</p>
<p>$tilde{mathbb X}=begin{bmatrix}x_{1,1},&amp;x_{1,2}&amp;dots&amp;x_{1,p}\x_{2,1},&amp;x_{2,2}&amp;dots&amp;x_{2,p}\vdots\x_{n,1},&amp;x_{n,2}&amp;dots&amp;x_{n,p}\sqrtlambda&amp;0&amp;dots&amp;0\0&amp;sqrtlambda&amp;dots&amp;0\vdots&amp;&amp;ddots\0&amp;0&amp;dots&amp;sqrtlambdaend{bmatrix}_{(n+p)times p}=begin{bmatrix}mathbb X\sqrtlambda I_pend{bmatrix},tilde{mathbb Y}= begin{bmatrix}Y_1\vdots\Y_n\0\vdots\0end{bmatrix}_{(n+p)}= begin{bmatrix}mathbb Y\0_pend{bmatrix}$
$implies hat{tildebeta}_{LSE}=(tilde{mathbb X}^Ttilde{mathbb X})^{-1}tilde{mathbb X}^Ttilde{mathbb Y}\[1ex]qquadqquad=(begin{bmatrix}mathbb X^T&amp;sqrtlambda I_pend{bmatrix}timesbegin{bmatrix}mathbb X\sqrtlambda I_pend{bmatrix})^{-1}begin{bmatrix}mathbb X^T&amp;sqrtlambda I_pend{bmatrix}begin{bmatrix}mathbb Y\sqrtlambda 0_pend{bmatrix}\[1ex]qquadqquad=(mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb Y$</p>
</section>
<section id="content">
<h4 id="content">Content<a class="headerlink" href="#content" title="Link to this heading">¶</a></h4>
<p><span class="defi">the estimator $hatbeta$ of Ridge Regression, Ridge</span>
$hatbeta_{text{ridge}}= minlimits_betaVert mathbb Y-mathbb XbetaVert^2+red{lambdaVertbetaVert^2}=(mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb Y,quad lambda&gt;0,$</p>
<p><strong>Properties</strong> of ridge regression (assuming  $mathbb X$ is nonrandom):</p>
<p>1. a <strong>biased</strong> estimator, $begin{cases}mathbb{E}hatbeta_text{ridge}=beta-lambda(mathbb X^Tmathbb X+lambda I)^{-1}beta\[1ex]text{bias}(hatbeta_text{ridge})=-lambda(mathbb X^Tmathbb X+lambda I)^{-1}betaend{cases}$
proof</p>
<p>$$begin{align*}
mathbb{E}hat{beta}&amp;=mathbb{E}Big{(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{Y}Big}\
&amp;xlongequal{mathbb{Y}=mathbb{X}beta+epsilon}mathbb{E}Big{(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^T(mathbb{X}beta+epsilon)Big}\
&amp;=mathbb{E}Big{(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{X}betaBig}+mathbb{E}Big{(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^TepsilonBig}\
&amp;=(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{X}beta+(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{E}epsilon\
&amp;xlongequal{mathbb{E}epsilon=0}(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{X}beta\
&amp;xlongequal{+1-1}(mathbb{X}^Tmathbb{X}+lambda I)^{-1}(mathbb{X}^Tmathbb{X}+lambda I)beta-(mathbb{X}^Tmathbb{X}+lambda I)^{-1}lambdabeta\
&amp;=beta-(mathbb{X}^Tmathbb{X}+lambda I)^{-1}lambdabetaneqbeta
end{align*}$$</p>
<p>$bias=red{beta}-mathbb{E}hat{beta}$</p>
<p>$Var=mathbb{E}(red{hat{beta}}-mathbb{E}hat{beta})^2$</p>
<p>1. Variance-covariance matrix
$text{if }Var(mathcal{E})=sigma^2I_nimplies Var(hatbeta_text{ridge})=(mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb X(mathbb X^Tmathbb X+lambda I)^{-1}sigma^2$
proof</p>
<p>$$begin{align*}
Var(hatbeta_text{ridge})&amp;=mathbb{E}Big{(hat{beta}_text{ridge}-mathbb{E}hat{beta}_text{ridge})^2Big}\
&amp;=mathbb{E}Big{big((X^TX+lambda I)^{-1}mathbb{X}^Tmathbb{Y}-mathbb{E}(X^TX+lambda I)^{-1}mathbb{X}^Tmathbb{Y}big)^2Big}\
&amp;xlongequal[mathbb{Y=X}beta+epsilon]{mathbb{Eepsilon=0}}mathbb{E}Big[(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^TepsilonBig]^2\
&amp;=mathbb{E}Big[(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tepsilonepsilon^Tmathbb{X}(mathbb{X}^Tmathbb{X}+lambda I)^{-1}Big]\
&amp;=(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{E}Big[epsilonepsilon^TBig]mathbb{X}(mathbb{X}^Tmathbb{X}+lambda I)^{-1}\
&amp;xlongequal{Var(epsilon)=sigma^2I}sigma^2cdot(mathbb{X}^Tmathbb{X}+lambda I)^{-1}mathbb{X}^Tmathbb{X}(mathbb{X}^Tmathbb{X}+lambda I)^{-1}
end{align*}$$</p>
<section id="the-shrinkage-parameter-ridge">
<h5 id="the-shrinkage-parameter-ridge">λ := the shrinkage parameter - ridge<a class="headerlink" href="#the-shrinkage-parameter-ridge" title="Link to this heading">¶</a></h5>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>controls</strong> the <strong>size</strong> of the coefficients, 它仅仅是调整参数的大小，并没有删去特征</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 283)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. <strong>controls</strong> the amount of regularization
$lambda rightarrow begin{cases}0&amp;hatbeta_{ridge} =hatbeta_{LSE}\infin&amp;hatbeta_text{ridge}=0end{cases}，qquad λuparrow, Biasuparrow,Var(beta_{ridge})downarrow$</p>
</div></blockquote>
<p>每一个  λ 可以求出一个 $hatbeta_{ridge} iff$ the solution is indexed by the parameter λ。
对每一个 lambda 我们都能 画出它的 ==solution path==.  λs trace out <strong>a path of solutions</strong></p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](pics/LRs_5.png){width=60%}&lt;p&gt;coefficients paths&lt;/p&gt;&lt;/figure&gt;</p>
<p>右图是根据课上 centralized and standardized diabetes data 画出来的
$y=beta_1x_1+dots+beta_{10}x_{10}+epsilon$
一共有10个 $β_i$, 每一条线对应的是一个 $β_i$
随着 λ 增大，logλ 增大，每一个 β 的绝对值都有不同程度的减少。当 λ 到达一个绝对大的值，所有的 β 都被收缩至 0.
&lt;/div&gt;</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](pics/LRs_6.png){width=80%}&lt;p&gt;coefficients paths&lt;/p&gt;&lt;/figure&gt;</p>
<p>Except for t = 0, in the other cases none of the coefficient is zero
&lt;/div&gt;</p>
<p>$beta^*[t]:=minlimits_beta Vertmathbb Y-mathbb XbetaVert^2,quad s.t.spaceVertbetaVert^2le t$</p>
<dl class="simple">
<dt>!!! p “How to choose λ?  How to compare the different λ?”</dt><dd><p><strong>a tuning parameter needs to be chosen</strong>. 作为人为设定的参数，它是可调参的</p>
</dd>
</dl>
<p><strong>via CV:</strong></p>
<p>We select a large range for possible $λin [0, c]$. For each fixed λ in [0, c], consider the CV as follows. For each j,</p>
<p>$$hatβ_{ridge}^j(λ) = (sumlimits_{i≠j} X_i^TX_i + λI)^{−1} sumlimits_{i≠j} X_i^TY_i.$$
The prediction error for $(X_j ,Y_j)$ is  $err^j(lambda)=(Y_j-X_jhatbeta_{ridge}^j(lambda))^2$
The CV value is then $CV(lambda)=n^{-1}sumlimits_{j=1}^nerr^j(lambda)$
$implieslambda^* = minlimits_{lambda}CV(lambda)$</p>
<p>&lt;figure markdown=”span”&gt;![](pics/LRs_6.png){width=60%}&lt;p&gt;The best λ is the minimum point of CV(λ). &lt;/p&gt;&lt;/figure&gt;</p>
</section>
</section>
</section>
<section id="lasso-l1-penalty-least-absolute-shrinkage-and-selection-operator">
<h3 id="lasso-l1-penalty-least-absolute-shrinkage-and-selection-operator">Lasso - L1 penalty, Least Absolute Shrinkage and Selection Operator<a class="headerlink" href="#lasso-l1-penalty-least-absolute-shrinkage-and-selection-operator" title="Link to this heading">¶</a></h3>
<p>!!! p “这比估计所有确切值要容易得多，因为估计非常小的东西需要你需要很多信息，因为你必须增加样本量。否则，你无法正确估计一些弱信号，但一旦我们将3个信号定位为零，定位它们的位置就更容易了。我们确实牺牲了一些东西，”</p>
<p><span class="defi">L1- Norm</span> $Vert wVert_1=(sumlimits_{i=1}^d|x_i|)=|w|$
<span class="defi">the estimator $hatbeta$ of Lasso Regression, lasso</span>
$hatbeta_{text{lasso}}(lambda):= minlimits_betasumlimits_{i=1}^n({Y_i-sumlimits_{j=1}^pX_{ij}beta_j}^2+lambdared{sumlimits_{j=1}^p|beta_j|}=minlimits_betaVert Y-XbetaVert^2+red{lambdaVertbetaVert_1}\[1em]qquadhatbeta_{text{lasso}}[lambda]:= minlimits_betasumlimits_{i=1}^n({Y_i-sumlimits_{j=1}^pX_{ij}beta_j}^2, quad red{sumlimits_{j=1}^p|beta_j|le t}iff red{VertbetaVert_1le t}$</p>
<p>The two approaches are also <strong>equivalent</strong>:
$$hatbeta_{lasso}[Verthatbeta_{lasso}(lambda)Vert_1]=hatbeta_{lasso}(beta)$$</p>
<p>==sparsity==. Because of the nature of the constraint, with letting t sufficiently small (or λ sufficiently large),  Lasso will cause <strong>some of the coefficients to be exactly zero</strong>. 对 Lasso 来说，他确实在产生0，所以 $hatbeta_{lasso}$ is sparse 稀疏的，那么这就可以进行 variable selection</p>
<p>!!! question “那如果在lasso之后把beta为0的特征值扔掉之后再LSE的准确率会一样吗？”</p>
<section id="the-shrinkage-parameter-lasso">
<h4 id="the-shrinkage-parameter-lasso">λ := the shrinkage parameter -lasso<a class="headerlink" href="#the-shrinkage-parameter-lasso" title="Link to this heading">¶</a></h4>
<p><span class="defi">t</span></p>
<ol class="arabic simple">
<li><p>如果 t 足够大到 $sumlimits_{i=1}^p|{hatbeta_{LSE}}_j|=Verthatbeta_{LSE}Vert_1=t_0le timplieshatbeta_{lasso}=hatbeta_{LSE}$</p></li>
<li><p>如果 $t = 0implies VertbetaVert_1=0implieshatbeta_{lasso}=0implies$no variable is selected</p></li>
<li><p>$s:= cfrac{t}{t_0}=cfrac{t}{Verthatbeta_{LSE}Vert_1}=begin{cases}0iff t = 0 impliestext{no variables selected}\1iff t = t_0implieshatbeta_{lasso}=hatbeta_{LSE}end{cases}$</p></li>
<li><p>it also inherits the properties of ridge regression and sometimes have more efficient estimator ——— exactly 0</p></li>
</ol>
<p><strong>with CV:</strong> Usually, 5-fold or 10-fold CV is used.</p>
<p><a href="#id2"><span class="problematic" id="id3">``</span></a><a href="#id4"><span class="problematic" id="id5">`</span></a>r
mycv = cv.glmnet(x, y, lambda, type.measure, nfolds)
plot(mycv)
to plot the CV values against lambda
==========================================================================</p>
<aside class="system-message" id="id2">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 348); <em><a href="#id3">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id4">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 348); <em><a href="#id5">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
<dl class="simple">
<dt>mylasso = glmnet(x, y, family, alpha, nlambda = 100, lambda=NULL,</dt><dd><p>standardize = TRUE, intercept=TRUE, standardize.response=FALSE)</p>
</dd>
</dl>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 356)</p>
<p>Definition list ends without a blank line; unexpected unindent.</p>
</aside>
<p>plot(mylasso):
to plot the path of the estimated coefficients against t
==================================================================================================================</p>
<p>mypred = predict(mylasso, newx, type=c(”link”,”response”, ”coefficients”,”nonzero”,”class”))
<a href="#id6"><span class="problematic" id="id7">``</span></a><a href="#id8"><span class="problematic" id="id9">`</span></a></p>
<aside class="system-message" id="id6">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 360); <em><a href="#id7">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id8">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 360); <em><a href="#id9">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
<p><cite>mycv = cv.glmnet(x, y, lambda, type.measure, nfolds)</cite></p>
<ul class="simple">
<li><dl class="simple">
<dt>input</dt><dd><ul>
<li><p>x: matrix of design of n×p</p></li>
<li><p>y: response y as in glmnet.</p></li>
<li><p>lambda=: Optional user-supplied lambda sequence; <strong>default is NULL, and glmnet chooses its own sequence</strong></p></li>
<li><p>nfolds=: number of folds - <strong>default is 10,</strong> nfolds=n is the delete-one-observation CV.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>output</dt><dd><ul>
<li><p><cite>mycv$cvm</cite>: the mean cross-validated error - a vector of length(lambda).</p></li>
<li><p><cite>mycv$cvsd</cite>: estimate of <cite>the</cite> standard error of cvm.</p></li>
<li><p><cite>mycv$cvup</cite>: upper curve = cvm+cvsd.</p></li>
<li><p><cite>mycv$cvlo</cite>: lower curve = cvm-cvsd.</p></li>
<li><p><cite>mycv$nzero</cite>: number of non-zero coefficients at each lambda.</p></li>
<li><p><cite>mycv$lambda.min</cite>: value of lambda that gives minimum cvm.</p></li>
<li><p><cite>mycv$lambda.1se</cite>: vlargest value of lambda such that error is within 1 standard error of the minimum.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><cite>mylasso = glmnet(x, y, family, alpha, nlambda = 100, lambda=NULL, standardize = TRUE, intercept=TRUE, standardize.response=FALSE)</cite></p>
<ul>
<li><dl>
<dt>input</dt><dd><ul class="simple">
<li><p>x: input matrix, of dimension $ntimes p$; each row is an observation vector.</p></li>
<li><p>y: response variable.</p></li>
<li><dl class="simple">
<dt><cite>family</cite>: Response type, different generalized linear regression model</dt><dd><ul>
<li><p><cite>gaussian</cite> linear regression model</p></li>
<li><p>binomial”,”poisson”, ”multinomial”,”cox”,”mgaussian”)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><cite>alpha</cite>: The elasticnet mixing parameter, with 0 ≤ α ≤ 1. The penalty is defined as</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 388)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>$(1-alpha)/2VertbetaVert_2+alphaVertbetaVert_1rightarrowalpha = begin{cases}1&amp;text{ lasso(default)}\0 &amp;text{ ridge}end{cases}$
- <cite>nlambda</cite>: the number of lambda values - default is 100. 用来生成 solution path
- <cite>lambda</cite>: a user supplied lambda sequence.
- <cite>standardize</cite> = TRUE( default) / FALSE. Logical flag for X variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale.
- <cite>intercept</cite>: Should intercept(s) be fitted (default=TRUE) or set to zero (FALSE)
- <cite>dfmax</cite>: Limit the maximum number of variables in the model. Useful for very large p, if a partial path is desired.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>output</dt><dd><ul class="simple">
<li><p>mylasso$a0: Intercept sequence of length length(lambda)</p></li>
<li><p>mylasso$beta: all the estimated β with different λs.</p></li>
<li><p>mylasso$lambda: The actual sequence of lambda values used</p></li>
<li><p>mylasso$df: The number of nonzero coefficients for each value of lambda.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><cite>mypred = predict(mylasso, newx, type=c(”link”,”response”, ”coefficients”,”nonzero”,”class”))</cite></p>
<ul class="simple">
<li><p>mylasso: or any other name fitted ”glmnet” model object.</p></li>
<li><p>newx: Matrix of new values for x at which predictions are to be made.</p></li>
<li><dl class="simple">
<dt>type:</dt><dd><ul>
<li><dl class="simple">
<dt><cite>link</cite></dt><dd><ul>
<li><p>gives the linear predictors for ”binomial”, and ”multinomial” models;</p></li>
<li><p>for ” Gaussian” models it gives the fitted values.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><cite>response</cite> gives the fitted <strong>probabilities</strong> for ”binomial” or ”multinomial”;</p></li>
<li><p><cite>class</cite> applies only to ”binomial” or ”multinomial” models. for ”binomial” models, results are returned only for the class corresponding to the second level.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Please state the procedure of selecting the tuning parameter based on 5-fold CV.</p></li>
</ul>
</section>
</section>
<section id="elastic-net-a-combination-of-lasso-and-ridge-regression">
<h3 id="elastic-net-a-combination-of-lasso-and-ridge-regression">elastic net: A combination of Lasso and Ridge regression<a class="headerlink" href="#elastic-net-a-combination-of-lasso-and-ridge-regression" title="Link to this heading">¶</a></h3>
<p>==ridge==. $hatbeta_{ridge}=minlimits_betasumlimits_{i=1}^n{Y_i-beta_1x_{i1}-beta_2x_{i2}}^2+lambdaVertbetaVert_2^2$</p>
<p>==lasso==. $hatbeta_{lasso} = minlimits_betasumlimits_{i=1}^n{Y_i-beta_1x_{i1}-beta_2x_{i2}}^2+lambdaVertbetaVert_1$</p>
<p><span class="defi">the elastic net estimator</span>
$hatbeta_{net}:=minlimits_betasumlimits_{i=1}^n{Y_i-beta_1x_{i1}-beta_2x_{i2}}^2+lambda{(1-alpha)VertbetaVert_2^2+alphaVertbetaVert_1},alphain[0,1]$</p>
</section>
</section>
<section id="comparison">
<h2 id="comparison">Comparison<a class="headerlink" href="#comparison" title="Link to this heading">¶</a></h2>
<div class="line-block">
<div class="line-block">
<div class="line">| LSE | Ridge | Lasso</div>
</div>
<div class="line">— | — | — | — |</div>
<div class="line">$mathbb{E}$ | $beta$ | $beta-lambda(mathbb X^Tmathbb X+lambda I)^{-1}beta$ | ?</div>
<div class="line">Bias | unbiased = 0 (smaller) | biased $= -lambda(mathbb X^Tmathbb X+lambda I)^{-1}beta$ | ?</div>
<div class="line">Variance | $(mathbb X^Tmathbb X)^{-1}sigma^2$ | $(mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb X(mathbb X^Tmathbb X+lambda I)^{-1}sigma^2$ (smaller) <a href="#id10"><span class="problematic" id="id11">|</span></a>?</div>
</div>
<aside class="system-message" id="id10">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 429); <em><a href="#id11">backlink</a></em></p>
<p>Inline substitution_reference start-string without end-string.</p>
</aside>
<p>!!! warning “Obviously, $|text{bias}(hatbeta_text{LSE})|le|text{bias}(hatbeta_text{ridge})|$, but $Var(hatbeta_text{LSE})ge Var(hatbeta_text{ridge})$. But how do we know which is better?”</p>
<p>==Mean Squared Error, MSE==. one of evaluations of an estimator of parameter:</p>
<p>$$begin{align*}MSE(hatmu)=mathbb EVert hatmu-muVert^2&amp;=mathbb E{red{(hatmu-mu)^T(hatmu-mu)_{inR}}}\&amp;=mathbb Etr{ red{(hatmu-mu)(hatmu-mu)^T_{in S^n}}}\&amp;=tr{Var(hatmu)}+Vert Bias(hatmu)Vert^2\&amp;=tr{Var(hatmu)}+Bias(hatmu)^TBias(hatmu)end{align*}$$</p>
<p>==Ridge &amp; LSE==.  $existlambdage0,text{ such that }mathbb E Verthatbeta_{text{ridge}}- betaVert^2lemathbb EVerthatbeta_{text{LSE}}-betaVert^2$
一个理论支撑：是存在这么一个 lambda，所以只要我们选对 lambda， ridge 会比 LSE 要好.(指的是MSE更小)</p>
<ul class="simple">
<li><p>Case 1 If X is orthonormal, then  $mathbb X^Tmathbb X = I_p,$</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/ML/lr.rst</span>, line 441)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>$begin{cases}hatbeta_{text{ridge}}= (mathbb X^Tmathbb X+lambda I)^{-1}mathbb X^Tmathbb Y= (1+lambda)^{-1}mathbb X^Tmathbb Y\hatbeta_{LSE}=mathbb X^T mathbb Yend{cases}implies hatbeta_{ridge}=cfrac{1}{1+lambda}hatbeta_{LSE}$
The optimal choice of λ minimizing $mathbb EVerthat β_{ridge} − βVert^2$ and the expected prediction error is:
$qquadlambda^*=minlimits_{lambda}mathbb EVerthat β_{ridge} − βVert^2\qquadqquadimplies lambda^* = cfrac{prho^2}{sumlimits_{j=1}^pbeta_j^2}$
$(beta_1,dots,beta_p):=$ the true coefficient vector.
At this time  $mathbb EVerthat β_{ridge} − βVert^2red{&lt;} mathbb EVerthat β_{LSE} − βVert^2$.
- Case 2  $mathbb X^Tmathbb X = nI_p,$</p>
<p>crease more bias but the variance is smaller iff more confident</p>
<section id="lse-lasso-ridge">
<h3 id="lse-lasso-ridge">LSE &amp; Lasso &amp; ridge<a class="headerlink" href="#lse-lasso-ridge" title="Link to this heading">¶</a></h3>
<p>![](pics/LRs_7.png){width=40%}</p>
<p>&gt; &gt; Consider a special case with <strong>1 variable</strong> and represent the problem。 $Y_i=beta x_i+epsilon_i,i=1,dots,n$
&gt; &gt; And assume that after centralized and scaled $ifffrac{1}{n}sumlimits_{i=1}^nx_i = 0,frac{1}{n}sumlimits_{i=1}^nx_i^2=mathbb X^Tmathbb X=1$
&gt;
&gt; $hatbeta_{LSE}=(mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y=mathbb X^Tmathbb Y=sumlimits_{i=1}^nx_iy_i$
&gt;
&gt; $hatbeta_{ridge} = (mathbb X^Tmathbb X+lambda I)mathbb X^Tmathbb Y=cfrac{hatbeta_{LSE}}{1+lambda}=cfrac{sumlimits_{i=1}^nx_iy_i}{1+lambda}$
&gt;
&gt; $hatbeta_{lasso}=minlimits_{beta}{Y_i-beta x_i}^2+lambda <a href="#id14"><span class="problematic" id="id15">|\beta|</span></a>xlongequal[tildelambda=lambda/2]{rewrite}minlimits_{beta}{Y_i-beta x_i}^2+2tildelambda <a href="#id16"><span class="problematic" id="id17">|\beta|</span></a>,quadlambda=2tildelambda&gt;0$
&gt;
&gt; ![](pics/LRs_11.png){width=70%}</p>
<ul class="simple">
<li><p>Proof of Lasso.</p></li>
</ul>
<p>$$begin{align*}
beta_text{lasso}&amp;:=min_betaVertmathbb{Y}-mathbb{X}betaVert^2+lambdaVertbetaVert_1\
&amp;xlongequal{1 text{variable}} min_betasum_{i=1}^n(y_i-beta x_i)^2+lambdavertbetavert\
&amp;xlongequal[tilde{lambda}=lambda/2]{Rewrite}min_betasum_{i=1}^n(y_i-beta x_i)^2+2tilde{lambda}vertbetavert
end{align*}$$</p>
<p>Take a derivate $cfrac{partial f}{partialbeta}xlongequal{SET}0$</p>
<p>$$begin{align*}
frac{1}{2}cdotcfrac{partial f}{partialbeta}&amp;=-sum_{i=1}^n(y_i-beta x_i)x_i+tilde{lambda}cfrac{partialvertbetavert}{partialbeta}\
&amp;=-sum_{i=1}^nx_iy_i+betasum_{i=1}^nx_i^2+tilde{lambda}cfrac{partialvertbetavert}{partialbeta}\
&amp;xlongequal{sumlimits_{i=1}^nx_i^2=1}-hat{beta}_{LSE}+beta+tilde{lambda}cfrac{partialvertbetavert}{partialbeta}=0
end{align*}$$</p>
<p>1. when 极值点存在于 $beta&gt;0$
$implieshat{beta}_{text{lasso}}=hat{beta}_{LSE}-tilde{lambda}$ exists when $hat{beta}_{LSE}-tilde{lambda}&gt;0$
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta=0)=-hat{beta}_{LSE}&lt;0$
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta&lt;0)=-hat{beta}_{LSE}+beta-tilde{lambda}&lt;-hat{beta}_{LSE}+0-tilde{lambda}&lt;0$
$implieshat{beta}_{text{lasso}}=hat{beta}_{LSE}-tilde{lambda}$ 是唯一一个极值点，开口向上，所是 <strong>global minimizer</strong>
1. when 极值点存在于 $beta&lt;0$
$implieshat{beta}_{text{lasso}}=hat{beta}_{LSE}+tilde{lambda}$ exists when $hat{beta}_{LSE}+tilde{lambda}&lt;0$
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta=0)=-hat{beta}_{LSE}&gt;0$
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta&gt;0)=-hat{beta}_{LSE}+beta+tilde{lambda}&gt;-hat{beta}_{LSE}+0+tilde{lambda}&gt;0$
$implieshat{beta}_{text{lasso}}=hat{beta}_{LSE}+tilde{lambda}$ 是唯一一个极值点，开口向上，所是 <strong>global minimizer</strong>
1. when 极值点存在于 $beta=0$
$implieshat{beta}_{text{lasso}}=0$ exists when
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta&gt;0)=-hat{beta}_{LSE}+beta+tilde{lambda}&gt;0$ always true when $hat{beta}_{LSE}&lt;tilde{beta}$
$frac{1}{2}cdotfrac{partial f}{partialbeta}(beta&lt;0)=-hat{beta}_{LSE}+beta-tilde{lambda}&lt;0$ always true when $hat{beta}_{LSE}&gt;-tilde{beta}$
$implieshat{beta}_{text{lasso}}=hat{beta}_{LSE}=0$ 是唯一一个极值点，开口向上，所是 <strong>global minimizer</strong></p>
<p>$$hat{beta}_text{lasso}=begin{cases}hat{beta}_text{LSE}-tilde{lambda}&amp;0&lt;tilde{lambda}&lt;hat{beta}_text{LSE}\
0&amp;-tilde{lambda}&lt;hat{beta}_{LSE}&lt;tilde{lambda}\
hat{beta}_text{LSE}+tilde{lambda}&amp;hat{beta}_text{LSE}&lt;-tilde{lambda}&lt;0\
end{cases}=hat{beta}_{LSE}+text{sgn}(hat{beta}_{LSE})tilde{lambda}$$</p>
<p>&lt;u&gt;shrinks big coeﬀicients by a constant $tilde{lambda}=frac{lambda}{2}$ towards zero.truncates small coeﬀicients to zero exactly&lt;/u&gt;</p>
</section>
<section id="lasso-ridge">
<h3 id="lasso-ridge">Lasso &amp; ridge<a class="headerlink" href="#lasso-ridge" title="Link to this heading">¶</a></h3>
<p><strong>同是 shrinkage method，，但是在 penalty 项有所不同 (subtle but important differences)。</strong></p>
<p>Consider a case with 2 variables after centralized and scaled and represent the problem with a constraint format.</p>
<p>$hatbeta_{ridge}=minlimits_betasumlimits_{i=1}^n{Y_i-beta_1x_{i1}-beta_2x_{i2}}^2,quad s.t. sumlimits_{j=1}^2beta_j^2le t$</p>
<p>$hatbeta_{lasso} = minlimits_betasumlimits_{i=1}^n{Y_i-beta_1x_{i1}-beta_2x_{i2}}^2,quad s.t. sumlimits_{j=1}^2|beta_j|le t$</p>
<div class="line-block">
<div class="line-block">
<div class="line">| Ridge | Lasso |</div>
</div>
<div class="line">— | — | — |</div>
<div class="line">constraint/penalty | 平方 | 绝对值 absolute value |</div>
<div class="line">space | circle | square |</div>
<div class="line-block">
<div class="line">|  | 稀疏解（最优解常出现在顶点上，且顶点上的 w 只有很少的元素是非零的） |</div>
<div class="line">| 凸函数，处处可微分 | 凸函数，不是处处可微分 |</div>
</div>
</div>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](pics/LRs_8.png)&lt;p&gt;the lasso (left) and ridge regression (right)&lt;/p&gt;&lt;/figure&gt;</p>
<p>The solid blue areas are the constraint regions.
the red ellipses are the contours of the least squares error function
&lt;/div&gt;</p>
<p><strong>Same:</strong>
很明显，它被迫限制 search in the constraint area
<strong>Difference:</strong></p>
<ol class="arabic simple">
<li><p>他们都在 shrinkage in beta towards 0, 但 lasso creates zeros （这个 property is very attractive）</p></li>
</ol>
</section>
</section>
<section id="solution-path">
<h2 id="solution-path">solution path<a class="headerlink" href="#solution-path" title="Link to this heading">¶</a></h2>
<p>Notice that the solution is indexed by the parameter λ – So for each λ, we have a solution</p>
<p>当我们不指定 coefficient lambda 就会给我们生成 a lot of different lambda.
when Lambda is very small, whereby your estimate is close to least square. All coefficient was estimated shrink is towards 0, so the L1 norm becomes smaller and smaller and smaller
And and and the solution path is <strong>something like how different lambdas affects the solution and each line is actually estimated for for the corresponding feature.</strong></p>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>