<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>high dimentional DA &#8212; cocobook  文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/def.css?v=5a9d86bd" />
    <script src="../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#AI/highd" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="cocobook  文档"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">cocobook  文档</span>
          <span class="md-header-nav__topic"> high dimentional DA </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">cocobook  文档</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="cocobook 文档" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="cocobook 文档">cocobook  文档</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#ai-highd--page-root" class="md-nav__link">high dimentional DA</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#some-special-notation" class="md-nav__link">some special notation</a>
        </li>
        <li class="md-nav__item"><a href="#special-matrix" class="md-nav__link">Special Matrix</a>
        </li>
        <li class="md-nav__item"><a href="#variable-selection" class="md-nav__link">Variable Selection</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#filter-screening" class="md-nav__link">Filter, Screening, 过滤法</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#perfect-models-with-general-variables" class="md-nav__link">Perfect Models with General Variables</a>
        </li>
        <li class="md-nav__item"><a href="#working-model-with-more-redundant-variables" class="md-nav__link">Working Model with More Redundant Variables</a>
        </li>
        <li class="md-nav__item"><a href="#working-model-with-less-important-variables" class="md-nav__link">Working Model with Less Important Variables</a>
        </li>
        <li class="md-nav__item"><a href="#candidate-models-for-p-1-predictor-1-x-1-x-p" class="md-nav__link">Candidate Models for p+1 predictor $1， x_1, …, x_p$</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#model-selection-for-lr" class="md-nav__link">Model Selection for LR</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#dimensionality-reduction" class="md-nav__link">Dimensionality Reduction，数据降维</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#principal-component-analysis-pca" class="md-nav__link">Principal Component Analysis, PCA, 主成分分析</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#key-maximize-the-variance" class="md-nav__link">KEY: Maximize the variance</a>
        </li>
        <li class="md-nav__item"><a href="#pca-transformation" class="md-nav__link">PCA Transformation</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#practical-use" class="md-nav__link">Practical Use</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#graphical-rotate-the-data-without-scaling" class="md-nav__link">Graphical: Rotate the data without scaling</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#linear-discriminant-analysis-lda" class="md-nav__link">Linear Discriminant Analysis, LDA, 线性判别分析</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#fishers-lda" class="md-nav__link">Fisher’s LDA</a>
        </li>
        <li class="md-nav__item"><a href="#bayes-lda" class="md-nav__link">Bayes’ LDA</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#quadratic-discriminant-analysis-qda" class="md-nav__link">Quadratic Discriminant Analysis, QDA</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/AI/highd.rst.txt">显示源代码</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="high-dimentional-da">
<h1 id="ai-highd--page-root">high dimentional DA<a class="headerlink" href="#ai-highd--page-root" title="Link to this heading">¶</a></h1>
<p><span class="defi">Data mining</span> is the process of discovering new patterns from LARGE DATA sets using methods of artificial intelligence, machine learning, statistics and database systems.</p>
<p>==Curse of Dimensionality 维度灾难==。 会导致分类器出现**过拟合**。这是因为&lt;u&gt;在样本容量固定时，随着特征数量的增加，单位空间中的样本数量会变少。&lt;/u&gt;恰当的维数特征数对于机器学习模型非常重要。深度学习通过对样本的特征进行复杂的变换，得到对类别最有效的特征，从而提高机器学习的性能。</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/highD_1.png){width=60%}&lt;figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/highD_2.png){width=60%}&lt;figure&gt;
&lt;p&gt;假设样本集是由圆形和三角形组成的20个样本，假设这些样本均匀地分布在这4个区域，则每个区域的样本个数约为5个。若希望在二维空间中每个区域的样本数量与一维时大致相等，则需要400个样本；若是三维空间，则需要8000个样本&lt;/p&gt;
&lt;/div&gt;</p>
<dl class="simple">
<dt>!!! danger “很少 <strong>observation n，</strong> 很多 <strong>features p</strong> 情况下的高维.&lt;br&gt; p is very large, but n is relatively small.”</dt><dd><p>就拿医学来说，病人总是少数的，但是相关的因素总是特别多的。譬如那个基因检测。我们总得解决这种高维问题。
&gt; 有569个observations，30个 features。对于求features 的 covariance matrix $Σin S$来说，有$cfrac{30*29}{2}approx 430$个 parameters 要去 estimate。如果 take average 几乎是一个parameter 一个observation，这已经算是 high dimensional problem，除非数据very clean.</p>
</dd>
</dl>
<div class="line-block">
<div class="line">数据降维 dimensionally reduction | 特征选择 Variable Selection |</div>
<div class="line">— | — |</div>
<div class="line">多个特征合成为一个特征 | 在多个特征中选择某个特征 |</div>
<div class="line">获取无法解释的特征与变量之间的关系 | 可解释性强 |</div>
</div>
<section id="some-special-notation">
<h2 id="some-special-notation">some special notation<a class="headerlink" href="#some-special-notation" title="Link to this heading">¶</a></h2>
<p><span class="defi">Random Vector</span> $Z=begin{bmatrix}Z_1\vdots\Z_pend{bmatrix}inR^p$</p>
<ul>
<li><p><span class="defi">Expectation Matrix</span> $mathbb EZ=begin{bmatrix}mathbb EZ_1\vdots\mathbb EZ_pend{bmatrix}$</p></li>
<li><dl>
<dt><span class="defi">Covariance Matrix</span> $Sigma=Var(Z)\=mathbb E{(Z-mathbb EZ)(Z-mathbb EZ)^T}\=begin{bmatrix}Var(Z_1)&amp;Cov(Z_1,Z_2)&amp;dots&amp;Cov{Z_1,Z_p}\Cov(Z_1,Z_2)&amp;Var(Z_2)&amp;dots&amp;Cov(Z_2,Z_p)\vdots&amp;&amp;ddots&amp;vdots\Cov(Z_p,Z_1)&amp;Cov(Z_p,Z_2)&amp;dots&amp;Var(Z_p)end{bmatrix}$</dt><dd><ul class="simple">
<li><p>$Sigma succeq0$</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 31)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>proof: the sample covariance matrix is non-negative definite.</p>
</dd>
</dl>
</li>
</ul>
<p>Correlation只是考察线性关系的相关性，并不是代表independent</p>
<ul>
<li><p>$mathbb E(X+Y)=mathbb EX+mathbb EY$</p></li>
<li><p>$W:=A_{ptimes p}Z_{ptimes 1}+cinR^p$, <strong>constant</strong> matrix A, <strong>constant</strong> vector c</p>
<blockquote>
<div><p>$mathbb E(AZ+c)=c+Amathbb EZ$</p>
<p>$Var(AZ+c)=Var(AZ)=AVar(Z)A^T$</p>
</div></blockquote>
</li>
<li><p>$mathbb E(AXB+c)=Amathbb EXB+c$</p></li>
</ul>
<ol class="arabic">
<li><p>Positive-definite matrices )</p>
<blockquote>
<div><p>Eigenvalue decomposition $A=GammaLambdaGamma^T=sumlimits_{i=1}^plambda_igamma_igamma_i^T$</p>
</div></blockquote>
</li>
</ol>
<p><span class="defi">Multivariate Normal Distribution</span> $Z～N(b,Sigma)$
Suppose $Z=begin{bmatrix}ξ_1\vdots\ξ_pend{bmatrix}inR^p$ is a random vector, $mathbb EZ=binR^p,Var(Z)=Sigmain S^p$
$forall linR^p,l^TZinR$ ～ Normal distribution. $implies Z$ follows Multivariate Normal Distribution.
$iff Z～N(b,Sigma),b=begin{bmatrix}b_1\vdots\b_pend{bmatrix},Sigma=begin{bmatrix}sigma_{11}&amp;dots&amp;sigma_{1p}\vdots&amp;ddots&amp;vdots\sigma_{p1}&amp;dots&amp;sigma_{pp}\end{bmatrix}\
iff  f_Z(ξ_1,dots,ξ_p)=cfrac{1}{(2pi)^{k/2}|Sigma|^{1/2}}expBig(-cfrac{1}{2}(Ζ-b)^T)Sigma^{-1}(Z-b) Big)$</p>
<p><strong>properties:</strong></p>
<ol class="arabic simple">
<li><p>$foralltext{ constant }linR^p,cinR,space l^TZ+c～N(l^Tb+c,l^TSigma l)$</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 57)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. Partial correlation and conditional independence
$(Ζ_1^T,Ζ_2^T)^T～NBig(begin{bmatrix}b_1^T&amp;b_2^Tend{bmatrix}^T,begin{bmatrix}Sigma_{11}&amp;Sigma_{12}\Sigma_{21}&amp;Sigma_{22}end{bmatrix}Big)$
$\quad iff begin{bmatrix}ξ_{11}\vdots\ξ_{1p}\ξ_{21}\vdots\ξ_{2p}end{bmatrix}～NBig(begin{bmatrix}b_1\b_2end{bmatrix},begin{bmatrix}Sigma_{11}&amp;Sigma_{12}\Sigma_{21}&amp;Sigma_{22}end{bmatrix}Big)\quadiff begin{cases}Z_i～Ν(b_i,Sigma_{ii})\Z_1|Z_2～NBig(b_1+Sigma_{12}Sigma_{22}^{-1}(X_2-b_2),Sigma_{11}-Sigma_{12}Sigma_{22}^{-1}Sigma_{21}Big)end{cases}$</p>
<ul class="simple">
<li><p>proof:</p></li>
</ul>
<p>1. 在 normal multivariate distribution 里 Covariance = 0  等同于 independent
$cov(Z_1,Z_2)=sigma_{ij}=0iff Z_1,Z_2 text{ are independent}$</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 65)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><ul class="simple">
<li><p>proof</p></li>
</ul>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 66)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<ol class="arabic simple" start="2">
<li><p>$ξ_k～N(b_k,sigma_{kk}),begin{cases}ξ_i+ξ_j～N(b_i+b_j,sigma_{ii}+2sigma_{ij}+sigma_{j})inR\ξ_i-ξ_j～N(b_i-b_j)～Ν(sigma_{ii}-2sigma_{ij}+sigma_{jj})inRend{cases}$</p></li>
</ol>
<p>==I.I.D. observations==. $text{Suppose } X_1,X_2,dots,X_minR^p text{ are i.d.d.}$</p>
<ol class="arabic simple">
<li><p>$mathbb EX=overline X=cfrac{1}{n}sumlimits_{i=1}^nX_i$</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 71)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. <span class="defi">Sample covariance matrix</span> $S=begin{cases}cfrac{1}{n}sumlimits_{i=1}^n(X_i-overline X)(X_i-overline X)^T\cfrac{1}{n-1}sumlimits_{i=1}^n(X_i-overline X)(X_i-overline X)^Tend{cases}in S^{p}$
关于这个 n-1 是因为均值已知，无偏估计
3. ==Central Limit Theorem CLT==. Suppose $X_1,X_2,dots,X_minR^p text{ are i.d.d.},mathbb EX=muinR^p,Cov(X)=Sigma\qquadlimlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^n(X_i-mu)rightarrow N(0,Sigma)$
4. ==Law of Large numbers==. Suppose $X_1,X_2,dots,X_minR^p text{ are i.d.d.},EX_i=mu,Cov(X_i)=Sigma\qquadbegin{cases}limlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^nX_irightarrow E(X_i)=mu\limlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^n(X_i-mu)(X_i-mu)^TrightarrowSigmaend{cases}$</p>
<p>&gt; &gt; (AMA565_L0_T2) Suppose $e ～ N(0,σ^2I)$, what is the distribution of $hat{β} = (X^TX)^{−1}X^T(Xβ + e)$? (Assume that the inverse and matrix multiplication are well defined)</p>
<p>—</p>
<p>&gt; &gt; (AMA565_L0_T3) Suppose $X ～ N (μ, Σ)$, and let the eigenvalue decomposition of $Σ = (σ_{ij})_{1≤i,j≤p}$ be given as $Σ = ΓΛΓ′$ where Γ is an orthogonal matrix and $Λ = text{diag}{λ_1, … , λ_p}$ is the matrix of the eigenvalues.
&gt; &gt; 1). What is the distribution of $Γ′X$?&gt; &gt; 2). Let $Σ^{− 1/2} = ΓΛ^{− 1/2} Γ′.$ What is the distribution of $Σ^{− 1/2} X$?
&gt; &gt; 3). Suppose p = 2 and denote $X = (X_1, X_2)′$. In addition, $σ_{11}=σ_{22} = 1, σ_{12}=ρ$. What is the distribution of $(Y_1,Y_2), Y_1=cfrac{(X_1+X_2)}{sqrt{2+2ρ}}, Y_2 = cfrac{(X_1 − X_2)}{sqrt{2 − 2ρ}} ?$</p>
</section>
<section id="special-matrix">
<h2 id="special-matrix">Special Matrix<a class="headerlink" href="#special-matrix" title="Link to this heading">¶</a></h2>
<p>$A=11^T=begin{bmatrix}1&amp;1&amp;dots&amp;1\vdots&amp;ddots&amp;dots&amp;vdots\1&amp;dots&amp;dots&amp;1end{bmatrix}inR^p=pcdot begin{bmatrix}cfrac{1}{sqrt p}\vdots\cfrac{1}{sqrt{p}}end{bmatrix}*begin{bmatrix}cfrac{1}{sqrt p}&amp;dots&amp;cfrac{1}{sqrt{p}}end{bmatrix}+0…$</p>
<p><span class="defi">AR(1) model</span> $A=begin{bmatrix}1&amp;rho&amp;rho^2&amp;dots&amp;rho^{n-1}\rho&amp;1&amp;rho&amp;dots&amp;rho^{n-2}\vdots&amp;vdots&amp;vdots&amp;ddots&amp;vdots\rho^{n-1}&amp;rho^{n-2}&amp;dots&amp;dots&amp;1end{bmatrix} xrightarrow{e.g,}begin{bmatrix}1&amp;0.9&amp;0.9^2&amp;0.9^3\0.9&amp;1&amp;0.9&amp;0.9^2\0.9^2&amp;0.9&amp;1&amp;0.9\0.9^3&amp;0.9^2&amp;0.9&amp;1end{bmatrix}$</p>
</section>
<section id="variable-selection">
<h2 id="variable-selection">Variable Selection<a class="headerlink" href="#variable-selection" title="Link to this heading">¶</a></h2>
<p>📑 ref</p>
<ul class="simple">
<li><p>[如何进行特征选择（理论篇）机器学习你会遇到的“坑”]</p></li>
<li><p>[Are screening methods useful in feature selection? An empirical study]</p></li>
</ul>
<p>[Are screening methods useful in feature selection? An empirical study]: <a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220842">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220842</a>
[如何进行特征选择（理论篇）机器学习你会遇到的“坑”]:<a class="reference external" href="https://baijiahao.baidu.com/s?id=1604074325918456186&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1604074325918456186&amp;wfr=spider&amp;for=pc</a></p>
<p>特征
<a href="#id7"><span class="problematic" id="id8">|useful, important features|irreverent features|redundant feature|</span></a>
<a href="#id9"><span class="problematic" id="id10">|--|</span></a>–<a href="#id11"><span class="problematic" id="id12">|--|</span></a>
<a href="#id13"><span class="problematic" id="id14">|我们希望能：extract 他们，他们能主导整个 signals|无关特征。有他没他都一样|冗余特征。Delete|</span></a></p>
<p>==redundant variable==. that preditor with 0 coefficient  $beta_k=0implies$  unimportant and meaningless</p>
<p>Variable, Predictor, in the model, plays two roles: <strong>improving the model flexibility and adversely affecting the model stability</strong>. Redundant variables are not helpful in prediction, thus should be removed.</p>
<p><strong>为什么我们要 Figure out what important variables are and Delete redundant features？</strong></p>
<p>因为如果我们不处理掉 redundant variables， 那些 noises created by redundant variables maybe dominate the signals, causing trouble to ask for allocating those useful signals.
比如，通过房屋的面积，卧室的面积，车库的面积，所在城市的消费水平，所在城市的税收水平等特征来预测房价，那么消费水平（或税收水平）就是多余特征。证据表明，消费水平和税收水平存在相关性，我们只需要其中一个特征就够了，因为另一个能从其中一个推演出来。（如果是线性相关，那么我们在用线性模型做回归的时候，会出现严重的多**重共线性问题**，将会导致过拟合。）
特征选择还可以使模型获得更好的解释性，加快模型的训练速度，一般的，还会获得更好的性能</p>
<p><strong>常见的方法包括过滤法（Filter, Screening、包裹法（Warpper），嵌入法（Embedding）。</strong></p>
<section id="filter-screening">
<h3 id="filter-screening">Filter, Screening, 过滤法<a class="headerlink" href="#filter-screening" title="Link to this heading">¶</a></h3>
<p>选择一些 important features, which is a <strong>subset</strong> of all features, 但我们不是简单的选择，而是设置一个 threshold 尽量 save all signals。但 No free lunch, it just a trade.</p>
<p><strong>SAME Assumptions:</strong></p>
<p>$begin{cases}text{centered X: } mathbb EX=0,Cov(X)=Sigma, text{centered Y: } mathbb EY=0implies beta_0=0\(X_i,Y_i)text{ are IDD}\(X_i,Y_i,epsilon_i)text{ are independent}\red {Xi}:=text{error between estimator and truth}inR^n=begin{bmatrix}epsilon_1&amp;dots&amp;epsilon_nend{bmatrix}^T\mathbb EXi=0,Var(Xi)=sigma^2IinR^{ntimes n}end{cases}$</p>
<div class="line-block">
<div class="line-block">
<div class="line">| Perfect Models | More Redundant Variables | Less Important Variables |</div>
</div>
<div class="line">— | — | — | — |</div>
<div class="line">p | $p=p_0$ | $p&gt;p_0$ | $p&lt;p_0$ |</div>
<div class="line-block">
<div class="line">| Correct  | Correct  | Wrong |</div>
</div>
<div class="line">$epsilon_i$ | $N(0,sigma^2)$ | $N(0,sigma^2)$ | $red {tildeepsilon_i ,E(tildeepsilon_i)neq0}$ |</div>
<div class="line">$Bias(hat{Y}_{new})$ | ❌ | ❌ | ⭕ $Etilde{beta}-beta)^TX_{new}$ |</div>
<div class="line">$E{(Y-hat{Y})^2}$ | $approxsigma^2(1+cfrac{p_0}{n})$ | $approxsigma^2(1+cfrac{p}{n})$ | $approxsigma^2+(beta_{p+1}^2+dots+beta_{p_o^2})lambda_{min}(Sigma)$ |</div>
</div>
<section id="perfect-models-with-general-variables">
<h4 id="perfect-models-with-general-variables">Perfect Models with General Variables<a class="headerlink" href="#perfect-models-with-general-variables" title="Link to this heading">¶</a></h4>
<p>==Perfect model ($p=p_0$) variables==. $Y_i=beta_1x_{i,1}+dots+beta_{p_0}x_{i,p_0}+red{epsilon_i}$
$iff Y_i=beta^TX_i+epsilon_i, begin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p_0}end{bmatrix}^TinR^{p_0}\X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p_0}end{bmatrix}^TinR^{p_0}\[1em]Cov(X)=SigmainR^{p_0times p_0}\red{epsilon_itext{ are IID}～N(0,sigma^2)}end{cases}$
$implies Y=Xbeta+Xi,begin{cases}YinR^n\ XinR^{ntimes p_0 }=begin{bmatrix}X_1^T&amp;dots&amp;X_n^Tend{bmatrix}^T\betainR^{p_0}end{cases}$</p>
<p>==the estimator==. $ hat{beta} = (mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y=beta+(mathbb X^Tmathbb X)^{-1}mathbb X^T red {Xi }$
$\quad begin{cases}mathbb X:= text{training dataset with n traing samples } X_i\mathbb X^Tmathbb  X =sumlimits_{i=1}^nX_iX_i^T,\X_i text{ are IID }～N(0,Sigma)impliesmathbb  X^Tmathbb X=sumlimits_{i=1}^n(X_i-0)(X_i-0)^T=red{nSigma}\end{cases}$</p>
<p>$hat{beta}=beta+(n^{-1}mathbb X^Tmathbb X)^{-1}n^{-1}mathbb X^TXiapprox beta+Sigma^{-1}n^{-1}mathbb X^TXiimplies\[1em]hat{beta}-beta=(n^{-1}mathbb X^Tmathbb X)^{-1}n^{-1}mathbb X^TXiapproxSigma^{-1}n^{-1}mathbb X^TXi$</p>
<p>!!! p “前者 $(n^{-1}mathbb X^Tmathbb X)$ 是样本算出来的，后者  $Sigma$  是分布的方差，前者 converge into 后者，减少了 randomness，所以是 approximately”</p>
<p><strong>For a new random observation</strong> $X_{new}$
$$begin{align*}Y_{new}&amp;=beta^TX_{new}+epsilon_{new}\hat{Y}_{new} &amp;=hat{beta}^TX_{new}
end{align*}$$</p>
<p>$(X_{new},Y_{new},epsilon_{new})$ is independent of $(X_i,Y_i,epsilon_i)$</p>
<p>$$begin{align*}
Y_{new}-hat{Y}_{new}&amp;=underline{red {epsilon_{new}}+beta^{red{T}}X_{new}}-underline{hat{beta}^{red T}X_{new}}\
&amp;=red {epsilon_{new}}+(beta-hat{beta})^{red T}X_{new}\
&amp;approxred {epsilon_{new}}-underline{n^{-1}Xi^Tmathbb XSigma^{-1}}X_{new}
end{align*}$$</p>
<p>Square of Prediction Error</p>
<p>$$
begin{align*}(Y_{new}-hat{Y}_{new})^2&amp;approx(red {epsilon_{new}}-underline{n^{-1}Xi^Tmathbb XSigma^{-1}}X_{new})^2\[1em]&amp;approxred {epsilon_{new}}^2-2red {epsilon_{new}}cdot n^{-1}X_{new}^TSigma^{-1}mathbb X^TXi+n^{-2}(Xi^Tmathbb XSigma^{-1}X_{new})^2end{align*}
$$</p>
<p>Take expectation:</p>
<p>$$
begin{align*}mathbb E{(Y-hat{Y})^2}&amp;=mathbb E{(Y-hat{beta}^TX)^2}\&amp;approxmathbb EBig(red {epsilon}^2-2red {epsilon}cdot n^{-1}X^TSigma^{-1}mathbb X^TXi+n^{-2}(Xi^Tmathbb XSigma^{-1}X)^2Big)\&amp;approxsigma^2-0+cfrac{p_0}{n}sigma^2\&amp;approxsigma^2(1+cfrac{p_0}{n})end{align*}
$$</p>
<p>$implies$ <strong>When n is large, the perfect model has the smallest prediction error</strong></p>
</section>
<section id="working-model-with-more-redundant-variables">
<h4 id="working-model-with-more-redundant-variables">Working Model with More Redundant Variables<a class="headerlink" href="#working-model-with-more-redundant-variables" title="Link to this heading">¶</a></h4>
<p>如果我们不选择重要特征，我们尝试为折有事物添加估计系数，那么误差将汇总权将汇总，即每次我们估计某些东西时，您都会创一个错误。</p>
<p><span class="defi">Wroking model with more redundant variables ($p$ variables, $red{p&gt;p_0}$)</span></p>
<blockquote>
<div><p>$Y_i=beta_1x_{i,1}+dots+beta_{p_0}x_{i,p_0}+0times x_{i,p_0}+dots+0times x_{i,p}+red{epsilon_i},$
$iff Y_i=beta^TX_i+epsilon_i,i=1,dots,n, \qquadbegin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p_0}&amp;0&amp;dots&amp;0end{bmatrix}^TinR^p,X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p_0}&amp;x_{i,p_0}&amp;dots&amp;x_{i,p}end{bmatrix}^TinR^p\Cov(X)=SigmainR^{ptimes p}\red{epsilon_itext{ are IID}～N(0,sigma^2)}end{cases}$</p>
</div></blockquote>
<p><span class="defi">the estimator</span> $hat{beta} = (mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y=beta+(mathbb X^Tmathbb X)^{-1}mathbb X^T red {Xi }\quad begin{cases}mathbb XinR^{ntimes p},mathbb X^Tmathbb  X =sumlimits_{i=1}^nX_iX_i^TinR^{ptimes p},\X_i text{ are IID }～N(0,Sigma)impliesmathbb  X^Tmathbb X=sumlimits_{i=1}^n(X_i-0)(X_i-0)^T=red{nSigma}inR^{ptimes p}\end{cases}$</p>
<p>Expected Square of Prediction Error</p>
<p>$$mathbb E{(Y-hat{Y})^2}=mathbb E{(Y-hat{beta}^TX)^2}approxsigma^2(1+cfrac{p}{n})&gt;sigma^2(1+cfrac{p_0}{n}),p&gt;p_0
$$</p>
<p>proof of</p>
<p>$implies$  <strong>the more redundant variables, the worse the prediction is.</strong>
$implies$  <strong>if n is very large, it is ok to put all features in model as well since $cfrac{p_0}{n}$  would vanish</strong></p>
</section>
<section id="working-model-with-less-important-variables">
<h4 id="working-model-with-less-important-variables">Working Model with Less Important Variables<a class="headerlink" href="#working-model-with-less-important-variables" title="Link to this heading">¶</a></h4>
<p><span class="defi">Wroking model with less important variables ($p$ variables, $red{p&lt;p_0, beta_{p_0}neq0}$</span>
$Y_i=beta_1x_{i,1}+dots+beta_ptimes x_{i,p}+red{tilde{epsilon_i}}$
$iff Y_i=beta^TX_i+red{tilde{epsilon_i}},i=1,dots,n, \qquadbegin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p}end{bmatrix}^TinR^p,X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p}end{bmatrix}^TinR^p\Cov(X)=SigmainR^{ptimes p}\red{!!E(tilde{epsilon})neq 0impliedbytext{wrong model without enough variables}}\lambda_{min}(Sigma)&gt;0end{cases}$</p>
<p><span class="defi">the estimator</span> $hat{beta}=begin{bmatrix}hat{beta_1}&amp;dots&amp;hat{beta_p}end{bmatrix}^TinR^p,red{p&lt;p_0}\xrightarrow{变形}red{tilde{beta}}=begin{bmatrix}hat{beta}\0end{bmatrix}=begin{bmatrix}hat{beta_1}&amp;dots&amp;hat{beta_p}&amp;0&amp;dots&amp;0end{bmatrix}^TinR^{p_0}$</p>
<p>$implieshat{Y}_{new}=hat{beta}_{1times p}{X_{new}}_{ptimes1}=tilde{beta}^T_{1times p_0}{X_{new}}_{p_0times1}\qquadqquad=hatbeta_1x_1+dots+hat{beta_p}x_p+0times x_p+dots+0times x_{p_0}$</p>
<p><strong>There is a bias in the prediction for a given X.</strong></p>
<p>$Bias(hat{Y}_{new})=mathbb EY_{new}-mathbb Ehat{Y}_{new}=mathbb E{(beta-tilde{beta})^TX_{new}+epsilon_{new}}=(mathbb Etilde{beta}-beta)^TX_{new}$</p>
<p>Expected Square of Prediction Error</p>
<p>$$
begin{align*}mathbb E{(Y-hat{Y})^2}&amp;=sigma^2+mathbb E{(beta-tilde{beta})^TSigma(beta-tilde{beta})}\[1em]&amp;gesigma^2+Vertbeta-tildebetaVert^2lambda_{min}(Sigma)\[1em]&amp;gesigma^2+(beta_{p+1}^2+dots+beta_{p_0^2})lambda_{min}(Sigma)end{align*}
$$</p>
<ul class="simple">
<li><p>proof of</p></li>
</ul>
<p>$implies$  If the working model does not include all the important variables (those with $β_{k}≠ 0,k=1,dots,p_0$), the prediction error (lower bound) is also bigger than the model with exactly the important variables.
$implies$ <strong>样本数 n 再大也拯救不了这个error 因为$(beta_{p+1}^2+dots+beta_{p_0^2})lambda_{min}(Sigma)$ is constant，而且这只是下界 lower bound</strong></p>
</section>
<section id="candidate-models-for-p-1-predictor-1-x-1-x-p">
<h4 id="candidate-models-for-p-1-predictor-1-x-1-x-p">Candidate Models for p+1 predictor $1， x_1, …, x_p$<a class="headerlink" href="#candidate-models-for-p-1-predictor-1-x-1-x-p" title="Link to this heading">¶</a></h4>
<p>&lt;figure markdown=”span”&gt;![](./pics/FS_1.png){width=80%}&lt;p&gt;un-centralized&lt;/p&gt;&lt;/figure&gt;</p>
<p>Suppose we have n samples. Consider any sub-model (A)
$(A):= Y =beta_0+β_1x’_1 +…+β_qx’_q+ε, space{x’_1,dots,x’_q}sub{x_1,dots,x_p}$</p>
<p>==RRS of A==. $RSS(A)=sumlimits_{i=1}^n{Y_i-hat{Y}_{A,i}}^2$.
$hat{Y}_{A,i}:=$ the fitted value(estimation) of $Y_i$ generated by model (A</p>
<p>!!! warning “Fitted error (RSS) 可以去衡量how good model are， 但是 cannot be used as one criterion for the selection.”</p>
<ol class="arabic simple">
<li><p>For any two models A and B, if A is a sub-model of B, then $RSS(A) ≥ RSS(B).$ 只要 A是 B 的子集，那么 RSS(A) 一定≥ RSS(B)。</p></li>
<li><p>而且这个RSS是在 training set 1-n 上进行，如果是 overfitting 的话，error再小，但是在鲁棒性还是很垃圾的。所以我们不能用在训练集上的RSS去 compare</p></li>
</ol>
</section>
</section>
<section id="model-selection-for-lr">
<h3 id="model-selection-for-lr">Model Selection for LR<a class="headerlink" href="#model-selection-for-lr" title="Link to this heading">¶</a></h3>
<p>For example, an empirical method like Cross-Validation, Bootstrap methods or sample penalties such as AIC, BIC, Mallow’s CP.</p>
<p>[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]</p>
<p>==Cross validation==. 因为要比较一些模型，如果每个模型都拿一些进行训练然后测验证集的准确率。当训练集的 n 非常大的时候，就很容易 time-consuming。</p>
<p>==K-fold==. 和交叉验证比，是 computation more efficient， 但更model is less stable</p>
<p><span class="defi">AIC</span></p>
<p>/BIC</p>
<p>the computational efficiency of AIC/BIC or when the sample size is relatively small for cross-validation
AIC and BIC explicitly penalize the number of parameters, cross-validation not, so again, it’s not surprising that they suggest a model with fewer parameters (though nothing prohibits cross-validation from picking a model with fewer parameters).</p>
<p>[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]: <a class="reference external" href="https://stats.stackexchange.com/questions/578982/model-selection-aic-bic-and-cross-validation-gives-different-conclusion">https://stats.stackexchange.com/questions/578982/model-selection-aic-bic-and-cross-validation-gives-different-conclusion</a></p>
</section>
</section>
<section id="dimensionality-reduction">
<h2 id="dimensionality-reduction">Dimensionality Reduction，数据降维<a class="headerlink" href="#dimensionality-reduction" title="Link to this heading">¶</a></h2>
<p>数据降维其实还有另一个好处：数据可视化。因为超过三维的数据就无法可视化了。数据降维最常用的方法是主成分分析。</p>
<dl class="simple">
<dt>!!! danger “我们想找到重要信号的位置。其次，我们找到重要信号或强信号，或者这些弱信号现在是零，就扔掉，我们希望为这些强信号提供适当的估计。”</dt><dd><p>我们肯定没有 enough information，因为同时存在着 noise。决定我们是否能完成目标就是：whether the important signals in data are stronger than noises。我们现在假定这个 important signals are <strong>stronger</strong> than noises. 接下来就要想如何将important information 剥离 noise？</p>
</dd>
</dl>
<section id="principal-component-analysis-pca">
<h3 id="principal-component-analysis-pca">Principal Component Analysis, PCA, 主成分分析<a class="headerlink" href="#principal-component-analysis-pca" title="Link to this heading">¶</a></h3>
<p>The basic idea is to transform the p random variables into &lt;u&gt;linear combinations&lt;/u&gt; called ==Principal Components==. Extracting linear combinations from multivariate data, a subset of PCs &lt;u&gt;captures most of the variability &lt;/u&gt; in the data.</p>
<p>正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，<a href="#id1"><span class="problematic" id="id2">**</span></a>线性无关**的变量称为:defi:<cite>主成分</cite></p>
<aside class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 273); <em><a href="#id2">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<section id="key-maximize-the-variance">
<h4 id="key-maximize-the-variance">KEY: Maximize the variance<a class="headerlink" href="#key-maximize-the-variance" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>!!! p “Maximize the variance”</dt><dd><p>Identify key components which can &lt;u&gt;maximize the information&lt;/u&gt; with a reasonable dimension.
Find <strong>unit-vector g</strong> to transform X into Y with the target that &lt;u&gt;maximizes the variance of Y&lt;/u&gt;.</p>
</dd>
</dl>
<p>Suppose $X=begin{bmatrix}X_1\vdots\X_pend{bmatrix}inR^p$ is a random vector with <span class="defi">Covariance Matrix</span> $Var{X}=Sigma=begin{bmatrix}sigma_{1}^2&amp;dots&amp;sigma_{1p}\vdots&amp;ddots&amp;vdots\sigma_{p1}&amp;dots&amp;sigma_{p}^2\end{bmatrix}$</p>
<p>Look for &lt;u&gt;the linear transformations&lt;/u&gt;:</p>
<p>$$
begin{align*}begin{cases}Y_1=g_{11}X_1+dots+g_{1p}X_p\Y_2=g_{21}X_1+dots+g_{2p}X_p\vdots\Y_p=g_{p1}X_1+dots+g_{pp}X_pend{cases}iffbegin{bmatrix}Y_1\vdots\Y_pend{bmatrix}&amp;=begin{bmatrix}g_{11}&amp;dots&amp;g_{1p}\vdots\g_{p1}&amp;dots&amp;g_{pp}end{bmatrix}begin{bmatrix}X_1\vdots\X_pend{bmatrix}\&amp;=begin{bmatrix}g_1^T&amp;dots&amp;g_p^T\end{bmatrix}begin{bmatrix}X_1\vdots\X_pend{bmatrix}end{align*}\vec{g_i}=begin{bmatrix}g_{i1}\vdots\g_{ip}end{bmatrix},Vert vec{g_i}Vert_2=1,forall i=1,…p
$$</p>
<p><span class="defi">property of  r.vector</span> $Y=g^TX, gintext{constant}\implies Var(Y)=g^TVar(X)g=g^TSigma g$</p>
<p><strong>Target:</strong></p>
<p>$$g=maxlimits_{Vert gVert_2=1} Var(Y)=maxlimits_{Vert gVert_2=1} g^TSigma gtag{1}$$</p>
<p>==eign about postive definite A==.$Sigma=sumlimits_{i=1}^{p}lambda_igamma_i^Tgamma_i=Gamma^TLambdaGamma$  specially suppose $lambda_1&gt;dots&gt;lambda_p&gt;0$ in &lt;u&gt;anascending order&lt;/u&gt;</p>
<p>1. $g_1:=maxlimits_{Vert gVert_2=1} g^TSigma gimplies g_1=gamma_1 text{ of }Sigma$
$implies g_1$ is the <strong>direction</strong> <strong>where the variance is maximized.</strong> &lt;u&gt;（not PC&lt;/u&gt;
$implies g_1^Tx$ is the <strong>1st PC</strong>
2. $g_2:=maxlimits_{Vert gVert_2=1,:g_2^Tg_1=0} g^TSigma gimplies g_2=gamma_2 text{ of }Sigma$
$implies g_2$ is the one that maximizes the variance among all directions <strong>orthogonal</strong> to $g_1$
3. proof: <strong>i th-PC = i th eigen vector</strong>
Let $lambda_i:=$ the i-th largest eigen-value of $Sigma$, $gamma_i:=$ the eigen-vector corresponding to $lambda_i$. Therefore, ${gamma_1,gamma_n}$ are one set of the basis of $R^p$ &lt;u&gt;特征向量是特征空间的一组 basic vectors。&lt;/u&gt;
$impliesforall ginR^p,exist c_1, c_2,dots, c_ptext{ s.t. } g=c_1gamma_1+c_2gamma_2+c_ngamma_n$</p>
<p>$$begin{align*}implies g^TSigma g&amp;=sumlimits_{i=1}^nsumlimits_{j=1}^nc_ic_jgamma_i^TSigmagamma_j\
&amp;xlongequal{Sigmagamma_i=lambda_igamma_i}sumlimits_{i=1}^nsumlimits_{j=1}^pc_ic_jgamma_i^Tgamma_jlambda_j\
&amp;xlongequal[lambda_i^Tlambda_i=1]{lambda_i^Tlambda_j=0,forall ineq j} sum_{i=1}^nc_i^2lambda_i\
&amp;lelambda_1sum_{i=1}^nc_i^2\
&amp;xlongequal[Vert gVert_2^2=1=c_1^2+dots+c_n^2]{text{when }c_2=dots=c_n=0 }lambda_1
end{align*}$$</p>
<p>$$implies g=gamma_1iff g^TSigma g=lambda_1$$</p>
<p><strong>Conclusion:</strong></p>
<ol class="arabic simple">
<li><p>the best direction is <strong>the direction of eigenvectors</strong></p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 319)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. the <strong>variance</strong> of the direction is <strong>Eigen value</strong>
$Var(Y_i)=g_i^TSigma g_i=g_i^Tlambda_i g_i = lambda_i\tr(Sigma)=tr(GammaLambdaGamma^T)=tr(LambdaGammaGamma^T)=tr(Lambda)=sumlimits_{i=1}^plambda_i$
3. What is the relationship between $Y_i &amp; Y_j, ineq j$
<strong>orthogonal and are the Eigen-vector of Sigma</strong>
4. What if X is following a multivariate normal distribution?
<span class="defi">Multivariate Normal Distribution</span> $Z～N(b,Sigma),forall linR^p,l^TZinR～$ Normal distri… $implies Z$ ～ Multivariate Normal Distri…
If X is following a multivariate normal distribution, $g_i^TX～N$</p>
<p><strong>Advantages:</strong></p>
<ol class="arabic simple">
<li><p>Identify <strong>key components</strong> which can <strong>maximize the information</strong> with a reasonable dimension. 发现数据中的基本结构，即数据中变量之间的关系,能近似地表达</p></li>
<li><p><strong>Reduce the dimension</strong> of other forms of analysis.</p></li>
<li><p>Linearity is assumed.</p></li>
</ol>
<p><strong>Limits:</strong></p>
<ol class="arabic simple">
<li><p>It can be <strong>more difficult to interpret</strong> than using a subset of the original variables.</p></li>
<li><p>It uses only covariances/correlations but not higher-order moments. This can be extended to <span class="defi">independent component analysis ICA</span></p></li>
</ol>
</section>
<section id="pca-transformation">
<h4 id="pca-transformation">PCA Transformation<a class="headerlink" href="#pca-transformation" title="Link to this heading">¶</a></h4>
<p>$Y_i = g_i^TXimplies Y=Gamma^TX$</p>
<p>the 1 PC $Y_1=g_1^TX, g_1:= $ the eigen-vector with the 1 largest eigen-value $lambda_1$
the 2 PC $Y_2=g_2^TX, g_2:=$ the eigen-vector with the 2 largest Eigen-value $lambda_2$
$$vdots\
begin{bmatrix}Y_1\vdots\Y_pend{bmatrix}=begin{bmatrix}g_1^T\vdots\g_p^Tend{bmatrix}_{ptimes p}begin{bmatrix}X_1\vdots\X_pend{bmatrix}iff Gamma=begin{bmatrix}g_1&amp;dots&amp;g_pend{bmatrix}inR^{ptimes p}$$</p>
<section id="practical-use">
<h5 id="practical-use">Practical Use<a class="headerlink" href="#practical-use" title="Link to this heading">¶</a></h5>
<ol class="arabic simple">
<li><p>&lt;u&gt;**Standardize 规范化**&lt;/u&gt; the data, 使得数据每一变量的均值为0，方差为1</p></li>
<li><p>&lt;u&gt;**SVD 正交分解**&lt;/u&gt; of the sample <strong>covariance M/correlation M</strong></p></li>
<li><p><strong>Sort</strong> the eigenvalues in descending order and choose the K largest eigenvectors (plots, the proportion of variances interpreted etc.)</p></li>
<li><p>&lt;u&gt;**Linear Transform 变换**&lt;/u&gt; X into Y(Dimension Reduction!)</p></li>
</ol>
<p>原来由线性相关变量表示的数据，通过正交变换变成由若干个线性无关的新变量表示的数据。新变量是可能的正交变换中变量的方差的和（信息保存）最大的，<strong>方差表示在新变量上信息的大小</strong>。将新变量依次称为第一主成分、第二主成分等。</p>
<dl class="simple">
<dt>!!! p “<strong>how large dimensions we keep：</strong>”</dt><dd><p>这里所说的**信息是指原有变量的方差**。
可以设置一个 certain level, maybe 85%，90%，然后我们看 &lt;u&gt;the cumulative proportion of k PCs&lt;/u&gt;, 因为这就是说 这k 个 PCs 能够在多大程度上描述这些点的差异间隔。而在剩下的PCs里，这些点大多是重叠，并不具有很高的差异信息价值。只要这个 cumulative达到了这个 certain level 我们就采用前 k 个 PCs</p>
</dd>
<dt>!!! p “covariance M vs correlation M? —— &lt;kbd&gt;Scale&lt;/kbd&gt;”</dt><dd><p>==covariance M==. $Cov(x,y)=cfrac{1}{n}sumlimits_{i=1}^n(x-overline{x})(y-overline{y})$
==correlation M==. $rho_{xy}=cfrac{Cov(x,y)}{sqrt{Var(x)}sqrt{Var(y)}}$
[Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]
当我们要去除**特征值量纲的区别** 我们使用 correlation，&lt;kbd&gt;scale=true&lt;/kbd&gt;
如果不去，就是covariance，&lt;kbd&gt;scale=false&lt;/kbd&gt;（默认
&gt; &gt; Suppose $x_1$ is a length measured either in cm. or mm., $x_2$ is a weight measurement in gm. The covariance M with $y_1$ in cm. is $begin{bmatrix}80&amp;44\44&amp;80end{bmatrix}$ the Covariance M with $y_1$ in mm. is $begin{bmatrix}8000&amp;440\440&amp;80end{bmatrix}$</p>
</dd>
</dl>
<p>[Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]:<a class="reference external" href="https://www.simplilearn.com/covariance-vs-correlation-article">https://www.simplilearn.com/covariance-vs-correlation-article</a></p>
</section>
</section>
<section id="graphical-rotate-the-data-without-scaling">
<h4 id="graphical-rotate-the-data-without-scaling">Graphical: Rotate the data without scaling<a class="headerlink" href="#graphical-rotate-the-data-without-scaling" title="Link to this heading">¶</a></h4>
<p>数据集合中的样本由实数空间（正交坐标系）中的点表示，空间的一个坐标轴表示一个变量，规范化处理后得到的数据**分布在原点附近**。对原坐标系中的数据进行主成分分析等价于进行**坐标系旋转变换，将数据投影到新坐标系的坐标轴上。**</p>
<dl>
<dt>!!! p “<strong>Graphical:</strong> $Y=Gamma^TX$: Multiplication by an orthogonal matrix: <strong>Rotation</strong>!”</dt><dd><p>&lt;u&gt;proof of rotation&lt;/u&gt;: $Vert y_1-y_2Vert_2=Vert x_1-x_2Vert_2, $ without scaling $forall x_1,x_2,$ any two points in space</p>
<p>$$begin{align*}Vert y_1-y_2Vert_2^2&amp;=(y_1-y_2)^T(y_1-y_2)\&amp;=(x_1-x_2)^TGammaGamma^T(x_1-x_2)\&amp;xlongequal[GammaGamma^T=I]{Gamma^T=Gamma^{-1}}(x_1-x_2)^T(x_1-x_2)=Vert x_1-x_2Vert_2^2end{align*}$$</p>
</dd>
</dl>
<p>新坐标系的第一坐标轴、第二坐标轴等分别表示第一主成分、第二主成分等，数据在每一轴上的坐标值的平方表示相应变量的方差；并且，这个坐标系是在所有可能的新的坐标系中，<strong>坐标轴上的方差的和最大的</strong></p>
<p><strong>方差和最大:</strong>
主成分分析旨在选取正交变换中方差最大的变量，作为第一主成分，旋转变换中**坐标值的平方和最大**的轴, 旋转变换中选取**离样本点的距离平方和最小**的轴</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/PCA_1.png){width=45%}&lt;p&gt;transformation：&lt;u&gt;旋转变换&lt;/u&gt;&lt;/p&gt;&lt;figure&gt;</p>
<p>&lt;p&gt;原坐标系:很明显有正向线性相关，13象限最多&lt;br&gt;新坐标系:线性无关，1234象限都差不多&lt;/p&gt;
&lt;/div&gt;</p>
<p>如果主成分分析只取第一主成分，即新坐标系的y1轴，那么等价于将数据投影在椭圆长轴上，用这个主轴表示数据，将二维空间的数据压缩到一维空间中。</p>
<div class="line-block">
<div class="line">1st PC | 2nd PC |</div>
<div class="line">— | — |</div>
<div class="line">方差最大 | 与第一坐标轴正交，且方差次之 |</div>
<div class="line">第一坐标轴 $y_1$ | 第二坐标轴 $y_2$ |</div>
<div class="line">椭圆的长轴 | 椭圆的短轴 |</div>
</div>
</section>
</section>
<section id="linear-discriminant-analysis-lda">
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis, LDA, 线性判别分析<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Link to this heading">¶</a></h3>
<p>LDA的目标是**提取一个新的坐标系，将原始数据集投影到一个低维空间中。**
和PCA的主要区别在于，LDA不会专注于数据的方差，而是优化低维空间，<strong>以获得最佳的类别可分性</strong>。意思是，新的坐标系在为分类模型查找**决策边界**时更有用，&lt;u&gt;非常适合用于构建分类流水线&lt;/u&gt;。</p>
<p><a href="#id3"><span class="problematic" id="id4">**</span></a>优点：<a href="#id5"><span class="problematic" id="id6">**</span></a>基于类别可分性的分类
- 有助于避免机器学习流水线的过拟合，也叫防止维度诅咒。
- LDA也会降低计算成本。</p>
<aside class="system-message" id="id3">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 407); <em><a href="#id4">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<aside class="system-message" id="id5">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 407); <em><a href="#id6">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<p>!!! danger “Fisher’s LDA and Bayes’ LDA are essentially different! They are equivalent under the &lt;u&gt;Gaussian assumption with a common Σ for the two-class case&lt;/u&gt;”</p>
<p>Both Fisher’s LDA and Bayes rule reduce to:</p>
<div class="line-block">
<div class="line-block">
<div class="line">| empirical estimators |</div>
</div>
<div class="line">— | — |</div>
<div class="line">$μ_X$ | $overline{X}$ |</div>
<div class="line">$μ_Y$ | $overline{Y}$ |</div>
<div class="line">$Σ$ | $Σ$ |</div>
</div>
<dl class="simple">
<dt>!!! danger “不可将线性判别分析与:defi:<cite>隐狄利克雷分配LatentDirichlet Allocation, LDA</cite> 相混淆。”</dt><dd><p>隐狄利克雷分配用于文本和自然语言处理，与线性判别分析没有关系。</p>
</dd>
</dl>
<div class="line-block">
<div class="line">类内 within-class |  类间 between-class</div>
<div class="line">— | — |
S_w | S_b</div>
</div>
<section id="fishers-lda">
<h4 id="fishers-lda">Fisher’s LDA<a class="headerlink" href="#fishers-lda" title="Link to this heading">¶</a></h4>
<p>LDA的目标是提取一个新的坐标系，将原始数据集投影到一个低维空间中，以获得最佳的类别可分性。</p>
<p><strong>Target：</strong> 获得最佳的类别可分性
Find the line $P_Z = w^TZ$ that best separates the two classes.
$$w =maxlimits_w cfrac{w^TS_Bw}{w^TS_Ww}$$
Force $begin{cases}S_B=(mu_X-mu_Y)(mu_X-mu_Y)^T&amp;text{ between-class}uparrow\S_W=Sigma&amp;text{ within-class}downarrowend{cases}$</p>
<ol class="arabic simple">
<li><p>the center of the two after transformation linear projection be as <strong>far</strong> away as possible $S_Buparrow$</p></li>
<li><p>the variance of two classes to be as <strong>small</strong> as possible  $S_w downarrow$</p></li>
</ol>
<p>&lt;figure markdown=”span”&gt;![](./pics/LDA_1.png){width=60%}&lt;p&gt;LDA:最佳的类别可分性&lt;br&gt;假设：正态分布&lt;/p&gt;&lt;/figure&gt;</p>
<p>[機器學習: 降維(Dimension Reduction)- 線性區別分析( Linear Discriminant Analysis)]</p>
</section>
<section id="bayes-lda">
<h4 id="bayes-lda">Bayes’ LDA<a class="headerlink" href="#bayes-lda" title="Link to this heading">¶</a></h4>
<p>!!! p “贝叶斯的优点：不需要知道具体的分布”</p>
<p>$f_X (·):=$ pdf  for Class-X, $f_Y (·) := $ pdf  for Class-Y</p>
<p>==Bayes rule==. $δ(Z) = I{π_1f_X(Z) &gt; π_2f_Y(Z)}$.
$begin{cases}pi_i text{ prior possibility },pi_1+pi_2=1 \f_i(Z)text{ likelihoood function}end{cases}$</p>
<p>For simplicity let’s assume that $π_1 = π_2 = 1/2.$ without any assumption.</p>
<p>$δ(Z) =I {[Z−(μ_X+mu_Y)/2]^T Σ^{−1}(μ_X−mu_Y)&gt;0}\hat{δ}(Z) =I {[Z−(overline{X}+overline{Y} )/2]^T hat{Σ}^{−1}(overline{X} − overline{Y})&gt;0}$
Z will be assigned as $ begin{cases}X&amp;δ(Z) =1\Y&amp;δ(Z) =0end{cases}$</p>
</section>
</section>
<section id="quadratic-discriminant-analysis-qda">
<h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis, QDA<a class="headerlink" href="#quadratic-discriminant-analysis-qda" title="Link to this heading">¶</a></h3>
<p>Assume: $Y_1∼N(μ_1,Σ_1), Y_2∼N(μ_2,Σ_2)$ The two classes have <strong>different</strong> covariance matrices!</p>
<p>$delta(x) = (x−μ)^TΩ(x−μ)+δ^T(x− μ)+η\begin{cases}μ=(μ_1+ μ_2)/2spacetext{ (mean)}\Omega=Sigma_2^{-1}-Sigma^{-1}spacetext{ the difference of the two precision matrices}\delta=(Sigma_1^{-1}+Sigma_2^{-1})(mu_1-mu_2)\η=2log(pi_1/pi_2)+frac{1}{4}(mu_1-mu_2)^TOmega(mu_1-mu_2)+log|Sigma_2|-log|Sigma_1|end{cases}$</p>
<p>因为是关于x的二次函数，所以是 quadratic 二次</p>
<p>[](<a class="reference external" href="https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b">https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b</a>)</p>
<p>&gt; &gt; (T1 in Chap1.1 in AMA565) Suppose the covariance M of a p-dimensional random vector X is $Σ = text{diag}{1, 2, … , p}.$ What are the Principal Components of X?
&gt; &gt; (T2 in Chap1.1 in AMA565) <em>Suppose the covariance matrix of a p-dimensional random vector X is $Σ = {11}^T$. What are the Principal Components of X?
&gt; &gt; (T3 in Chap1.1 in AMA565) *Suppose $X ∼ N(μ_1,Σ),Y ∼ N(μ_2,Σ).:f_1(x):=text{ pdf of }X, f_2(x):= text{ pdf of }Y$, and let π_i be the prior probability that X is coming from class i, i = 1, 2. Show that $π_1f_1(x)/[π_2f_2(x)] &gt; 1iff (Σ^{−1}(μ_1 − μ_2))T (x − (μ_1 + (μ_2)/2)) &gt; c$ Derive c.</em></p>
<p>[機器學習: 降維(Dimension Reduction)- 線性區別分析( Linear Discriminant Analysis)]: <a class="reference external" href="https://chih-sheng-huang821.medium.com">https://chih-sheng-huang821.medium.com</a>/機器學習-降維-dimension-reduction-線性區別分析-linear-discriminant-analysis-d4c40c4cf937</p>
</section>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>