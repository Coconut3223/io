<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>high dimentional DA &#8212; cocobook  æ–‡æ¡£</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/def.css?v=5a9d86bd" />
    <script src="../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="ç´¢å¼•" href="../genindex.html" />
    <link rel="search" title="æœç´¢" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#AI/highd" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="cocobook  æ–‡æ¡£"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">cocobook  æ–‡æ¡£</span>
          <span class="md-header-nav__topic"> high dimentional DA </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">cocobook  æ–‡æ¡£</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="cocobook æ–‡æ¡£" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="cocobook æ–‡æ¡£">cocobook  æ–‡æ¡£</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#ai-highd--page-root" class="md-nav__link">high dimentional DA</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#some-special-notation" class="md-nav__link">some special notation</a>
        </li>
        <li class="md-nav__item"><a href="#special-matrix" class="md-nav__link">Special Matrix</a>
        </li>
        <li class="md-nav__item"><a href="#variable-selection" class="md-nav__link">Variable Selection</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#filter-screening" class="md-nav__link">Filter, Screening, è¿‡æ»¤æ³•</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#perfect-models-with-general-variables" class="md-nav__link">Perfect Models with General Variables</a>
        </li>
        <li class="md-nav__item"><a href="#working-model-with-more-redundant-variables" class="md-nav__link">Working Model with More Redundant Variables</a>
        </li>
        <li class="md-nav__item"><a href="#working-model-with-less-important-variables" class="md-nav__link">Working Model with Less Important Variables</a>
        </li>
        <li class="md-nav__item"><a href="#candidate-models-for-p-1-predictor-1-x-1-x-p" class="md-nav__link">Candidate Models for p+1 predictor $1ï¼Œ x_1, â€¦, x_p$</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#model-selection-for-lr" class="md-nav__link">Model Selection for LR</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#dimensionality-reduction" class="md-nav__link">Dimensionality Reductionï¼Œæ•°æ®é™ç»´</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#principal-component-analysis-pca" class="md-nav__link">Principal Component Analysis, PCA, ä¸»æˆåˆ†åˆ†æ</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#key-maximize-the-variance" class="md-nav__link">KEY: Maximize the variance</a>
        </li>
        <li class="md-nav__item"><a href="#pca-transformation" class="md-nav__link">PCA Transformation</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#practical-use" class="md-nav__link">Practical Use</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#graphical-rotate-the-data-without-scaling" class="md-nav__link">Graphical: Rotate the data without scaling</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#linear-discriminant-analysis-lda" class="md-nav__link">Linear Discriminant Analysis, LDA, çº¿æ€§åˆ¤åˆ«åˆ†æ</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#fishers-lda" class="md-nav__link">Fisherâ€™s LDA</a>
        </li>
        <li class="md-nav__item"><a href="#bayes-lda" class="md-nav__link">Bayesâ€™ LDA</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#quadratic-discriminant-analysis-qda" class="md-nav__link">Quadratic Discriminant Analysis, QDA</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/AI/highd.rst.txt">æ˜¾ç¤ºæºä»£ç </a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="high-dimentional-da">
<h1 id="ai-highd--page-root">high dimentional DA<a class="headerlink" href="#ai-highd--page-root" title="Link to this heading">Â¶</a></h1>
<p><span class="defi">Data mining</span> is the process of discovering new patterns from LARGE DATA sets using methods of artificial intelligence, machine learning, statistics and database systems.</p>
<p>==Curse of Dimensionality ç»´åº¦ç¾éš¾==ã€‚ ä¼šå¯¼è‡´åˆ†ç±»å™¨å‡ºç°**è¿‡æ‹Ÿåˆ**ã€‚è¿™æ˜¯å› ä¸º&lt;u&gt;åœ¨æ ·æœ¬å®¹é‡å›ºå®šæ—¶ï¼Œéšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œå•ä½ç©ºé—´ä¸­çš„æ ·æœ¬æ•°é‡ä¼šå˜å°‘ã€‚&lt;/u&gt;æ°å½“çš„ç»´æ•°ç‰¹å¾æ•°å¯¹äºæœºå™¨å­¦ä¹ æ¨¡å‹éå¸¸é‡è¦ã€‚æ·±åº¦å­¦ä¹ é€šè¿‡å¯¹æ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå¤æ‚çš„å˜æ¢ï¼Œå¾—åˆ°å¯¹ç±»åˆ«æœ€æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä»è€Œæé«˜æœºå™¨å­¦ä¹ çš„æ€§èƒ½ã€‚</p>
<p>&lt;div class=â€gridâ€ markdown&gt;
&lt;figure markdown=â€spanâ€&gt;![](./pics/highD_1.png){width=60%}&lt;figure&gt;
&lt;figure markdown=â€spanâ€&gt;![](./pics/highD_2.png){width=60%}&lt;figure&gt;
&lt;p&gt;å‡è®¾æ ·æœ¬é›†æ˜¯ç”±åœ†å½¢å’Œä¸‰è§’å½¢ç»„æˆçš„20ä¸ªæ ·æœ¬ï¼Œå‡è®¾è¿™äº›æ ·æœ¬å‡åŒ€åœ°åˆ†å¸ƒåœ¨è¿™4ä¸ªåŒºåŸŸï¼Œåˆ™æ¯ä¸ªåŒºåŸŸçš„æ ·æœ¬ä¸ªæ•°çº¦ä¸º5ä¸ªã€‚è‹¥å¸Œæœ›åœ¨äºŒç»´ç©ºé—´ä¸­æ¯ä¸ªåŒºåŸŸçš„æ ·æœ¬æ•°é‡ä¸ä¸€ç»´æ—¶å¤§è‡´ç›¸ç­‰ï¼Œåˆ™éœ€è¦400ä¸ªæ ·æœ¬ï¼›è‹¥æ˜¯ä¸‰ç»´ç©ºé—´ï¼Œåˆ™éœ€è¦8000ä¸ªæ ·æœ¬&lt;/p&gt;
&lt;/div&gt;</p>
<dl class="simple">
<dt>!!! danger â€œå¾ˆå°‘ <strong>observation nï¼Œ</strong> å¾ˆå¤š <strong>features p</strong> æƒ…å†µä¸‹çš„é«˜ç»´.&lt;br&gt; p is very large, but n is relatively small.â€</dt><dd><p>å°±æ‹¿åŒ»å­¦æ¥è¯´ï¼Œç—…äººæ€»æ˜¯å°‘æ•°çš„ï¼Œä½†æ˜¯ç›¸å…³çš„å› ç´ æ€»æ˜¯ç‰¹åˆ«å¤šçš„ã€‚è­¬å¦‚é‚£ä¸ªåŸºå› æ£€æµ‹ã€‚æˆ‘ä»¬æ€»å¾—è§£å†³è¿™ç§é«˜ç»´é—®é¢˜ã€‚
&gt; æœ‰569ä¸ªobservationsï¼Œ30ä¸ª featuresã€‚å¯¹äºæ±‚features çš„ covariance matrix $Î£in S$æ¥è¯´ï¼Œæœ‰$cfrac{30*29}{2}approx 430$ä¸ª parameters è¦å» estimateã€‚å¦‚æœ take average å‡ ä¹æ˜¯ä¸€ä¸ªparameter ä¸€ä¸ªobservationï¼Œè¿™å·²ç»ç®—æ˜¯ high dimensional problemï¼Œé™¤éæ•°æ®very clean.</p>
</dd>
</dl>
<div class="line-block">
<div class="line">æ•°æ®é™ç»´ dimensionally reduction | ç‰¹å¾é€‰æ‹© Variable Selection |</div>
<div class="line">â€” | â€” |</div>
<div class="line">å¤šä¸ªç‰¹å¾åˆæˆä¸ºä¸€ä¸ªç‰¹å¾ | åœ¨å¤šä¸ªç‰¹å¾ä¸­é€‰æ‹©æŸä¸ªç‰¹å¾ |</div>
<div class="line">è·å–æ— æ³•è§£é‡Šçš„ç‰¹å¾ä¸å˜é‡ä¹‹é—´çš„å…³ç³» | å¯è§£é‡Šæ€§å¼º |</div>
</div>
<section id="some-special-notation">
<h2 id="some-special-notation">some special notation<a class="headerlink" href="#some-special-notation" title="Link to this heading">Â¶</a></h2>
<p><span class="defi">Random Vector</span> $Z=begin{bmatrix}Z_1\vdots\Z_pend{bmatrix}inR^p$</p>
<ul>
<li><p><span class="defi">Expectation Matrix</span> $mathbb EZ=begin{bmatrix}mathbb EZ_1\vdots\mathbb EZ_pend{bmatrix}$</p></li>
<li><dl>
<dt><span class="defi">Covariance Matrix</span> $Sigma=Var(Z)\=mathbb E{(Z-mathbb EZ)(Z-mathbb EZ)^T}\=begin{bmatrix}Var(Z_1)&amp;Cov(Z_1,Z_2)&amp;dots&amp;Cov{Z_1,Z_p}\Cov(Z_1,Z_2)&amp;Var(Z_2)&amp;dots&amp;Cov(Z_2,Z_p)\vdots&amp;&amp;ddots&amp;vdots\Cov(Z_p,Z_1)&amp;Cov(Z_p,Z_2)&amp;dots&amp;Var(Z_p)end{bmatrix}$</dt><dd><ul class="simple">
<li><p>$Sigma succeq0$</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 31)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>proof: the sample covariance matrix is non-negative definite.</p>
</dd>
</dl>
</li>
</ul>
<p>Correlationåªæ˜¯è€ƒå¯Ÿçº¿æ€§å…³ç³»çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸æ˜¯ä»£è¡¨independent</p>
<ul>
<li><p>$mathbb E(X+Y)=mathbb EX+mathbb EY$</p></li>
<li><p>$W:=A_{ptimes p}Z_{ptimes 1}+cinR^p$, <strong>constant</strong> matrixÂ A, <strong>constant</strong> vector c</p>
<blockquote>
<div><p>$mathbb E(AZ+c)=c+Amathbb EZ$</p>
<p>$Var(AZ+c)=Var(AZ)=AVar(Z)A^T$</p>
</div></blockquote>
</li>
<li><p>$mathbb E(AXB+c)=Amathbb EXB+c$</p></li>
</ul>
<ol class="arabic">
<li><p>Positive-definite matrices )</p>
<blockquote>
<div><p>Eigenvalue decomposition $A=GammaLambdaGamma^T=sumlimits_{i=1}^plambda_igamma_igamma_i^T$</p>
</div></blockquote>
</li>
</ol>
<p><span class="defi">Multivariate Normal Distribution</span> $Zï½N(b,Sigma)$
Suppose $Z=begin{bmatrix}Î¾_1\vdots\Î¾_pend{bmatrix}inR^p$ is a random vector, $mathbb EZ=binR^p,Var(Z)=Sigmain S^p$
$forall linR^p,l^TZinR$ ï½ Normal distribution. $implies Z$ follows Multivariate Normal Distribution.
$iff Zï½N(b,Sigma),b=begin{bmatrix}b_1\vdots\b_pend{bmatrix},Sigma=begin{bmatrix}sigma_{11}&amp;dots&amp;sigma_{1p}\vdots&amp;ddots&amp;vdots\sigma_{p1}&amp;dots&amp;sigma_{pp}\end{bmatrix}\
iff  f_Z(Î¾_1,dots,Î¾_p)=cfrac{1}{(2pi)^{k/2}|Sigma|^{1/2}}expBig(-cfrac{1}{2}(Î–-b)^T)Sigma^{-1}(Z-b) Big)$</p>
<p><strong>properties:</strong></p>
<ol class="arabic simple">
<li><p>$foralltext{ constant }linR^p,cinR,space l^TZ+cï½N(l^Tb+c,l^TSigma l)$</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 57)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. Partial correlation and conditional independence
$(Î–_1^T,Î–_2^T)^Tï½NBig(begin{bmatrix}b_1^T&amp;b_2^Tend{bmatrix}^T,begin{bmatrix}Sigma_{11}&amp;Sigma_{12}\Sigma_{21}&amp;Sigma_{22}end{bmatrix}Big)$
$\quad iff begin{bmatrix}Î¾_{11}\vdots\Î¾_{1p}\Î¾_{21}\vdots\Î¾_{2p}end{bmatrix}ï½NBig(begin{bmatrix}b_1\b_2end{bmatrix},begin{bmatrix}Sigma_{11}&amp;Sigma_{12}\Sigma_{21}&amp;Sigma_{22}end{bmatrix}Big)\quadiff begin{cases}Z_iï½Î(b_i,Sigma_{ii})\Z_1|Z_2ï½NBig(b_1+Sigma_{12}Sigma_{22}^{-1}(X_2-b_2),Sigma_{11}-Sigma_{12}Sigma_{22}^{-1}Sigma_{21}Big)end{cases}$</p>
<ul class="simple">
<li><p>proof:</p></li>
</ul>
<p>1. åœ¨ normal multivariate distribution é‡Œ Covariance = 0  ç­‰åŒäº independent
$cov(Z_1,Z_2)=sigma_{ij}=0iff Z_1,Z_2 text{ are independent}$</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 65)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><ul class="simple">
<li><p>proof</p></li>
</ul>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 66)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<ol class="arabic simple" start="2">
<li><p>$Î¾_kï½N(b_k,sigma_{kk}),begin{cases}Î¾_i+Î¾_jï½N(b_i+b_j,sigma_{ii}+2sigma_{ij}+sigma_{j})inR\Î¾_i-Î¾_jï½N(b_i-b_j)ï½Î(sigma_{ii}-2sigma_{ij}+sigma_{jj})inRend{cases}$</p></li>
</ol>
<p>==I.I.D. observations==. $text{Suppose } X_1,X_2,dots,X_minR^p text{ are i.d.d.}$</p>
<ol class="arabic simple">
<li><p>$mathbb EX=overline X=cfrac{1}{n}sumlimits_{i=1}^nX_i$</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 71)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. <span class="defi">Sample covariance matrix</span> $S=begin{cases}cfrac{1}{n}sumlimits_{i=1}^n(X_i-overline X)(X_i-overline X)^T\cfrac{1}{n-1}sumlimits_{i=1}^n(X_i-overline X)(X_i-overline X)^Tend{cases}in S^{p}$
å…³äºè¿™ä¸ª n-1 æ˜¯å› ä¸ºå‡å€¼å·²çŸ¥ï¼Œæ— åä¼°è®¡
3. ==Central Limit Theorem CLT==. Suppose $X_1,X_2,dots,X_minR^p text{ are i.d.d.},mathbb EX=muinR^p,Cov(X)=Sigma\qquadlimlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^n(X_i-mu)rightarrow N(0,Sigma)$
4. ==Law of Large numbers==. Suppose $X_1,X_2,dots,X_minR^p text{ are i.d.d.},EX_i=mu,Cov(X_i)=Sigma\qquadbegin{cases}limlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^nX_irightarrow E(X_i)=mu\limlimits_{nrightarrowinfin}cfrac{1}{n}sumlimits_{i=1}^n(X_i-mu)(X_i-mu)^TrightarrowSigmaend{cases}$</p>
<p>&gt; &gt; (AMA565_L0_T2) SupposeÂ $eÂ ï½Â N(0,Ïƒ^2I)$, what is the distribution of $hat{Î²}Â = (X^TX)^{âˆ’1}X^T(XÎ²Â +Â e)$? (Assume that the inverse and matrix multiplication are well defined)</p>
<p>â€”</p>
<p>&gt; &gt; (AMA565_L0_T3) SupposeÂ $XÂ ï½Â NÂ (Î¼,Â Î£)$, and let the eigenvalue decomposition ofÂ $Î£ = (Ïƒ_{ij})_{1â‰¤i,jâ‰¤p}$Â be given asÂ $Î£ = Î“Î›Î“â€²$Â whereÂ Î“Â is an orthogonal matrix andÂ $Î› = text{diag}{Î»_1, â€¦ , Î»_p}$Â is the matrix of the eigenvalues.
&gt; &gt; 1). What is the distribution ofÂ $Î“â€²X$?&gt; &gt; 2). LetÂ $Î£^{âˆ’Â 1/2}Â = Î“Î›^{âˆ’Â 1/2}Â Î“â€².$ What is the distribution ofÂ $Î£^{âˆ’Â 1/2}Â X$?
&gt; &gt; 3). SupposeÂ pÂ = 2Â and denoteÂ $XÂ = (X_1, X_2)â€²$. In addition,Â $Ïƒ_{11}=Ïƒ_{22} = 1, Ïƒ_{12}=Ï$. What is the distribution of $(Y_1,Y_2), Y_1=cfrac{(X_1+X_2)}{sqrt{2+2Ï}}, Y_2 = cfrac{(X_1 âˆ’ X_2)}{sqrt{2 âˆ’ 2Ï}} ?$</p>
</section>
<section id="special-matrix">
<h2 id="special-matrix">Special Matrix<a class="headerlink" href="#special-matrix" title="Link to this heading">Â¶</a></h2>
<p>$A=11^T=begin{bmatrix}1&amp;1&amp;dots&amp;1\vdots&amp;ddots&amp;dots&amp;vdots\1&amp;dots&amp;dots&amp;1end{bmatrix}inR^p=pcdot begin{bmatrix}cfrac{1}{sqrt p}\vdots\cfrac{1}{sqrt{p}}end{bmatrix}*begin{bmatrix}cfrac{1}{sqrt p}&amp;dots&amp;cfrac{1}{sqrt{p}}end{bmatrix}+0â€¦$</p>
<p><span class="defi">AR(1) model</span> $A=begin{bmatrix}1&amp;rho&amp;rho^2&amp;dots&amp;rho^{n-1}\rho&amp;1&amp;rho&amp;dots&amp;rho^{n-2}\vdots&amp;vdots&amp;vdots&amp;ddots&amp;vdots\rho^{n-1}&amp;rho^{n-2}&amp;dots&amp;dots&amp;1end{bmatrix} xrightarrow{e.g,}begin{bmatrix}1&amp;0.9&amp;0.9^2&amp;0.9^3\0.9&amp;1&amp;0.9&amp;0.9^2\0.9^2&amp;0.9&amp;1&amp;0.9\0.9^3&amp;0.9^2&amp;0.9&amp;1end{bmatrix}$</p>
</section>
<section id="variable-selection">
<h2 id="variable-selection">Variable Selection<a class="headerlink" href="#variable-selection" title="Link to this heading">Â¶</a></h2>
<p>ğŸ“‘Â ref</p>
<ul class="simple">
<li><p>[å¦‚ä½•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆç†è®ºç¯‡ï¼‰æœºå™¨å­¦ä¹ ä½ ä¼šé‡åˆ°çš„â€œå‘â€]</p></li>
<li><p>[Are screening methods useful in feature selection? An empirical study]</p></li>
</ul>
<p>[Are screening methods useful in feature selection? An empirical study]: <a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220842">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220842</a>
[å¦‚ä½•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆç†è®ºç¯‡ï¼‰æœºå™¨å­¦ä¹ ä½ ä¼šé‡åˆ°çš„â€œå‘â€]:<a class="reference external" href="https://baijiahao.baidu.com/s?id=1604074325918456186&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1604074325918456186&amp;wfr=spider&amp;for=pc</a></p>
<p>ç‰¹å¾
<a href="#id7"><span class="problematic" id="id8">|useful, important features|irreverent features|redundant feature|</span></a>
<a href="#id9"><span class="problematic" id="id10">|--|</span></a>â€“<a href="#id11"><span class="problematic" id="id12">|--|</span></a>
<a href="#id13"><span class="problematic" id="id14">|æˆ‘ä»¬å¸Œæœ›èƒ½ï¼šextract ä»–ä»¬ï¼Œä»–ä»¬èƒ½ä¸»å¯¼æ•´ä¸ª signals|æ— å…³ç‰¹å¾ã€‚æœ‰ä»–æ²¡ä»–éƒ½ä¸€æ ·|å†—ä½™ç‰¹å¾ã€‚Delete|</span></a></p>
<p>==redundant variable==. that preditor with 0 coefficient  $beta_k=0implies$  unimportant and meaningless</p>
<p>Variable, Predictor, in the model, plays two roles: <strong>improving the model flexibility and adversely affecting the model stability</strong>. Redundant variables are not helpful in prediction, thus should be removed.</p>
<p><strong>ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ Figure out what important variables are and Delete redundant featuresï¼Ÿ</strong></p>
<p>å› ä¸ºå¦‚æœæˆ‘ä»¬ä¸å¤„ç†æ‰ redundant variablesï¼Œ é‚£äº› noises created by redundant variables maybe dominate the signals, causing trouble to ask for allocating those useful signals.
æ¯”å¦‚ï¼Œé€šè¿‡æˆ¿å±‹çš„é¢ç§¯ï¼Œå§å®¤çš„é¢ç§¯ï¼Œè½¦åº“çš„é¢ç§¯ï¼Œæ‰€åœ¨åŸå¸‚çš„æ¶ˆè´¹æ°´å¹³ï¼Œæ‰€åœ¨åŸå¸‚çš„ç¨æ”¶æ°´å¹³ç­‰ç‰¹å¾æ¥é¢„æµ‹æˆ¿ä»·ï¼Œé‚£ä¹ˆæ¶ˆè´¹æ°´å¹³ï¼ˆæˆ–ç¨æ”¶æ°´å¹³ï¼‰å°±æ˜¯å¤šä½™ç‰¹å¾ã€‚è¯æ®è¡¨æ˜ï¼Œæ¶ˆè´¹æ°´å¹³å’Œç¨æ”¶æ°´å¹³å­˜åœ¨ç›¸å…³æ€§ï¼Œæˆ‘ä»¬åªéœ€è¦å…¶ä¸­ä¸€ä¸ªç‰¹å¾å°±å¤Ÿäº†ï¼Œå› ä¸ºå¦ä¸€ä¸ªèƒ½ä»å…¶ä¸­ä¸€ä¸ªæ¨æ¼”å‡ºæ¥ã€‚ï¼ˆå¦‚æœæ˜¯çº¿æ€§ç›¸å…³ï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨ç”¨çº¿æ€§æ¨¡å‹åšå›å½’çš„æ—¶å€™ï¼Œä¼šå‡ºç°ä¸¥é‡çš„å¤š**é‡å…±çº¿æ€§é—®é¢˜**ï¼Œå°†ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚ï¼‰
ç‰¹å¾é€‰æ‹©è¿˜å¯ä»¥ä½¿æ¨¡å‹è·å¾—æ›´å¥½çš„è§£é‡Šæ€§ï¼ŒåŠ å¿«æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¸€èˆ¬çš„ï¼Œè¿˜ä¼šè·å¾—æ›´å¥½çš„æ€§èƒ½</p>
<p><strong>å¸¸è§çš„æ–¹æ³•åŒ…æ‹¬è¿‡æ»¤æ³•ï¼ˆFilter, Screeningã€åŒ…è£¹æ³•ï¼ˆWarpperï¼‰ï¼ŒåµŒå…¥æ³•ï¼ˆEmbeddingï¼‰ã€‚</strong></p>
<section id="filter-screening">
<h3 id="filter-screening">Filter, Screening, è¿‡æ»¤æ³•<a class="headerlink" href="#filter-screening" title="Link to this heading">Â¶</a></h3>
<p>é€‰æ‹©ä¸€äº› important features, which is a <strong>subset</strong> of all features, ä½†æˆ‘ä»¬ä¸æ˜¯ç®€å•çš„é€‰æ‹©ï¼Œè€Œæ˜¯è®¾ç½®ä¸€ä¸ª threshold å°½é‡ save all signalsã€‚ä½† No free lunch, it just a trade.</p>
<p><strong>SAME Assumptions:</strong></p>
<p>$begin{cases}text{centered X: } mathbb EX=0,Cov(X)=Sigma, text{centered Y: } mathbb EY=0implies beta_0=0\(X_i,Y_i)text{ are IDD}\(X_i,Y_i,epsilon_i)text{ are independent}\red {Xi}:=text{error between estimator and truth}inR^n=begin{bmatrix}epsilon_1&amp;dots&amp;epsilon_nend{bmatrix}^T\mathbb EXi=0,Var(Xi)=sigma^2IinR^{ntimes n}end{cases}$</p>
<div class="line-block">
<div class="line-block">
<div class="line">| Perfect Models | More Redundant Variables | Less Important Variables |</div>
</div>
<div class="line">â€” | â€” | â€” | â€” |</div>
<div class="line">p | $p=p_0$ | $p&gt;p_0$ | $p&lt;p_0$ |</div>
<div class="line-block">
<div class="line">| Correct  | Correct  | Wrong |</div>
</div>
<div class="line">$epsilon_i$ | $N(0,sigma^2)$ | $N(0,sigma^2)$ | $red {tildeepsilon_i ,E(tildeepsilon_i)neq0}$ |</div>
<div class="line">$Bias(hat{Y}_{new})$ | âŒ | âŒ | â­• $Etilde{beta}-beta)^TX_{new}$ |</div>
<div class="line">$E{(Y-hat{Y})^2}$ | $approxsigma^2(1+cfrac{p_0}{n})$ | $approxsigma^2(1+cfrac{p}{n})$ | $approxsigma^2+(beta_{p+1}^2+dots+beta_{p_o^2})lambda_{min}(Sigma)$ |</div>
</div>
<section id="perfect-models-with-general-variables">
<h4 id="perfect-models-with-general-variables">Perfect Models with General Variables<a class="headerlink" href="#perfect-models-with-general-variables" title="Link to this heading">Â¶</a></h4>
<p>==Perfect model ($p=p_0$) variables==. $Y_i=beta_1x_{i,1}+dots+beta_{p_0}x_{i,p_0}+red{epsilon_i}$
$iff Y_i=beta^TX_i+epsilon_i, begin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p_0}end{bmatrix}^TinR^{p_0}\X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p_0}end{bmatrix}^TinR^{p_0}\[1em]Cov(X)=SigmainR^{p_0times p_0}\red{epsilon_itext{ are IID}ï½N(0,sigma^2)}end{cases}$
$implies Y=Xbeta+Xi,begin{cases}YinR^n\ XinR^{ntimes p_0 }=begin{bmatrix}X_1^T&amp;dots&amp;X_n^Tend{bmatrix}^T\betainR^{p_0}end{cases}$</p>
<p>==the estimator==. $ hat{beta} = (mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y=beta+(mathbb X^Tmathbb X)^{-1}mathbb X^T red {Xi }$
$\quad begin{cases}mathbb X:= text{training dataset with n traing samples } X_i\mathbb X^Tmathbb  X =sumlimits_{i=1}^nX_iX_i^T,\X_i text{ are IID }ï½N(0,Sigma)impliesmathbb  X^Tmathbb X=sumlimits_{i=1}^n(X_i-0)(X_i-0)^T=red{nSigma}\end{cases}$</p>
<p>$hat{beta}=beta+(n^{-1}mathbb X^Tmathbb X)^{-1}n^{-1}mathbb X^TXiapprox beta+Sigma^{-1}n^{-1}mathbb X^TXiimplies\[1em]hat{beta}-beta=(n^{-1}mathbb X^Tmathbb X)^{-1}n^{-1}mathbb X^TXiapproxSigma^{-1}n^{-1}mathbb X^TXi$</p>
<p>!!! p â€œå‰è€… $(n^{-1}mathbb X^Tmathbb X)$ æ˜¯æ ·æœ¬ç®—å‡ºæ¥çš„ï¼Œåè€…  $Sigma$  æ˜¯åˆ†å¸ƒçš„æ–¹å·®ï¼Œå‰è€… converge into åè€…ï¼Œå‡å°‘äº† randomnessï¼Œæ‰€ä»¥æ˜¯ approximatelyâ€</p>
<p><strong>For a new random observation</strong> $X_{new}$
$$begin{align*}Y_{new}&amp;=beta^TX_{new}+epsilon_{new}\hat{Y}_{new} &amp;=hat{beta}^TX_{new}
end{align*}$$</p>
<p>$(X_{new},Y_{new},epsilon_{new})$ is independent of $(X_i,Y_i,epsilon_i)$</p>
<p>$$begin{align*}
Y_{new}-hat{Y}_{new}&amp;=underline{red {epsilon_{new}}+beta^{red{T}}X_{new}}-underline{hat{beta}^{red T}X_{new}}\
&amp;=red {epsilon_{new}}+(beta-hat{beta})^{red T}X_{new}\
&amp;approxred {epsilon_{new}}-underline{n^{-1}Xi^Tmathbb XSigma^{-1}}X_{new}
end{align*}$$</p>
<p>Square of Prediction Error</p>
<p>$$
begin{align*}(Y_{new}-hat{Y}_{new})^2&amp;approx(red {epsilon_{new}}-underline{n^{-1}Xi^Tmathbb XSigma^{-1}}X_{new})^2\[1em]&amp;approxred {epsilon_{new}}^2-2red {epsilon_{new}}cdot n^{-1}X_{new}^TSigma^{-1}mathbb X^TXi+n^{-2}(Xi^Tmathbb XSigma^{-1}X_{new})^2end{align*}
$$</p>
<p>Take expectation:</p>
<p>$$
begin{align*}mathbb E{(Y-hat{Y})^2}&amp;=mathbb E{(Y-hat{beta}^TX)^2}\&amp;approxmathbb EBig(red {epsilon}^2-2red {epsilon}cdot n^{-1}X^TSigma^{-1}mathbb X^TXi+n^{-2}(Xi^Tmathbb XSigma^{-1}X)^2Big)\&amp;approxsigma^2-0+cfrac{p_0}{n}sigma^2\&amp;approxsigma^2(1+cfrac{p_0}{n})end{align*}
$$</p>
<p>$implies$ <strong>When n is large, the perfect model has the smallest prediction error</strong></p>
</section>
<section id="working-model-with-more-redundant-variables">
<h4 id="working-model-with-more-redundant-variables">Working Model with More Redundant Variables<a class="headerlink" href="#working-model-with-more-redundant-variables" title="Link to this heading">Â¶</a></h4>
<p>å¦‚æœæˆ‘ä»¬ä¸é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œæˆ‘ä»¬å°è¯•ä¸ºæŠ˜æœ‰äº‹ç‰©æ·»åŠ ä¼°è®¡ç³»æ•°ï¼Œé‚£ä¹ˆè¯¯å·®å°†æ±‡æ€»æƒå°†æ±‡æ€»ï¼Œå³æ¯æ¬¡æˆ‘ä»¬ä¼°è®¡æŸäº›ä¸œè¥¿æ—¶ï¼Œæ‚¨éƒ½ä¼šåˆ›ä¸€ä¸ªé”™è¯¯ã€‚</p>
<p><span class="defi">Wroking model with more redundant variables ($p$ variables, $red{p&gt;p_0}$)</span></p>
<blockquote>
<div><p>$Y_i=beta_1x_{i,1}+dots+beta_{p_0}x_{i,p_0}+0times x_{i,p_0}+dots+0times x_{i,p}+red{epsilon_i},$
$iff Y_i=beta^TX_i+epsilon_i,i=1,dots,n, \qquadbegin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p_0}&amp;0&amp;dots&amp;0end{bmatrix}^TinR^p,X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p_0}&amp;x_{i,p_0}&amp;dots&amp;x_{i,p}end{bmatrix}^TinR^p\Cov(X)=SigmainR^{ptimes p}\red{epsilon_itext{ are IID}ï½N(0,sigma^2)}end{cases}$</p>
</div></blockquote>
<p><span class="defi">the estimator</span> $hat{beta} = (mathbb X^Tmathbb X)^{-1}mathbb X^Tmathbb Y=beta+(mathbb X^Tmathbb X)^{-1}mathbb X^T red {Xi }\quad begin{cases}mathbb XinR^{ntimes p},mathbb X^Tmathbb  X =sumlimits_{i=1}^nX_iX_i^TinR^{ptimes p},\X_i text{ are IID }ï½N(0,Sigma)impliesmathbb  X^Tmathbb X=sumlimits_{i=1}^n(X_i-0)(X_i-0)^T=red{nSigma}inR^{ptimes p}\end{cases}$</p>
<p>Expected Square of Prediction Error</p>
<p>$$mathbb E{(Y-hat{Y})^2}=mathbb E{(Y-hat{beta}^TX)^2}approxsigma^2(1+cfrac{p}{n})&gt;sigma^2(1+cfrac{p_0}{n}),p&gt;p_0
$$</p>
<p>proof of</p>
<p>$implies$  <strong>the more redundant variables, the worse the prediction is.</strong>
$implies$  <strong>if n is very large, it is ok to put all features in model as well since $cfrac{p_0}{n}$  would vanish</strong></p>
</section>
<section id="working-model-with-less-important-variables">
<h4 id="working-model-with-less-important-variables">Working Model with Less Important Variables<a class="headerlink" href="#working-model-with-less-important-variables" title="Link to this heading">Â¶</a></h4>
<p><span class="defi">Wroking model with less important variables ($p$ variables, $red{p&lt;p_0, beta_{p_0}neq0}$</span>
$Y_i=beta_1x_{i,1}+dots+beta_ptimes x_{i,p}+red{tilde{epsilon_i}}$
$iff Y_i=beta^TX_i+red{tilde{epsilon_i}},i=1,dots,n, \qquadbegin{cases}Y_iinR,\beta=begin{bmatrix}beta_1&amp;dots&amp;beta_{p}end{bmatrix}^TinR^p,X_i=begin{bmatrix}x_{i,1}&amp;dots&amp;x_{i,p}end{bmatrix}^TinR^p\Cov(X)=SigmainR^{ptimes p}\red{!!E(tilde{epsilon})neq 0impliedbytext{wrong model without enough variables}}\lambda_{min}(Sigma)&gt;0end{cases}$</p>
<p><span class="defi">the estimator</span> $hat{beta}=begin{bmatrix}hat{beta_1}&amp;dots&amp;hat{beta_p}end{bmatrix}^TinR^p,red{p&lt;p_0}\xrightarrow{å˜å½¢}red{tilde{beta}}=begin{bmatrix}hat{beta}\0end{bmatrix}=begin{bmatrix}hat{beta_1}&amp;dots&amp;hat{beta_p}&amp;0&amp;dots&amp;0end{bmatrix}^TinR^{p_0}$</p>
<p>$implieshat{Y}_{new}=hat{beta}_{1times p}{X_{new}}_{ptimes1}=tilde{beta}^T_{1times p_0}{X_{new}}_{p_0times1}\qquadqquad=hatbeta_1x_1+dots+hat{beta_p}x_p+0times x_p+dots+0times x_{p_0}$</p>
<p><strong>There is a bias in the prediction for a given X.</strong></p>
<p>$Bias(hat{Y}_{new})=mathbb EY_{new}-mathbb Ehat{Y}_{new}=mathbb E{(beta-tilde{beta})^TX_{new}+epsilon_{new}}=(mathbb Etilde{beta}-beta)^TX_{new}$</p>
<p>Expected Square of Prediction Error</p>
<p>$$
begin{align*}mathbb E{(Y-hat{Y})^2}&amp;=sigma^2+mathbb E{(beta-tilde{beta})^TSigma(beta-tilde{beta})}\[1em]&amp;gesigma^2+Vertbeta-tildebetaVert^2lambda_{min}(Sigma)\[1em]&amp;gesigma^2+(beta_{p+1}^2+dots+beta_{p_0^2})lambda_{min}(Sigma)end{align*}
$$</p>
<ul class="simple">
<li><p>proof of</p></li>
</ul>
<p>$implies$  If the working model does not include all the important variables (those with $Î²_{k}â‰  0,k=1,dots,p_0$), the prediction error (lower bound) is also bigger than the model with exactly the important variables.
$implies$ <strong>æ ·æœ¬æ•° n å†å¤§ä¹Ÿæ‹¯æ•‘ä¸äº†è¿™ä¸ªerror å› ä¸º$(beta_{p+1}^2+dots+beta_{p_0^2})lambda_{min}(Sigma)$ is constantï¼Œè€Œä¸”è¿™åªæ˜¯ä¸‹ç•Œ lower bound</strong></p>
</section>
<section id="candidate-models-for-p-1-predictor-1-x-1-x-p">
<h4 id="candidate-models-for-p-1-predictor-1-x-1-x-p">Candidate Models for p+1 predictor $1ï¼Œ x_1, â€¦, x_p$<a class="headerlink" href="#candidate-models-for-p-1-predictor-1-x-1-x-p" title="Link to this heading">Â¶</a></h4>
<p>&lt;figure markdown=â€spanâ€&gt;![](./pics/FS_1.png){width=80%}&lt;p&gt;un-centralized&lt;/p&gt;&lt;/figure&gt;</p>
<p>Suppose we have n samples. Consider any sub-model (A)
$(A):= Y =beta_0+Î²_1xâ€™_1 +â€¦+Î²_qxâ€™_q+Îµ, space{xâ€™_1,dots,xâ€™_q}sub{x_1,dots,x_p}$</p>
<p>==RRS of A==. $RSS(A)=sumlimits_{i=1}^n{Y_i-hat{Y}_{A,i}}^2$.
$hat{Y}_{A,i}:=$ the fitted value(estimation) of $Y_i$ generated by model (A</p>
<p>!!! warning â€œFitted error (RSS) å¯ä»¥å»è¡¡é‡how good model areï¼Œ ä½†æ˜¯ cannot be used as one criterion for the selection.â€</p>
<ol class="arabic simple">
<li><p>For any two models A and B, if A is a sub-model of B, then $RSS(A) â‰¥ RSS(B).$ åªè¦ Aæ˜¯ B çš„å­é›†ï¼Œé‚£ä¹ˆ RSS(A) ä¸€å®šâ‰¥ RSS(B)ã€‚</p></li>
<li><p>è€Œä¸”è¿™ä¸ªRSSæ˜¯åœ¨ training set 1-n ä¸Šè¿›è¡Œï¼Œå¦‚æœæ˜¯ overfitting çš„è¯ï¼Œerrorå†å°ï¼Œä½†æ˜¯åœ¨é²æ£’æ€§è¿˜æ˜¯å¾ˆåƒåœ¾çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨åœ¨è®­ç»ƒé›†ä¸Šçš„RSSå» compare</p></li>
</ol>
</section>
</section>
<section id="model-selection-for-lr">
<h3 id="model-selection-for-lr">Model Selection for LR<a class="headerlink" href="#model-selection-for-lr" title="Link to this heading">Â¶</a></h3>
<p>For example, an empirical method like Cross-Validation, Bootstrap methods or sample penalties such as AIC, BIC, Mallowâ€™s CP.</p>
<p>[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]</p>
<p>==Cross validation==. å› ä¸ºè¦æ¯”è¾ƒä¸€äº›æ¨¡å‹ï¼Œå¦‚æœæ¯ä¸ªæ¨¡å‹éƒ½æ‹¿ä¸€äº›è¿›è¡Œè®­ç»ƒç„¶åæµ‹éªŒè¯é›†çš„å‡†ç¡®ç‡ã€‚å½“è®­ç»ƒé›†çš„ n éå¸¸å¤§çš„æ—¶å€™ï¼Œå°±å¾ˆå®¹æ˜“ time-consumingã€‚</p>
<p>==K-fold==. å’Œäº¤å‰éªŒè¯æ¯”ï¼Œæ˜¯ computation more efficientï¼Œ ä½†æ›´model is less stable</p>
<p><span class="defi">AIC</span></p>
<p>/BIC</p>
<p>the computational efficiency of AIC/BIC or when the sample size is relatively small for cross-validation
AIC and BIC explicitly penalize the number of parameters, cross-validation not, so again, itâ€™s not surprising that they suggest a model with fewer parameters (though nothing prohibits cross-validation from picking a model with fewer parameters).</p>
<p>[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]: <a class="reference external" href="https://stats.stackexchange.com/questions/578982/model-selection-aic-bic-and-cross-validation-gives-different-conclusion">https://stats.stackexchange.com/questions/578982/model-selection-aic-bic-and-cross-validation-gives-different-conclusion</a></p>
</section>
</section>
<section id="dimensionality-reduction">
<h2 id="dimensionality-reduction">Dimensionality Reductionï¼Œæ•°æ®é™ç»´<a class="headerlink" href="#dimensionality-reduction" title="Link to this heading">Â¶</a></h2>
<p>æ•°æ®é™ç»´å…¶å®è¿˜æœ‰å¦ä¸€ä¸ªå¥½å¤„ï¼šæ•°æ®å¯è§†åŒ–ã€‚å› ä¸ºè¶…è¿‡ä¸‰ç»´çš„æ•°æ®å°±æ— æ³•å¯è§†åŒ–äº†ã€‚æ•°æ®é™ç»´æœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯ä¸»æˆåˆ†åˆ†æã€‚</p>
<dl class="simple">
<dt>!!! danger â€œæˆ‘ä»¬æƒ³æ‰¾åˆ°é‡è¦ä¿¡å·çš„ä½ç½®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ‰¾åˆ°é‡è¦ä¿¡å·æˆ–å¼ºä¿¡å·ï¼Œæˆ–è€…è¿™äº›å¼±ä¿¡å·ç°åœ¨æ˜¯é›¶ï¼Œå°±æ‰”æ‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºè¿™äº›å¼ºä¿¡å·æä¾›é€‚å½“çš„ä¼°è®¡ã€‚â€</dt><dd><p>æˆ‘ä»¬è‚¯å®šæ²¡æœ‰ enough informationï¼Œå› ä¸ºåŒæ—¶å­˜åœ¨ç€ noiseã€‚å†³å®šæˆ‘ä»¬æ˜¯å¦èƒ½å®Œæˆç›®æ ‡å°±æ˜¯ï¼šwhether the important signals in data are stronger than noisesã€‚æˆ‘ä»¬ç°åœ¨å‡å®šè¿™ä¸ª important signals are <strong>stronger</strong> than noises. æ¥ä¸‹æ¥å°±è¦æƒ³å¦‚ä½•å°†important information å‰¥ç¦» noiseï¼Ÿ</p>
</dd>
</dl>
<section id="principal-component-analysis-pca">
<h3 id="principal-component-analysis-pca">Principal Component Analysis, PCA, ä¸»æˆåˆ†åˆ†æ<a class="headerlink" href="#principal-component-analysis-pca" title="Link to this heading">Â¶</a></h3>
<p>The basic idea is to transform the p random variables into &lt;u&gt;linear combinations&lt;/u&gt; called ==Principal Components==. Extracting linear combinations from multivariate data, a subset of PCs &lt;u&gt;captures most of the variability &lt;/u&gt; in the data.</p>
<p>æ­£äº¤å˜æ¢æŠŠç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„è§‚æµ‹æ•°æ®è½¬æ¢ä¸ºå°‘æ•°å‡ ä¸ªç”±çº¿æ€§æ— å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®ï¼Œ<a href="#id1"><span class="problematic" id="id2">**</span></a>çº¿æ€§æ— å…³**çš„å˜é‡ç§°ä¸º:defi:<cite>ä¸»æˆåˆ†</cite></p>
<aside class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 273); <em><a href="#id2">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<section id="key-maximize-the-variance">
<h4 id="key-maximize-the-variance">KEY: Maximize the variance<a class="headerlink" href="#key-maximize-the-variance" title="Link to this heading">Â¶</a></h4>
<dl class="simple">
<dt>!!! p â€œMaximize the varianceâ€</dt><dd><p>Identify key components which can &lt;u&gt;maximize the information&lt;/u&gt; with a reasonable dimension.
Find <strong>unit-vector g</strong> to transform X into Y with the target that &lt;u&gt;maximizes the variance of Y&lt;/u&gt;.</p>
</dd>
</dl>
<p>Suppose $X=begin{bmatrix}X_1\vdots\X_pend{bmatrix}inR^p$ is a random vector with <span class="defi">Covariance Matrix</span> $Var{X}=Sigma=begin{bmatrix}sigma_{1}^2&amp;dots&amp;sigma_{1p}\vdots&amp;ddots&amp;vdots\sigma_{p1}&amp;dots&amp;sigma_{p}^2\end{bmatrix}$</p>
<p>Look for &lt;u&gt;the linear transformations&lt;/u&gt;:</p>
<p>$$
begin{align*}begin{cases}Y_1=g_{11}X_1+dots+g_{1p}X_p\Y_2=g_{21}X_1+dots+g_{2p}X_p\vdots\Y_p=g_{p1}X_1+dots+g_{pp}X_pend{cases}iffbegin{bmatrix}Y_1\vdots\Y_pend{bmatrix}&amp;=begin{bmatrix}g_{11}&amp;dots&amp;g_{1p}\vdots\g_{p1}&amp;dots&amp;g_{pp}end{bmatrix}begin{bmatrix}X_1\vdots\X_pend{bmatrix}\&amp;=begin{bmatrix}g_1^T&amp;dots&amp;g_p^T\end{bmatrix}begin{bmatrix}X_1\vdots\X_pend{bmatrix}end{align*}\vec{g_i}=begin{bmatrix}g_{i1}\vdots\g_{ip}end{bmatrix},Vert vec{g_i}Vert_2=1,forall i=1,â€¦p
$$</p>
<p><span class="defi">property of  r.vector</span> $Y=g^TX, gintext{constant}\implies Var(Y)=g^TVar(X)g=g^TSigma g$</p>
<p><strong>Target:</strong></p>
<p>$$g=maxlimits_{Vert gVert_2=1} Var(Y)=maxlimits_{Vert gVert_2=1} g^TSigma gtag{1}$$</p>
<p>==eign about postive definite A==.$Sigma=sumlimits_{i=1}^{p}lambda_igamma_i^Tgamma_i=Gamma^TLambdaGamma$  specially suppose $lambda_1&gt;dots&gt;lambda_p&gt;0$ in &lt;u&gt;anascending order&lt;/u&gt;</p>
<p>1. $g_1:=maxlimits_{Vert gVert_2=1} g^TSigma gimplies g_1=gamma_1 text{ of }Sigma$
$implies g_1$ is the <strong>direction</strong> <strong>where the variance is maximized.</strong> &lt;u&gt;ï¼ˆnot PC&lt;/u&gt;
$implies g_1^Tx$ is the <strong>1st PC</strong>
2. $g_2:=maxlimits_{Vert gVert_2=1,:g_2^Tg_1=0} g^TSigma gimplies g_2=gamma_2 text{ of }Sigma$
$implies g_2$ is the one that maximizes the variance among all directions <strong>orthogonal</strong> to $g_1$
3. proof: <strong>i th-PC = i th eigen vector</strong>
Let $lambda_i:=$ the i-th largest eigen-value of $Sigma$, $gamma_i:=$ the eigen-vector corresponding to $lambda_i$. Therefore, ${gamma_1,gamma_n}$ are one set of the basis of $R^p$ &lt;u&gt;ç‰¹å¾å‘é‡æ˜¯ç‰¹å¾ç©ºé—´çš„ä¸€ç»„ basic vectorsã€‚&lt;/u&gt;
$impliesforall ginR^p,exist c_1, c_2,dots, c_ptext{ s.t. } g=c_1gamma_1+c_2gamma_2+c_ngamma_n$</p>
<p>$$begin{align*}implies g^TSigma g&amp;=sumlimits_{i=1}^nsumlimits_{j=1}^nc_ic_jgamma_i^TSigmagamma_j\
&amp;xlongequal{Sigmagamma_i=lambda_igamma_i}sumlimits_{i=1}^nsumlimits_{j=1}^pc_ic_jgamma_i^Tgamma_jlambda_j\
&amp;xlongequal[lambda_i^Tlambda_i=1]{lambda_i^Tlambda_j=0,forall ineq j} sum_{i=1}^nc_i^2lambda_i\
&amp;lelambda_1sum_{i=1}^nc_i^2\
&amp;xlongequal[Vert gVert_2^2=1=c_1^2+dots+c_n^2]{text{when }c_2=dots=c_n=0 }lambda_1
end{align*}$$</p>
<p>$$implies g=gamma_1iff g^TSigma g=lambda_1$$</p>
<p><strong>Conclusion:</strong></p>
<ol class="arabic simple">
<li><p>the best direction is <strong>the direction of eigenvectors</strong></p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 319)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. the <strong>variance</strong> of the direction is <strong>Eigen value</strong>
$Var(Y_i)=g_i^TSigma g_i=g_i^Tlambda_i g_i = lambda_i\tr(Sigma)=tr(GammaLambdaGamma^T)=tr(LambdaGammaGamma^T)=tr(Lambda)=sumlimits_{i=1}^plambda_i$
3. What is the relationship between $Y_i &amp; Y_j, ineq j$
<strong>orthogonal and are the Eigen-vector of Sigma</strong>
4. What if X is following a multivariate normal distribution?
<span class="defi">Multivariate Normal Distribution</span> $Zï½N(b,Sigma),forall linR^p,l^TZinRï½$ Normal distriâ€¦ $implies Z$ ï½ Multivariate Normal Distriâ€¦
If X is following a multivariate normal distribution, $g_i^TXï½N$</p>
<p><strong>Advantages:</strong></p>
<ol class="arabic simple">
<li><p>Identify <strong>key components</strong> which can <strong>maximize the information</strong> with a reasonable dimension. å‘ç°æ•°æ®ä¸­çš„åŸºæœ¬ç»“æ„ï¼Œå³æ•°æ®ä¸­å˜é‡ä¹‹é—´çš„å…³ç³»,èƒ½è¿‘ä¼¼åœ°è¡¨è¾¾</p></li>
<li><p><strong>Reduce the dimension</strong> of other forms of analysis.</p></li>
<li><p>Linearity is assumed.</p></li>
</ol>
<p><strong>Limits:</strong></p>
<ol class="arabic simple">
<li><p>It can be <strong>more difficult to interpret</strong> than using a subset of the original variables.</p></li>
<li><p>It uses only covariances/correlations but not higher-order moments. This can be extended to <span class="defi">independent component analysis ICA</span></p></li>
</ol>
</section>
<section id="pca-transformation">
<h4 id="pca-transformation">PCA Transformation<a class="headerlink" href="#pca-transformation" title="Link to this heading">Â¶</a></h4>
<p>$Y_i = g_i^TXimplies Y=Gamma^TX$</p>
<p>the 1 PC $Y_1=g_1^TX, g_1:= $ the eigen-vector with the 1 largest eigen-value $lambda_1$
the 2 PC $Y_2=g_2^TX, g_2:=$ the eigen-vector with the 2 largest Eigen-value $lambda_2$
$$vdots\
begin{bmatrix}Y_1\vdots\Y_pend{bmatrix}=begin{bmatrix}g_1^T\vdots\g_p^Tend{bmatrix}_{ptimes p}begin{bmatrix}X_1\vdots\X_pend{bmatrix}iff Gamma=begin{bmatrix}g_1&amp;dots&amp;g_pend{bmatrix}inR^{ptimes p}$$</p>
<section id="practical-use">
<h5 id="practical-use">Practical Use<a class="headerlink" href="#practical-use" title="Link to this heading">Â¶</a></h5>
<ol class="arabic simple">
<li><p>&lt;u&gt;**Standardize è§„èŒƒåŒ–**&lt;/u&gt; the data, ä½¿å¾—æ•°æ®æ¯ä¸€å˜é‡çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1</p></li>
<li><p>&lt;u&gt;**SVD æ­£äº¤åˆ†è§£**&lt;/u&gt; of the sample <strong>covariance M/correlation M</strong></p></li>
<li><p><strong>Sort</strong> the eigenvalues in descending order and choose the K largest eigenvectors (plots, the proportion of variances interpreted etc.)</p></li>
<li><p>&lt;u&gt;**Linear Transform å˜æ¢**&lt;/u&gt; X into Y(Dimension Reduction!)</p></li>
</ol>
<p>åŸæ¥ç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®ï¼Œé€šè¿‡æ­£äº¤å˜æ¢å˜æˆç”±è‹¥å¹²ä¸ªçº¿æ€§æ— å…³çš„æ–°å˜é‡è¡¨ç¤ºçš„æ•°æ®ã€‚æ–°å˜é‡æ˜¯å¯èƒ½çš„æ­£äº¤å˜æ¢ä¸­å˜é‡çš„æ–¹å·®çš„å’Œï¼ˆä¿¡æ¯ä¿å­˜ï¼‰æœ€å¤§çš„ï¼Œ<strong>æ–¹å·®è¡¨ç¤ºåœ¨æ–°å˜é‡ä¸Šä¿¡æ¯çš„å¤§å°</strong>ã€‚å°†æ–°å˜é‡ä¾æ¬¡ç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ç­‰ã€‚</p>
<dl class="simple">
<dt>!!! p â€œ<strong>how large dimensions we keepï¼š</strong>â€</dt><dd><p>è¿™é‡Œæ‰€è¯´çš„**ä¿¡æ¯æ˜¯æŒ‡åŸæœ‰å˜é‡çš„æ–¹å·®**ã€‚
å¯ä»¥è®¾ç½®ä¸€ä¸ª certain level, maybe 85%ï¼Œ90%ï¼Œç„¶åæˆ‘ä»¬çœ‹ &lt;u&gt;the cumulative proportion of k PCs&lt;/u&gt;, å› ä¸ºè¿™å°±æ˜¯è¯´ è¿™k ä¸ª PCs èƒ½å¤Ÿåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæè¿°è¿™äº›ç‚¹çš„å·®å¼‚é—´éš”ã€‚è€Œåœ¨å‰©ä¸‹çš„PCsé‡Œï¼Œè¿™äº›ç‚¹å¤§å¤šæ˜¯é‡å ï¼Œå¹¶ä¸å…·æœ‰å¾ˆé«˜çš„å·®å¼‚ä¿¡æ¯ä»·å€¼ã€‚åªè¦è¿™ä¸ª cumulativeè¾¾åˆ°äº†è¿™ä¸ª certain level æˆ‘ä»¬å°±é‡‡ç”¨å‰ k ä¸ª PCs</p>
</dd>
<dt>!!! p â€œcovariance M vs correlation M? â€”â€” &lt;kbd&gt;Scale&lt;/kbd&gt;â€</dt><dd><p>==covariance M==. $Cov(x,y)=cfrac{1}{n}sumlimits_{i=1}^n(x-overline{x})(y-overline{y})$
==correlation M==. $rho_{xy}=cfrac{Cov(x,y)}{sqrt{Var(x)}sqrt{Var(y)}}$
[Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]
å½“æˆ‘ä»¬è¦å»é™¤**ç‰¹å¾å€¼é‡çº²çš„åŒºåˆ«** æˆ‘ä»¬ä½¿ç”¨ correlationï¼Œ&lt;kbd&gt;scale=true&lt;/kbd&gt;
å¦‚æœä¸å»ï¼Œå°±æ˜¯covarianceï¼Œ&lt;kbd&gt;scale=false&lt;/kbd&gt;ï¼ˆé»˜è®¤
&gt; &gt; Suppose $x_1$ is a length measured either in cm. or mm., $x_2$ is a weight measurement in gm. The covariance M with $y_1$ in cm. is $begin{bmatrix}80&amp;44\44&amp;80end{bmatrix}$ the Covariance M with $y_1$ in mm. is $begin{bmatrix}8000&amp;440\440&amp;80end{bmatrix}$</p>
</dd>
</dl>
<p>[Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]:<a class="reference external" href="https://www.simplilearn.com/covariance-vs-correlation-article">https://www.simplilearn.com/covariance-vs-correlation-article</a></p>
</section>
</section>
<section id="graphical-rotate-the-data-without-scaling">
<h4 id="graphical-rotate-the-data-without-scaling">Graphical: Rotate the data without scaling<a class="headerlink" href="#graphical-rotate-the-data-without-scaling" title="Link to this heading">Â¶</a></h4>
<p>æ•°æ®é›†åˆä¸­çš„æ ·æœ¬ç”±å®æ•°ç©ºé—´ï¼ˆæ­£äº¤åæ ‡ç³»ï¼‰ä¸­çš„ç‚¹è¡¨ç¤ºï¼Œç©ºé—´çš„ä¸€ä¸ªåæ ‡è½´è¡¨ç¤ºä¸€ä¸ªå˜é‡ï¼Œè§„èŒƒåŒ–å¤„ç†åå¾—åˆ°çš„æ•°æ®**åˆ†å¸ƒåœ¨åŸç‚¹é™„è¿‘**ã€‚å¯¹åŸåæ ‡ç³»ä¸­çš„æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æç­‰ä»·äºè¿›è¡Œ**åæ ‡ç³»æ—‹è½¬å˜æ¢ï¼Œå°†æ•°æ®æŠ•å½±åˆ°æ–°åæ ‡ç³»çš„åæ ‡è½´ä¸Šã€‚**</p>
<dl>
<dt>!!! p â€œ<strong>Graphical:</strong> $Y=Gamma^TX$: Multiplication by an orthogonal matrix: <strong>Rotation</strong>!â€</dt><dd><p>&lt;u&gt;proof of rotation&lt;/u&gt;: $Vert y_1-y_2Vert_2=Vert x_1-x_2Vert_2, $ without scaling $forall x_1,x_2,$ any two points in space</p>
<p>$$begin{align*}Vert y_1-y_2Vert_2^2&amp;=(y_1-y_2)^T(y_1-y_2)\&amp;=(x_1-x_2)^TGammaGamma^T(x_1-x_2)\&amp;xlongequal[GammaGamma^T=I]{Gamma^T=Gamma^{-1}}(x_1-x_2)^T(x_1-x_2)=Vert x_1-x_2Vert_2^2end{align*}$$</p>
</dd>
</dl>
<p>æ–°åæ ‡ç³»çš„ç¬¬ä¸€åæ ‡è½´ã€ç¬¬äºŒåæ ‡è½´ç­‰åˆ†åˆ«è¡¨ç¤ºç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ç­‰ï¼Œæ•°æ®åœ¨æ¯ä¸€è½´ä¸Šçš„åæ ‡å€¼çš„å¹³æ–¹è¡¨ç¤ºç›¸åº”å˜é‡çš„æ–¹å·®ï¼›å¹¶ä¸”ï¼Œè¿™ä¸ªåæ ‡ç³»æ˜¯åœ¨æ‰€æœ‰å¯èƒ½çš„æ–°çš„åæ ‡ç³»ä¸­ï¼Œ<strong>åæ ‡è½´ä¸Šçš„æ–¹å·®çš„å’Œæœ€å¤§çš„</strong></p>
<p><strong>æ–¹å·®å’Œæœ€å¤§:</strong>
ä¸»æˆåˆ†åˆ†ææ—¨åœ¨é€‰å–æ­£äº¤å˜æ¢ä¸­æ–¹å·®æœ€å¤§çš„å˜é‡ï¼Œä½œä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼Œæ—‹è½¬å˜æ¢ä¸­**åæ ‡å€¼çš„å¹³æ–¹å’Œæœ€å¤§**çš„è½´, æ—‹è½¬å˜æ¢ä¸­é€‰å–**ç¦»æ ·æœ¬ç‚¹çš„è·ç¦»å¹³æ–¹å’Œæœ€å°**çš„è½´</p>
<p>&lt;div class=â€gridâ€ markdown&gt;
&lt;figure markdown=â€spanâ€&gt;![](./pics/PCA_1.png){width=45%}&lt;p&gt;transformationï¼š&lt;u&gt;æ—‹è½¬å˜æ¢&lt;/u&gt;&lt;/p&gt;&lt;figure&gt;</p>
<p>&lt;p&gt;åŸåæ ‡ç³»:å¾ˆæ˜æ˜¾æœ‰æ­£å‘çº¿æ€§ç›¸å…³ï¼Œ13è±¡é™æœ€å¤š&lt;br&gt;æ–°åæ ‡ç³»:çº¿æ€§æ— å…³ï¼Œ1234è±¡é™éƒ½å·®ä¸å¤š&lt;/p&gt;
&lt;/div&gt;</p>
<p>å¦‚æœä¸»æˆåˆ†åˆ†æåªå–ç¬¬ä¸€ä¸»æˆåˆ†ï¼Œå³æ–°åæ ‡ç³»çš„y1è½´ï¼Œé‚£ä¹ˆç­‰ä»·äºå°†æ•°æ®æŠ•å½±åœ¨æ¤­åœ†é•¿è½´ä¸Šï¼Œç”¨è¿™ä¸ªä¸»è½´è¡¨ç¤ºæ•°æ®ï¼Œå°†äºŒç»´ç©ºé—´çš„æ•°æ®å‹ç¼©åˆ°ä¸€ç»´ç©ºé—´ä¸­ã€‚</p>
<div class="line-block">
<div class="line">1st PC | 2nd PC |</div>
<div class="line">â€” | â€” |</div>
<div class="line">æ–¹å·®æœ€å¤§ | ä¸ç¬¬ä¸€åæ ‡è½´æ­£äº¤ï¼Œä¸”æ–¹å·®æ¬¡ä¹‹ |</div>
<div class="line">ç¬¬ä¸€åæ ‡è½´ $y_1$ | ç¬¬äºŒåæ ‡è½´ $y_2$ |</div>
<div class="line">æ¤­åœ†çš„é•¿è½´ | æ¤­åœ†çš„çŸ­è½´ |</div>
</div>
</section>
</section>
<section id="linear-discriminant-analysis-lda">
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis, LDA, çº¿æ€§åˆ¤åˆ«åˆ†æ<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Link to this heading">Â¶</a></h3>
<p>LDAçš„ç›®æ ‡æ˜¯**æå–ä¸€ä¸ªæ–°çš„åæ ‡ç³»ï¼Œå°†åŸå§‹æ•°æ®é›†æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­ã€‚**
å’ŒPCAçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒLDAä¸ä¼šä¸“æ³¨äºæ•°æ®çš„æ–¹å·®ï¼Œè€Œæ˜¯ä¼˜åŒ–ä½ç»´ç©ºé—´ï¼Œ<strong>ä»¥è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§</strong>ã€‚æ„æ€æ˜¯ï¼Œæ–°çš„åæ ‡ç³»åœ¨ä¸ºåˆ†ç±»æ¨¡å‹æŸ¥æ‰¾**å†³ç­–è¾¹ç•Œ**æ—¶æ›´æœ‰ç”¨ï¼Œ&lt;u&gt;éå¸¸é€‚åˆç”¨äºæ„å»ºåˆ†ç±»æµæ°´çº¿&lt;/u&gt;ã€‚</p>
<p><a href="#id3"><span class="problematic" id="id4">**</span></a>ä¼˜ç‚¹ï¼š<a href="#id5"><span class="problematic" id="id6">**</span></a>åŸºäºç±»åˆ«å¯åˆ†æ€§çš„åˆ†ç±»
- æœ‰åŠ©äºé¿å…æœºå™¨å­¦ä¹ æµæ°´çº¿çš„è¿‡æ‹Ÿåˆï¼Œä¹Ÿå«é˜²æ­¢ç»´åº¦è¯…å’’ã€‚
- LDAä¹Ÿä¼šé™ä½è®¡ç®—æˆæœ¬ã€‚</p>
<aside class="system-message" id="id3">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 407); <em><a href="#id4">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<aside class="system-message" id="id5">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/highd.rst</span>, line 407); <em><a href="#id6">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<p>!!! danger â€œFisherâ€™s LDA and Bayesâ€™ LDA are essentially different! They are equivalent under the &lt;u&gt;Gaussian assumption with a common Î£ for the two-class case&lt;/u&gt;â€</p>
<p>Both Fisherâ€™s LDA and Bayes rule reduce to:</p>
<div class="line-block">
<div class="line-block">
<div class="line">| empirical estimators |</div>
</div>
<div class="line">â€” | â€” |</div>
<div class="line">$Î¼_X$ | $overline{X}$ |</div>
<div class="line">$Î¼_Y$ | $overline{Y}$ |</div>
<div class="line">$Î£$ | $Î£$ |</div>
</div>
<dl class="simple">
<dt>!!! danger â€œä¸å¯å°†çº¿æ€§åˆ¤åˆ«åˆ†æä¸:defi:<cite>éšç‹„åˆ©å…‹é›·åˆ†é…LatentDirichlet Allocation, LDA</cite> ç›¸æ··æ·†ã€‚â€</dt><dd><p>éšç‹„åˆ©å…‹é›·åˆ†é…ç”¨äºæ–‡æœ¬å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä¸çº¿æ€§åˆ¤åˆ«åˆ†ææ²¡æœ‰å…³ç³»ã€‚</p>
</dd>
</dl>
<div class="line-block">
<div class="line">ç±»å†… within-class |  ç±»é—´ between-class</div>
<div class="line">â€” | â€” |
S_w | S_b</div>
</div>
<section id="fishers-lda">
<h4 id="fishers-lda">Fisherâ€™s LDA<a class="headerlink" href="#fishers-lda" title="Link to this heading">Â¶</a></h4>
<p>LDAçš„ç›®æ ‡æ˜¯æå–ä¸€ä¸ªæ–°çš„åæ ‡ç³»ï¼Œå°†åŸå§‹æ•°æ®é›†æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­ï¼Œä»¥è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§ã€‚</p>
<p><strong>Targetï¼š</strong> è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§
Find the lineÂ $P_ZÂ =Â w^TZ$Â that best separates the two classes.
$$w =maxlimits_w cfrac{w^TS_Bw}{w^TS_Ww}$$
Force $begin{cases}S_B=(mu_X-mu_Y)(mu_X-mu_Y)^T&amp;text{ between-class}uparrow\S_W=Sigma&amp;text{ within-class}downarrowend{cases}$</p>
<ol class="arabic simple">
<li><p>the center of the two after transformation linear projection be as <strong>far</strong> away as possible $S_Buparrow$</p></li>
<li><p>the variance of two classes to be as <strong>small</strong> as possible  $S_w downarrow$</p></li>
</ol>
<p>&lt;figure markdown=â€spanâ€&gt;![](./pics/LDA_1.png){width=60%}&lt;p&gt;LDA:æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§&lt;br&gt;å‡è®¾ï¼šæ­£æ€åˆ†å¸ƒ&lt;/p&gt;&lt;/figure&gt;</p>
<p>[æ©Ÿå™¨å­¸ç¿’: é™ç¶­(Dimension Reduction)- ç·šæ€§å€åˆ¥åˆ†æ( Linear Discriminant Analysis)]</p>
</section>
<section id="bayes-lda">
<h4 id="bayes-lda">Bayesâ€™ LDA<a class="headerlink" href="#bayes-lda" title="Link to this heading">Â¶</a></h4>
<p>!!! p â€œè´å¶æ–¯çš„ä¼˜ç‚¹ï¼šä¸éœ€è¦çŸ¥é“å…·ä½“çš„åˆ†å¸ƒâ€</p>
<p>$f_X (Â·):=$ pdf  for Class-X, $f_Y (Â·) := $ pdf  for Class-Y</p>
<p>==Bayes rule==. $Î´(Z) = I{Ï€_1f_X(Z) &gt; Ï€_2f_Y(Z)}$.
$begin{cases}pi_i text{ prior possibility },pi_1+pi_2=1 \f_i(Z)text{ likelihoood function}end{cases}$</p>
<p>For simplicity letâ€™s assume that $Ï€_1 = Ï€_2 = 1/2.$ without any assumption.</p>
<p>$Î´(Z) =I {[Zâˆ’(Î¼_X+mu_Y)/2]^T Î£^{âˆ’1}(Î¼_Xâˆ’mu_Y)&gt;0}\hat{Î´}(Z) =I {[Zâˆ’(overline{X}+overline{Y} )/2]^T hat{Î£}^{âˆ’1}(overline{X} âˆ’ overline{Y})&gt;0}$
Z will be assigned as $ begin{cases}X&amp;Î´(Z) =1\Y&amp;Î´(Z) =0end{cases}$</p>
</section>
</section>
<section id="quadratic-discriminant-analysis-qda">
<h3 id="quadratic-discriminant-analysis-qda">Quadratic Discriminant Analysis, QDA<a class="headerlink" href="#quadratic-discriminant-analysis-qda" title="Link to this heading">Â¶</a></h3>
<p>Assume:Â $Y_1âˆ¼N(Î¼_1,Î£_1), Y_2âˆ¼N(Î¼_2,Î£_2)$ The two classes have <strong>different</strong> covariance matrices!</p>
<p>$delta(x) = (xâˆ’Î¼)^TÎ©(xâˆ’Î¼)+Î´^T(xâˆ’ Î¼)+Î·\begin{cases}Î¼=(Î¼_1+ Î¼_2)/2spacetext{ (mean)}\Omega=Sigma_2^{-1}-Sigma^{-1}spacetext{ the difference of the two precision matrices}\delta=(Sigma_1^{-1}+Sigma_2^{-1})(mu_1-mu_2)\Î·=2log(pi_1/pi_2)+frac{1}{4}(mu_1-mu_2)^TOmega(mu_1-mu_2)+log|Sigma_2|-log|Sigma_1|end{cases}$</p>
<p>å› ä¸ºæ˜¯å…³äºxçš„äºŒæ¬¡å‡½æ•°ï¼Œæ‰€ä»¥æ˜¯ quadratic äºŒæ¬¡</p>
<p>[](<a class="reference external" href="https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b">https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b</a>)</p>
<p>&gt; &gt; (T1 in Chap1.1 in AMA565) Suppose the covariance M of a p-dimensional random vectorÂ XÂ isÂ $Î£ = text{diag}{1, 2, â€¦ , p}.$ What are the Principal Components ofÂ X?
&gt; &gt; (T2 in Chap1.1 in AMA565) <em>Suppose the covariance matrix of a p-dimensional random vectorÂ XÂ isÂ $Î£ =Â {11}^T$. What are the Principal Components of X?
&gt; &gt; (T3 in Chap1.1 in AMA565) *Suppose $X âˆ¼ N(Î¼_1,Î£),Y âˆ¼ N(Î¼_2,Î£).:f_1(x):=text{ pdf of }X, f_2(x):= text{ pdf of }Y$, and let Ï€_i be the prior probability that X is coming from class i, i = 1, 2. Show that $Ï€_1f_1(x)/[Ï€_2f_2(x)] &gt; 1iff (Î£^{âˆ’1}(Î¼_1 âˆ’ Î¼_2))T (x âˆ’ (Î¼_1 + (Î¼_2)/2)) &gt; c$ Derive c.</em></p>
<p>[æ©Ÿå™¨å­¸ç¿’: é™ç¶­(Dimension Reduction)- ç·šæ€§å€åˆ¥åˆ†æ( Linear Discriminant Analysis)]: <a class="reference external" href="https://chih-sheng-huang821.medium.com">https://chih-sheng-huang821.medium.com</a>/æ©Ÿå™¨å­¸ç¿’-é™ç¶­-dimension-reduction-ç·šæ€§å€åˆ¥åˆ†æ-linear-discriminant-analysis-d4c40c4cf937</p>
</section>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>