<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Loss &#8212; cocobook  文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/def.css?v=5a9d86bd" />
    <script src="../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#AI/loss" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="cocobook  文档"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">cocobook  文档</span>
          <span class="md-header-nav__topic"> Loss </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">cocobook  文档</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="cocobook 文档" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="cocobook 文档">cocobook  文档</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#ai-loss--page-root" class="md-nav__link">Loss</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#data-loss" class="md-nav__link">Data Loss</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#id3" class="md-nav__link">结构风险 & 经验风险</a>
        </li>
        <li class="md-nav__item"><a href="#loss-functions" class="md-nav__link">loss functions</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#id8" class="md-nav__link">0-1</a>
        </li>
        <li class="md-nav__item"><a href="#least-squares-error-lse-l2-loss-conditional-mean" class="md-nav__link">Least Squares Error, LSE, L2-loss —— conditional mean</a>
        </li>
        <li class="md-nav__item"><a href="#least-absolute-deviation-lad-conditional-median" class="md-nav__link">Least Absolute Deviation, LAD —— conditional median</a>
        </li>
        <li class="md-nav__item"><a href="#check-loss-function-conditional-quantile" class="md-nav__link">Check Loss function —— conditional quantile</a>
        </li>
        <li class="md-nav__item"><a href="#hinge-loss" class="md-nav__link">Hinge Loss</a>
        </li>
        <li class="md-nav__item"><a href="#squared-hinge-loss" class="md-nav__link">Squared Hinge Loss</a>
        </li>
        <li class="md-nav__item"><a href="#softmax" class="md-nav__link">Softmax</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#regularization-measure-complexity-and-penalize" class="md-nav__link">Regularization - measure complexity and penalize</a>
        </li>
        <li class="md-nav__item"><a href="#classification" class="md-nav__link">Classification</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#from-0-1-loss" class="md-nav__link">From 0-1 loss</a>
        </li>
        <li class="md-nav__item"><a href="#the-likelihood" class="md-nav__link">The Likelihood 似然的角度</a>
        </li>
        <li class="md-nav__item"><a href="#cross-entropy" class="md-nav__link">Cross Entropy</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#ref" class="md-nav__link">📑 ref</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/AI/loss.rst.txt">显示源代码</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="loss">
<h1 id="ai-loss--page-root">Loss<a class="headerlink" href="#ai-loss--page-root" title="Link to this heading">¶</a></h1>
<p><strong>What do we care about？</strong></p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>What types of errors do we care about？ 什么样的错误是我更关注的</dt><dd><p>一些很微小的 errors 需要在意吗？
一些错得很离谱的errors 我们要怎么调整它的权重呢？(outlier?)
which class data do we care about？</p>
</dd>
</dl>
</li>
<li><p>How much do we care about the errors？ 我们非常在意这个错误吗？</p></li>
</ol>
<p><strong>Two types of error 𝜂 are considered:</strong></p>
<p>1. Standard Cauchy distribution(with location parameter 0 and scale parameter 1)
$$η～text{Cauchy}(0; 1);$$
2. Normal mixture distribution, denoted by “Mixture”,
$$η～ 0.8 × N(0,1) + 0.2 × N(0,10^4)$$</p>
<section id="data-loss">
<h2 id="data-loss">Data Loss<a class="headerlink" href="#data-loss" title="Link to this heading">¶</a></h2>
<p>==Data loss==: Model predictions should match training data. Loss over the dataset is an average of loss over examples:</p>
<p>做 data mining 目的是要找到数据的分布，这个时候的数据就是一个大的概念，真正的数据，属于population level，此时的所有数据的统计值包括均值，包括损失函数，都是 <strong>expected 期望的，population 总体的</strong>。但是我们不可能知道真正的数据分布是什么，因为在不知道具体分布情况下，我们只能通过极高极高数量的样本去靠近它，但是 observations 是无穷无尽的，我们几乎不可能靠有限的 observations 去找到数据分布。所以我们只能说我们收集样本，通过有限的样本去观测去 observe，此时我们所得到统计值只是建立在我们所采的样本，是 <a href="#id1"><span class="problematic" id="id2">**</span></a>empirical 经验**的。</p>
<aside class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 24); <em><a href="#id2">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<section id="id3">
<h3 id="id3">结构风险 &amp; 经验风险<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>$$min_fOmega(f)+Csum_{i=1}^nl(f(x_i),y_i)$$</p>
<p>$Omega(f)$, ==结构风险，structural risk==，描述 $f$ 的某些性质。<span class="defi">正则化项</span>
<a href="#id4"><span class="problematic" id="id5">**</span></a>结构经验最小化**可以看作是采用了**最大后验概率估计**的思想来推测模型参数，不仅依赖数据，海依靠模型参数的先验假设。</p>
<aside class="system-message" id="id4">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 31); <em><a href="#id5">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<p>$sumlimits_{i=1}^nl(f(x_i),y_i)$, ==经验风险，empirical risk==，描述 $f$ 与数据的契合程度. = <span class="defi">训练误差 training error</span>
<a href="#id6"><span class="problematic" id="id7">**</span></a>经验风险最小化**可以看作是采用了**极大似然**的参数评估方法，更侧重从数据中学习模型的潜在参数，只看重数据本身。</p>
<aside class="system-message" id="id6">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 34); <em><a href="#id7">backlink</a></em></p>
<p>Inline strong start-string without end-string.</p>
</aside>
<p>$C$，折中</p>
<p>&gt; <a href="#id11"><span class="problematic" id="id12">|实质计算|概率角度|～|</span></a>
&gt; <a href="#id13"><span class="problematic" id="id14">|--|</span></a>–<a href="#id15"><span class="problematic" id="id16">|--|</span></a>
&gt; <a href="#id17"><span class="problematic" id="id18">|最小二乘法 |最大似然估计 MAE|</span></a> $(y-hat{y})^2$|
&gt; <a href="#id19"><span class="problematic" id="id20">|岭回归 |最大后验估计 MAP|引入正则项 $P(w), w^2$|</span></a>
&gt;
&gt; 最大似然法 $xrightarrow{text{奠定}}\xrightarrow{text{概率解释}}$ 最小二乘法
最大后验估计 $xrightarrow{text{奠定}}\xrightarrow{text{概率解释}}$ 岭回归
&gt; (最小二乘法 &amp; 最大似然法),(岭回归 &amp; 最大后验估计) <strong>形式实质相等，实质思想一致，但出发角度不同</strong>
&gt; 最大后验估计是增加了 $p(w)$先验，作为正则项存在。
&gt; <a href="#id21"><span class="problematic" id="id22">|～|最大似然|最大后验估计|</span></a>
&gt; <a href="#id23"><span class="problematic" id="id24">|--|</span></a>–<a href="#id25"><span class="problematic" id="id26">|--|</span></a>
&gt; <a href="#id27"><span class="problematic" id="id28">|目标函数|$P(x\vert w)$|$P(w\vert x)=\cfrac{P(x\vert w)P(w)}{P(x)}$|</span></a>
&gt; <a href="#id29"><span class="problematic" id="id30">|假设|$\epsilon～N(0,\sigma^2)$高斯噪声|$\epsilon～N(0,\sigma_\epsilon^2)$高斯噪声&lt;br&gt;$w～N(0,\sigma_w^2)$高斯先验|</span></a></p>
<p>从结构风险最小化的角度上看，$Omega(f)$ 希望获得具有何种性质的模型(e.g. 复杂性较小 or 引入领域知识 or 加入用户意图)，有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。</p>
<dl class="simple">
<dt>!!! danger “如果在一个完全相同的训练集上训练出五个不同的模型，哪怕他们单个准确率都很高，将它们使用投票集成组合成一个新的分类器，通常也会带来更好的结果。尤其是模型之间非常不同，例如(SVM, DT, LR, …) 效果更优。”</dt><dd><p>!!! question “如果他们是不同的训练实例上完成训练，那就更好了”</p>
</dd>
</dl>
<p>\qquad rightarrow begin{cases}R_{LAD}(f)=mathbb EVert Y-f(X)Vert_1&amp;text{population level}\R_n(f)=frac{1}{n}sumlimits_{i=1}^nVert Y_i-f(X_i)Vert_1&amp;text{empirical risk}end{cases}$</p>
</section>
<section id="loss-functions">
<h3 id="loss-functions">loss functions<a class="headerlink" href="#loss-functions" title="Link to this heading">¶</a></h3>
<p><strong>requirements:</strong> (其实本质上就是预测点和实际真实点的 <strong>distance measure</strong> )</p>
<ol class="arabic simple">
<li><p>symmetric</p></li>
<li><p>non-negative</p></li>
<li><p>identified</p></li>
<li><p>尽可能的 robust</p></li>
</ol>
<p>![](./pics/Loss_3.png){width=80%}</p>
<section id="id8">
<h4 id="id8">0-1<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h4>
<p>$$L_i=I(Y_ineq f(X_i;theta))$$</p>
<p>&lt;figure markdown=”span”&gt;![](./pics/Loss_1.png){width=40%}&lt;p&gt;non-continuous, non-smooth&lt;br&gt;&lt;/p&gt;&lt;/figure&gt;</p>
<p><strong>extremely complicated ! The optimization problem is extremely hard !</strong></p>
</section>
<section id="least-squares-error-lse-l2-loss-conditional-mean">
<h4 id="least-squares-error-lse-l2-loss-conditional-mean">Least Squares Error, LSE, L2-loss —— conditional mean<a class="headerlink" href="#least-squares-error-lse-l2-loss-conditional-mean" title="Link to this heading">¶</a></h4>
<p>$$L_i=(Y_i-f(X_i;theta))^2$$</p>
<p>Regression → Ordinary Least Squares (OLS) according to estimation 分类</p>
<p><strong>Targets</strong>: <strong>conditional mean</strong> $iff f^*(x)=mathbb E(Y|X=x)=minlimits_fmathbb Emathbb{(Y-f(X))^2|X=x}$</p>
<p><strong>Properties:</strong></p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>differentiable and convex</dt><dd><p>Differentiability allows us to take the <strong>derivative</strong> and locate the <strong>minimum</strong> point. Convexity allows us to claim a <strong>global</strong> <strong>minimizer</strong> (also unique if the objective function is strictly convex).</p>
</dd>
</dl>
</li>
<li><p>会更重视 outliers</p></li>
</ol>
<dl>
<dt>!!! danger “LSE fail with contaminated data &lt;br&gt; 数据存在 outliers 的时候就容易不 robust”</dt><dd><ul class="simple">
<li><p>因为平方放大了差别 $text{large}rightarrowtext{large}^2,text{small}rightarrowtext{small}^2$. 和 outlier 相对应的 loss 就会 dominate the empirical risk, 在 regression with outliers 里就会更偏向 approximate the outliers, the fitted curve has been distorted quite significantly.</p></li>
<li><p>LSE 预测出来的 conditional mean，其中 outlier 点有参与计算，（比起 median 来说 mean 是更容易受到 outlier 影响，更不 robust metric）</p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 99)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>&lt;div class=”grid” markdown&gt;
![](./pics/Loss_4.png){width=60%}
![](./pics/Loss_5.png){width=60%}
&lt;/div&gt;</p>
</dd>
</dl>
</section>
<section id="least-absolute-deviation-lad-conditional-median">
<h4 id="least-absolute-deviation-lad-conditional-median">Least Absolute Deviation, LAD —— conditional median<a class="headerlink" href="#least-absolute-deviation-lad-conditional-median" title="Link to this heading">¶</a></h4>
<p>$$Vert Y_i-f(X_i)Vert_1=|Y_i-f(X_i;theta)|$$</p>
<p><strong>Targets</strong>: <strong>conditional median</strong> $iff f^*(x)=text{median}(Y|X=x)=minlimits_fmathbb E{vert Y-f(X)vert: <a href="#id9"><span class="problematic" id="id10">|</span></a>X=x}$</p>
<aside class="system-message" id="id9">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 109); <em><a href="#id10">backlink</a></em></p>
<p>Inline substitution_reference start-string without end-string.</p>
</aside>
<p>![](./pics/Loss_6.jpeg){width=80%}</p>
<ul>
<li><p>proof $f^*(x)=text{median}(Yvert X=x)=minlimits_fmathbb{E}{Vert Y-f(X)Vert_1vert X=x}$
Assume:</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 115)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><ul class="simple">
<li><p>$forall x, mathbb{E}[Y|X=x]ltinfin$(存在)</p></li>
<li><p>$F_{Y|X=x}(cdot):=$ the conditional cdf of $Y|X=x, begin{cases}text{cdf of} -infin=0\text{cdf of} +infin=1\text{cdf of median} =frac{1}{2}end{cases}$</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>$$begin{align*}
mathcal{L}(f)&amp;=mathbb{E}{Vert Y-f(X)Vert_1vert X=x}\
&amp;=int_{-infin}^{f(x)}f(x)-ytext{d}F_{Y|X=x}(y)+int_{f(x)}^{+infin}y-f(x)text{d}F_{Y|X=x}(y)\
cfrac{partialmathcal{L}(f)}{partial f}&amp;=int_{-infin}^{f(x)}1cdottext{d}F_{Y|X=x}(y)+int_{f(x)}^{+infin}-1cdottext{d}F_{Y|X=x}(y)\
&amp;=F_{Y|X=x}(y)Bigvert_{-infin}^{f(x)}-F_{Y|X=x}(y)Bigvert^{+infin}_{f(x)}\
&amp;=F_{Y|X=x}(f(x))-0-1+F_{Y|X=x}(f(x))xlongequal{SET}0\
implies &amp; F_{Y|X=x}(f(x))=cfrac{1}{2}implies f(x)text{ is median}
end{align*}$$</p>
<p><strong>Properties:</strong></p>
<ol class="arabic simple">
<li><p>No amplification 放大. $text{large}rightarrowtext{large},text{small}rightarrowtext{small}$. 在很多数据的情况下， their contributions are less prominent. （如果太多 strong outliers 依旧会failed</p></li>
<li><p><strong>try to downplay the importance of the data point with a large deviation.</strong> 尝试以较大的偏差淡化数据点的重要性。</p></li>
<li><dl class="simple">
<dt>Non-differentiable.</dt><dd><p>exists an alternative approach for solving this problem: using linear programming 单纯形法</p>
</dd>
</dl>
</li>
</ol>
</section>
<section id="check-loss-function-conditional-quantile">
<h4 id="check-loss-function-conditional-quantile">Check Loss function —— conditional quantile<a class="headerlink" href="#check-loss-function-conditional-quantile" title="Link to this heading">¶</a></h4>
<p>!!! p “Which class of data do we care about？ We can trace the Quartile”</p>
<p>$$L_i=rho_τ(a)=(τ-I{a&lt;0})*a=begin{cases}τa&amp;a&gt;0\(τ-1)a&amp;a&lt;0end{cases}$$</p>
<p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/Loss_7.png){width=80%}&lt;/figure&gt;
&lt;figure markdown=”span”&gt;![](./pics/Loss_8.png)&lt;/figure&gt;
&lt;/div&gt;</p>
<p><strong>Targets: conditional median</strong> $iff f^*(x)=τ-text{th quantile of }(Y|X=x)=argmin_fmathbb{Vert Y-f(X)Vert_1｜X=x}$</p>
<p>![](./pics/Loss_9.png)</p>
<ul class="simple">
<li><p>proof:
Assume:
- $forall x, mathbb{E}[Y|X=x]ltinfin$(存在)
- $F_{Y|X=x}(cdot):=$ the conditional cdf of $Y|X=x, begin{cases}text{cdf of} -infin=0\text{cdf of} +infin=1\text{cdf of median} =frac{1}{2}end{cases}$</p></li>
</ul>
<p>$$begin{align*}
mathcal{L}(f)&amp;=mathbb{E}{rho_τcdotVert Y-f(X)Vert_1vert X=x}\
&amp;=int_{-infin}^{f(x)}(τ-1)(y-f(x))text{d}F_{Y|X=x}(y)+int_{f(x)}^{+infin}τ(y-f(x))text{d}F_{Y|X=x}(y)\
cfrac{partialmathcal{L}(f)}{partial f}&amp;=int_{-infin}^{f(x)}(1-τ)cdottext{d}F_{Y|X=x}(y)+int_{f(x)}^{+infin}-τcdottext{d}F_{Y|X=x}(y)\
&amp;=(1-τ)F_{Y|X=x}(y)Bigvert_{-infin}^{f(x)}-τF_{Y|X=x}(y)Bigvert^{+infin}_{f(x)}\
&amp;=(1-τ)(F_{Y|X=x}(f(x))-0)-τ(1-F_{Y|X=x}(f(x)))xlongequal{SET}0\
implies &amp; F_{Y|X=x}(f(x))=τ implies f(x)text{ is τ th quantile of}
end{align*}$$</p>
<blockquote>
<div><p>$F_{Y|X=x}(f^*(x))=τ,forall τin(0,1)implies f^*(x)=F_{Y|X=x}^{-1}(τ)$ <strong>will be the conditional</strong> 𝝉–th quantile of 𝒀|𝑿 = 𝒙</p>
</div></blockquote>
</section>
<section id="hinge-loss">
<h4 id="hinge-loss">Hinge Loss<a class="headerlink" href="#hinge-loss" title="Link to this heading">¶</a></h4>
<p>$$L_i=sumlimits_{j≠y_i}max(0,s_j-s_{y_i}+1)$$</p>
</section>
<section id="squared-hinge-loss">
<h4 id="squared-hinge-loss">Squared Hinge Loss<a class="headerlink" href="#squared-hinge-loss" title="Link to this heading">¶</a></h4>
<p>$$L_i=sumlimits_{j≠y_i}max(0,s_j-s_{y_i}+1)^2$$</p>
</section>
<section id="softmax">
<h4 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Link to this heading">¶</a></h4>
<p>$$L_i=-logBig(cfrac{exp(s_{y_i})}{sumlimits_jexp(s_j)}Big)$$
更关注少见的错误</p>
</section>
</section>
</section>
<section id="regularization-measure-complexity-and-penalize">
<h2 id="regularization-measure-complexity-and-penalize">Regularization - measure complexity and penalize<a class="headerlink" href="#regularization-measure-complexity-and-penalize" title="Link to this heading">¶</a></h2>
<p>Prevent the model from doing <em>too</em> well on training data, control 复杂度</p>
<p>according to problem</p>
</section>
<section id="classification">
<h2 id="classification">Classification<a class="headerlink" href="#classification" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>data: $(X_i,Y_i),i=1,dots,n,X_iinR^p, XinR^{ntimes p}, Y_i$ is categorical</p></li>
<li><p>Classifier: $mathcal{F}={f:f(cdot)in text{dom}(Y)}$</p></li>
</ul>
<p>Y 是类别属性 without numerical meaning，我们只在乎 <strong>whether sample is assigned into the correct label or not</strong>。</p>
<p>回归的时候处理的是误差，所以要最小化，而现在考虑的是联合概率，我们希望概率尽可能大，所以要最大化</p>
<p>&lt;figure markdown=”span”&gt;![](./pics/classi_1.png){width=40%}&lt;p&gt;多分类&lt;br&gt; Adjust the output of neural network&lt;/p&gt;&lt;/figure&gt;</p>
<p><strong>the number of success 错有多少的角度</strong> 。我们先想到的是：Indictor &amp; 0-1 loss</p>
<section id="from-0-1-loss">
<h3 id="from-0-1-loss">From 0-1 loss<a class="headerlink" href="#from-0-1-loss" title="Link to this heading">¶</a></h3>
<p>==Empirical Risk with 0-1 Loss==. with $Y_iin{-1,1}$</p>
<p>$$min_f R(f) =cfrac{1}{n}sumlimits_{i=1}^nI(f(X_i≠Y_i))Leftrightarrowmax_f R(f)=cfrac{{1}}{n}sumlimits_{i=1}^nf(X_i, theta)times Y_i$$</p>
<dl class="simple">
<dt>!!! danger “0-1 loss is non-continuous, non-smooth.”</dt><dd><p>&lt;div class=”grid” markdown&gt;
&lt;figure markdown=”span”&gt;![](./pics/Loss_1.png){width=40%}&lt;p&gt;non-continuous, non-smooth&lt;/p&gt;&lt;/figure&gt;
&lt;p&gt;but we expect: &lt;b&gt;continuous, smooth&lt;/b&gt;&lt;br&gt; 💡 &lt;u&gt;Surrogate Loss function 代理损失函数&lt;/u&gt;。Proper surrogate loss function will lead to a consistent classifier. &lt;/p&gt;
&lt;/div&gt;</p>
</dd>
</dl>
<p>==Surrogate Loss function 代理损失函数==. $L_i=phi(L_i) $, $phi$ is continuous and &lt;u&gt;decreasing&lt;/u&gt;.</p>
<p><strong>properties of</strong> $phi(cdot):$</p>
<ol class="arabic simple">
<li><p>continuous: 能通过梯度求解优化</p></li>
</ol>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\AI/loss.rst</span>, line 221)</p>
<p>Enumerated list ends without a blank line; unexpected unindent.</p>
</aside>
<p>2. decreasing: $f(Χ_i,theta)Y_iuparrowiff phi(f(X_i,theta)times Y_i)downarrow $
$begin{cases}Y_i=+1&amp;xrightarrow{text{force}} f(X_i,theta)&gt;0uparrowimplieshat{Y_i}=+1 \Y_i=-1&amp;xrightarrow{text{force}} f(X_i,theta)&lt;0downarrow implies hat{Y_i}=-1end{cases}$</p>
<p>&lt;div class=”grid” markdown&gt;
<span class="defi">Empirical Risk with 0-1 Loss</span></p>
<p><span class="defi">Empirical Risk with Surrogate Loss Function</span>
&lt;p&gt;$max_f R(f)=cfrac{{1}}{n}sumlimits_{i=1}^nf(X_i, theta)times Y_i$&lt;/p&gt;
&lt;p&gt;$min_f R(f) =cfrac{1}{n}sumlimits_{i=1}^nphi(f(X_i,theta)times Y_i)$&lt;/p&gt;
&lt;/div&gt;</p>
<div class="line-block">
<div class="line-block">
<div class="line">| $phi(cdot)$ | Loss Function |</div>
</div>
<div class="line">— | — | — |</div>
<div class="line">0-1 loss: | $I(cdot)$ | $I(ycdot f(x,theta)&lt;0)$ |</div>
<div class="line">Exponential loss&lt;br&gt;(AdaBoost) | $e^{-(cdot)}$ | $e^{-ycdot f(x,theta)}$ |</div>
<div class="line">Logistic loss  | $log{1+e^{-(cdot)}}$ | $log{1+exp(-ycdot f(x,theta))}$ |</div>
<div class="line">Hinge loss&lt;br&gt;(SVM) | $max{1-(cdot),0}$ | $ max{1-ycdot f(x,theta),0}$ |</div>
</div>
<p>![](./pics/Loss_2.png){width=50%}</p>
</section>
<section id="the-likelihood">
<h3 id="the-likelihood">The Likelihood 似然的角度<a class="headerlink" href="#the-likelihood" title="Link to this heading">¶</a></h3>
<p><span class="defi">The Likelihood Function</span></p>
<p>$small{[P(Y_i=(1,0,…)|X_i=x)]^{I(Y_i=(1,0,…))}timesdotstimes [P(Y_i=(0,…,1)|X_i=x)]^{I(Y_i=(0,…,1))}}\
=prodlimits_{j=1}^{text{#category}}[P(Y_i=j|X=x)]^{I(Y_i=j)}\
=[hat{y_{i1}}]^{I(Y_{i1}=red{1})}times[hat{y_{i2}}]^{I(Y_{i2}=red{1})}timesdotstimes [hat{y_{ij}}]^{I(Y_{ij}=red{1})}timesdots, red{begin{cases}hat{y_i}=(hat{y_{i1}},…,hat{y_{ij}}dots)\hat{y_{ij}}=P(Y_i=j|X=x)\hat{y_{ij}}in[0,1],sumlimits_{j=1}^mhat{y_{ij}}=1end{cases}}\
=[hat{y}_{i1}]^{Y_{i1}}times[hat{y}_{i2}]^{Y_{i2}}timesdotstimes [hat{y}_{ij}]^{Y_{ij}}timesdots,qquad red{Y_{ij}in{0,1}:=X_itext{是不是属于}j类}$</p>
<p>$$L(Y_i|X_i)=prod limits_{j=1}^{text{#category}}[hat{y}_{ij}]^{Y_{ij}}=[hat{y}_{i1}]^{Y_{i1}}times[hat{y}_{i2}]^{Y_{i2}}timesdotstimes [hat{y}_{ij}]^{Y_{ij}}timesdots$$</p>
<p>==Log Likelihood Function==. $l(Y_i|X_i)=log(L(cdot))=sumlimits_{j=1}^{text{#category}}Y_{ij}timeslog[hat{y_{ij}}]\qquad =Y_{i1}log[hat{y}_{i1}]+Y_{i2}log[hat{y}_{i2}]+dots+Y_{ij}log[hat{y}_{ij}]+dots$</p>
</section>
<section id="cross-entropy">
<h3 id="cross-entropy">Cross Entropy<a class="headerlink" href="#cross-entropy" title="Link to this heading">¶</a></h3>
<p>==Cross Entropy Loss==. $text{CELoss}_i =-sumlimits_{j=1}^{text{#category}}Y_{ij}times log hat{y}_{ij}$</p>
<p>==Empirical Risk with Cross Entropy Loss==. $R(f)=frac{1}{text{#sample}} sumlimits_{i=1}^{text{#sample}}Big[-sumlimits_{j=1}^{text{#category}}Y_{ij}times log hat{y_{ij}}Big]=cfrac{1}{red{n}} sumlimits_{i=1}^{red{n}}Big[-sumlimits_{j=1}^{text{red{m}}}Y_{ij}times log hat{y_{ij}}Big]$</p>
<p>$begin{cases}n:=text{#samples},m:=text{#catrgories}\Y_{ij}in{0,1}, hat{y_{ij}}in[0,1],sumlimits_{j=1}^mhat{y_{ij}}=1end{cases}$</p>
<p>首先它是联合概率。概率都是1以下的数，所以像联合概率这种概率乘法的值会越来越小。[插图] 的确如此。如果值太小，编程时会出现精度问题—— <strong>为什么float16会损害正确率</strong></p>
<p>可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。</p>
</section>
</section>
<section id="ref">
<h2 id="ref">📑 ref<a class="headerlink" href="#ref" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>[Understanding the Bias-Variance Tradeoff]</p></li>
<li><p>[偏差（Bias）与方差（Variance）]</p></li>
<li><p>[【深度学习】一文读懂机器学习常用损失函数（Loss Function）]</p></li>
<li><p>[Chapter 7 Regression]</p></li>
</ul>
<p>[偏差（Bias）与方差（Variance）]: <a class="reference external" href="https://zhuanlan.zhihu.com/p/38853908">https://zhuanlan.zhihu.com/p/38853908</a>
[Understanding the Bias-Variance Tradeoff]:<a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a>
[【深度学习】一文读懂机器学习常用损失函数（Loss Function）]:<a class="reference external" href="https://cloud.tencent.com/developer/article/1165263">https://cloud.tencent.com/developer/article/1165263</a>
[Chapter 7 Regression]: <a class="reference external" href="https://probability4datascience.com/ch07.html">https://probability4datascience.com/ch07.html</a></p>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>