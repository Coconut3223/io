<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Data Gathering and Processing &#8212; HomePage</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/css/def.css?v=5a9d86bd" />
    <script src="../_static/documentation_options.js?v=7d86a446"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=6dbb43f8"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#NLP/opennmt" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="HomePage"
           class="md-header-nav__button md-logo">
          
            <i class="md-icon">&#xe869</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Cocobook</span>
          <span class="md-header-nav__topic"> Data Gathering and Processing </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../"versions.json"",
        target_loc = "../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="../index.html" class="md-tabs__link">HomePage</a></li>
            
            <li class="md-tabs__item"><a href="../AI/index.html" class="md-tabs__link">AI</a></li>
            
            <li class="md-tabs__item"><a href="../python/index.html" class="md-tabs__link">Python</a></li>
            
            <li class="md-tabs__item"><a href="index.html" class="md-tabs__link">NLP</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="HomePage" class="md-nav__button md-logo">
      
        <i class="md-icon">&#xe869</i>
      
    </a>
    <a href="../index.html"
       title="HomePage">Cocobook</a>
  </label>
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">AI</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../AI/main.html" class="md-nav__link">Artificial Intelligence, AI</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#nlp-opennmt--page-root" class="md-nav__link">Data Gathering and Processing</a>
        </li>
        <li class="md-nav__item"><a href="#tokenization-sub-wording" class="md-nav__link">Tokenization / Sub-wording</a>
        </li>
        <li class="md-nav__item"><a href="#data-splitting" class="md-nav__link">Data Splitting</a>
        </li>
        <li class="md-nav__item"><a href="#create-the-training-configuration-file" class="md-nav__link">Create the Training Configuration File</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#example" class="md-nav__link">Example</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#training-files" class="md-nav__link">Training files</a>
        </li>
        <li class="md-nav__item"><a href="#filter-out-source-target-longer-than-n-if-filtertoolong-enabled" class="md-nav__link">Filter out source/target longer than n if [filtertoolong] enabled</a>
        </li>
        <li class="md-nav__item"><a href="#tokenization-options" class="md-nav__link">Tokenization options</a>
        </li>
        <li class="md-nav__item"><a href="#where-to-save-the-log-file-and-the-output-models-checkpoints" class="md-nav__link">Where to save the log file and the output models/checkpoints</a>
        </li>
        <li class="md-nav__item"><a href="#checkpoint" class="md-nav__link">Checkpoint</a>
        </li>
        <li class="md-nav__item"><a href="#default-100000-train-the-model-to-max-n-steps" class="md-nav__link">Default: 100000 - Train the model to max n steps</a>
        </li>
        <li class="md-nav__item"><a href="#gpu" class="md-nav__link">GPU</a>
        </li>
        <li class="md-nav__item"><a href="#batching" class="md-nav__link">Batching</a>
        </li>
        <li class="md-nav__item"><a href="#optimization" class="md-nav__link">Optimization</a>
        </li>
        <li class="md-nav__item"><a href="#model" class="md-nav__link">Model</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#content" class="md-nav__link">Content</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#build-vocabulary" class="md-nav__link">Build Vocabulary</a>
        </li>
        <li class="md-nav__item"><a href="#check-gpu" class="md-nav__link">Check GPU</a>
        </li>
        <li class="md-nav__item"><a href="#traning" class="md-nav__link">Traning</a>
        </li>
        <li class="md-nav__item"><a href="#applying" class="md-nav__link">Applying</a>
        </li>
        <li class="md-nav__item"><a href="#evaluation" class="md-nav__link">Evaluation</a>
        </li>
        <li class="md-nav__item"><a href="#id47" class="md-nav__link">model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#ensemble-decoding" class="md-nav__link">Ensemble Decoding</a>
        </li>
        <li class="md-nav__item"><a href="#release-model" class="md-nav__link">Release Model</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#using-pre-trained-nmt-models" class="md-nav__link">Using pre-trained NMT models</a>
        </li>
        <li class="md-nav__item"><a href="#multilingual-neural-machine-translation-mnmt" class="md-nav__link">Multilingual Neural Machine Translation，MNMT</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#tensorboard" class="md-nav__link">tensorboard</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#ref" class="md-nav__link">Ref</a>
        </li>
        <li class="md-nav__item"><a href="#temp" class="md-nav__link">Temp</a>
        </li>
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <p># OpenNMT</p>
<p>==Open-Source Toolkit for Neural Machine Translation==。
<span class="defi">Neural Machine Translation NMT 神经机器翻译</span></p>
<p>The system is successor to <strong>seq2seq-attn</strong> developed at Harvard, and has been completely rewritten for ease of efficiency, readability, and generalizability.
It includes vanilla NMT models along with support for <strong>attention, gating, stacking, input feeding, regularization, beam search and all other options</strong> necessary for state-of-the-art performance.
The project is fully self-contained depending on minimal number of external Lua libraries and including also a simple language independent reversible tokenization and detokenization tools. 完全独立，只有少部分依赖 标记 &amp; 去标记 的东西。</p>
<dl class="simple">
<dt>!!! quote “官方 paper 【Abstract】”</dt><dd><p>We describe an open-source toolkit for neural machine translation (NMT). The toolkit <strong>prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities</strong>, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.
[Open-Source Toolkit for Neural Machine Translation]</p>
</dd>
</dl>
<section id="data-gathering-and-processing">
<h1 id="nlp-opennmt--page-root">Data Gathering and Processing<a class="headerlink" href="#nlp-opennmt--page-root" title="Link to this heading">¶</a></h1>
<ul>
<li><dl>
<dt>包括：至少4个</dt><dd><p>两种语言分为 ① 源 &lt;kbd&gt;src.txt&lt;/kbd&gt; ② 目标 &lt;kbd&gt;tgt.txt&lt;/kbd&gt;；从数据集上：分为训练集和验证集。
每行一个sample，&lt;kbd&gt;src.txt&lt;/kbd&gt; 和 &lt;kbd&gt;tgt.txt&lt;/kbd&gt; 相互对应。Each file has a sentence/segment per line, and it is matching translation in the same line in the other file. This is what the “Moses” file format means.</p>
<div class="admonition- admonition note">
<p class="admonition-title">“多源怎么办？有偏重怎么办？”</p>
<blockquote>
<div><p><a href="#id1"><span class="problematic" id="id2">``</span></a>`` <a href="#id3"><span class="problematic" id="id4">`</span></a>yaml title=”config.yml”</p>
<aside class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 21); <em><a href="#id2">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id3">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 21); <em><a href="#id4">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 22)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<dl>
<dt>data:</dt><dd><blockquote>
<div><dl class="simple">
<dt>corpus_1:</dt><dd><p>path_src: src1_train.txt
path_tgt: tgt1_train.txt
weight: 1</p>
</dd>
<dt>corpus_2:</dt><dd><p>path_src: src2_train.txt
path_tgt: tgt2_train.txt
weight: 6</p>
</dd>
<dt>valid:</dt><dd><p>path_src: src2_valid.txt
path_tgt: tgt2_valid.txt</p>
</dd>
</dl>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 34)</p>
<p>Definition list ends without a blank line; unexpected unindent.</p>
</aside>
<p>…</p>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 35)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a>`` `</p>
<aside class="system-message" id="id5">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 35); <em><a href="#id6">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</dd>
</dl>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt>内容【see ## Tokenization / Sub-wording】</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>token 形式，以&lt;u&gt;空格&lt;/u&gt;分词</dt><dd><ul>
<li><p>本身一个就是最小token形式存在： 英、法：不需要处理</p></li>
<li><p>不以特殊标记分割词的：中：&lt;u&gt;需要分词&lt;/u&gt;</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>!!! P “如果不先分词传进去，也可以指定分词  <code class="docutils literal notranslate"><span class="pre">[tokenizer]</span></code> “</dt><dd><p>而一般用 &lt;u&gt;Sentence Piece&lt;/u&gt; 基于语料训练一个 tokenizer，类似于 jieba，&lt;U&gt;会形成固定大小的词表&lt;/U&gt;。这会影响后来的 &lt;u&gt; <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> &lt;/u&gt;</p>
<blockquote>
<div><dl>
<dt><a href="#id7"><span class="problematic" id="id8">``</span></a>`` <a href="#id9"><span class="problematic" id="id10">`</span></a>yaml title=”config.yml”</dt><dd><aside class="system-message" id="id7">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 45); <em><a href="#id8">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id9">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 45); <em><a href="#id10">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
<dl class="simple">
<dt>data:</dt><dd><dl class="simple">
<dt>corpus_1:</dt><dd><p>path_src: src1_train.txt
path_tgt: tgt1_train.txt
weight: 1
transforms: [tokenizer]</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 52)</p>
<p>Definition list ends without a blank line; unexpected unindent.</p>
</aside>
<p><a href="#id11"><span class="problematic" id="id12">``</span></a>`` `</p>
<aside class="system-message" id="id11">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 52); <em><a href="#id12">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
</dd>
</dl>
</li>
</ul>
</section>
<section id="tokenization-sub-wording">
<h1 id="tokenization-sub-wording">Tokenization / Sub-wording<a class="headerlink" href="#tokenization-sub-wording" title="Link to this heading">¶</a></h1>
<p>However, an MT model can only learn a specific number of vocabulary tokens due to limited hardware resources. &lt;u&gt;To solve this issue, sub-words are used instead of whole words.&lt;/u&gt; At translation time, when the model sees a new word/token that looks like a word/token it has in the vocabulary, it still can try to continue the translation instead of marking this word as “unknown” or “unk”.</p>
<p>因为硬件资源有限，MT model 只能学一定量的词汇。为了让它在碰见新的词汇能继续翻译而不是标记成&lt;kbd&gt;unknown&lt;/kbd&gt; or &lt;kbd&gt;UNK&lt;/kbd&gt;.</p>
<div class="admonition-the-small-units admonition note">
<p class="admonition-title">“最小单元 the small units是什么？”</p>
<p>&gt; &lt;u&gt;English&lt;/u&gt;
▁Hav ing ▁considered ▁the ▁report ▁of ▁the ▁Committee ▁on ▁Conferences ▁for ▁ 2 0 0 6 Official ▁Record s ▁of ▁the ▁General ▁Assembly , ▁S ixty - first ▁Session , ▁Supplement ▁No . ▁ 3 2 ▁( A / 6 1 / 3 2 ). ▁and ▁the ▁relevant ▁reports ▁of ▁the ▁Secretary - General , A / 6 1 / 1 2 9 ▁and ▁Add . 1 ▁and ▁A / 6 1 / 3 0 0 .&lt;/kbd&gt;</p>
</div>
</section>
<section id="data-splitting">
<h1 id="data-splitting">Data Splitting<a class="headerlink" href="#data-splitting" title="Link to this heading">¶</a></h1>
<p>tra, test, valid</p>
</section>
<section id="create-the-training-configuration-file">
<h1 id="create-the-training-configuration-file">Create the Training Configuration File<a class="headerlink" href="#create-the-training-configuration-file" title="Link to this heading">¶</a></h1>
<p>Create the &lt;u&gt;YAML configuration file&lt;/u&gt;. On a regular machine, you can create it manually or with &lt;u&gt;nano&lt;/u&gt;.</p>
<section id="example">
<h2 id="example">Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p><a href="#id13"><span class="problematic" id="id14">``</span></a>`` <a href="#id15"><span class="problematic" id="id16">`</span></a>yaml title=”config.yml”</p>
<aside class="system-message" id="id13">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 79); <em><a href="#id14">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id15">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 79); <em><a href="#id16">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 80)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<p>save_data: run  # Where the samples will be written</p>
<section id="training-files">
<h3 id="training-files">Training files<a class="headerlink" href="#training-files" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>data:</dt><dd><dl class="simple">
<dt>corpus_1:</dt><dd><p>path_src: UN.en-fr.fr-filtered.fr.subword.train
path_tgt: UN.en-fr.en-filtered.en.subword.train
transforms: [filtertoolong]</p>
</dd>
<dt>valid:</dt><dd><p>path_src: UN.en-fr.fr-filtered.fr.subword.dev
path_tgt: UN.en-fr.en-filtered.en.subword.dev
transforms: [filtertoolong]</p>
</dd>
</dl>
</dd>
</dl>
<p>src_vocab: run/source.vocab  # Vocabulary files, generated by onmt_build_vocab
tgt_vocab: run/target.vocab</p>
<p>src_vocab_size: 50000  # Max Vocabulary size
tgt_vocab_size: 50000  # Max Vocabulary size</p>
</section>
<section id="filter-out-source-target-longer-than-n-if-filtertoolong-enabled">
<h3 id="filter-out-source-target-longer-than-n-if-filtertoolong-enabled">Filter out source/target longer than n if [filtertoolong] enabled<a class="headerlink" href="#filter-out-source-target-longer-than-n-if-filtertoolong-enabled" title="Link to this heading">¶</a></h3>
<p>src_seq_length: 150
src_seq_length: 150</p>
</section>
<section id="tokenization-options">
<h3 id="tokenization-options">Tokenization options<a class="headerlink" href="#tokenization-options" title="Link to this heading">¶</a></h3>
<p>src_subword_model: source.model
tgt_subword_model: target.model</p>
</section>
<section id="where-to-save-the-log-file-and-the-output-models-checkpoints">
<h3 id="where-to-save-the-log-file-and-the-output-models-checkpoints">Where to save the log file and the output models/checkpoints<a class="headerlink" href="#where-to-save-the-log-file-and-the-output-models-checkpoints" title="Link to this heading">¶</a></h3>
<p>log_file: train.log
save_model: models/model.fren</p>
</section>
<section id="checkpoint">
<h3 id="checkpoint">Checkpoint<a class="headerlink" href="#checkpoint" title="Link to this heading">¶</a></h3>
<p>save_checkpoint_steps: 1000  # Default: 5000 - Save a model checkpoint for each n
keep_checkpoint: 3  # To save space, limit checkpoints to last n</p>
<p>seed: 3435</p>
</section>
<section id="default-100000-train-the-model-to-max-n-steps">
<h3 id="default-100000-train-the-model-to-max-n-steps">Default: 100000 - Train the model to max n steps<a class="headerlink" href="#default-100000-train-the-model-to-max-n-steps" title="Link to this heading">¶</a></h3>
<p># Increase to 200000 or more for large datasets
For fine-tuning, add up the required steps to the original steps
==================================================================================================================================
train_steps: 3000</p>
<p>valid_steps: 1000 # Default: 10000 - Run validation after n steps</p>
<p>early_stopping: 4  # Stop training if it does not imporve after n validations
report_every: 100</p>
</section>
<section id="gpu">
<h3 id="gpu">GPU<a class="headerlink" href="#gpu" title="Link to this heading">¶</a></h3>
<p>world_size: 1  # Number of GPUs, and
gpu_ranks: [0]  # IDs of GPUs</p>
</section>
<section id="batching">
<h3 id="batching">Batching<a class="headerlink" href="#batching" title="Link to this heading">¶</a></h3>
<p>bucket_size: 262144
num_workers: 0  # Default: 2, set to 0 when RAM out of memory
batch_type: “tokens”
batch_size: 4096   # Tokens per batch, change when CUDA out of memory
valid_batch_size: 2048
max_generator_batches: 2
accum_count: [4]
accum_steps: [0]</p>
</section>
<section id="optimization">
<h3 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Link to this heading">¶</a></h3>
<p>model_dtype: “fp16”
optim: “adam”
learning_rate: 2
warmup_steps: 1000  # Default: 4000 - for large datasets, try up to 8000
decay_method: “noam”
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: “tokens”</p>
</section>
<section id="model">
<h3 id="model">Model<a class="headerlink" href="#model" title="Link to this heading">¶</a></h3>
<p>encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 178)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p><a href="#id17"><span class="problematic" id="id18">``</span></a>`` `</p>
<aside class="system-message" id="id17">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 178); <em><a href="#id18">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
</section>
</section>
<section id="content">
<h2 id="content">Content<a class="headerlink" href="#content" title="Link to this heading">¶</a></h2>
<ul>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">src_vocab_size=50k</span></code>  &amp;  <code class="docutils literal notranslate"><span class="pre">tgt_vocab_size=50k</span></code>  Vocabulary size</dt><dd><ul class="simple">
<li><p>default 50k</p></li>
<li><p>如果使用 &lt;u&gt;Sentence Piece&lt;/u&gt; 得到的tokenizer来分词，这两个 params = size_of_SentencePiece</p></li>
<li><p>但事先分好词进去，就会直接  <code class="docutils literal notranslate"><span class="pre">counters</span></code>  统计。统计的大小看语料的丰富的程度。</p></li>
</ul>
<blockquote>
<div><p><a href="#id19"><span class="problematic" id="id20">``</span></a>`` <a href="#id21"><span class="problematic" id="id22">`</span></a>bash</p>
<aside class="system-message" id="id19">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 188); <em><a href="#id20">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id21">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 188); <em><a href="#id22">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 189)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<p>[2023-12-07 14:23:22,059 INFO] Counters src: 17008
[2023-12-07 14:23:22,060 INFO] Counters tgt: 16939</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 191)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p><a href="#id23"><span class="problematic" id="id24">``</span></a>`` `</p>
<aside class="system-message" id="id23">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 191); <em><a href="#id24">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
</dd>
</dl>
</li>
</ul>
<p>For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_steps</span></code></p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 196)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<dl class="simple">
<dt>for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more!</dt><dd><ul class="simple">
<li><p>&lt;u&gt; <code class="docutils literal notranslate"><span class="pre">early_stopping:</span> <span class="pre">int</span></code> &lt;/u&gt; can help stop the training when there is no considerable improvement.</p></li>
</ul>
</dd>
</dl>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 198)</p>
<p>Definition list ends without a blank line; unexpected unindent.</p>
</aside>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">valid_steps</span></code></p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 199)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>10000 can be good if the value train_steps is big enough.
-  <code class="docutils literal notranslate"><span class="pre">warmup_steps</span></code>
obviously, its value must be less than train_steps. Try 4000 and 8000 values.</p>
</section>
</section>
<section id="build-vocabulary">
<h1 id="build-vocabulary">Build Vocabulary<a class="headerlink" href="#build-vocabulary" title="Link to this heading">¶</a></h1>
<p>对于大的数据集来说，用所有的词进行训练是 not feasible. 所以需要构建小的词库。</p>
<p>Main purpose: To extract a specific set of vocabulary(usually &lt;u&gt;betweeen 32k and 100k &lt;/u&gt;words) from the traning set.</p>
<blockquote>
<div><p><a href="#id25"><span class="problematic" id="id26">``</span></a>`` <a href="#id27"><span class="problematic" id="id28">`</span></a>yaml title=”config.yml”</p>
<aside class="system-message" id="id25">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 210); <em><a href="#id26">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id27">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 210); <em><a href="#id28">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 211)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<p>…
src_vocab: run/src.vocab  # Vocabulary files, generated by onmt_build_vocab
tgt_vocab: run/tgt.vocab</p>
<p>src_vocab_size: 50000  # MAX Vocabulary size
tgt_vocab_size: 50000  # MAX Vocabulary size</p>
<p>src_words_min_frequency: 2 # 單獨運行無意義的感覺
tgt_words_min_frequency: 2</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 220)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p><a href="#id29"><span class="problematic" id="id30">``</span></a>`` `</p>
<aside class="system-message" id="id29">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 220); <em><a href="#id30">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
<p>&lt;kbd&gt;onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2&lt;/kbd&gt;</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-config</span></code>  data…&amp;词典大小和保存位置</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">-n_sample</span></code></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">=-1</span></code>  on <strong>all</strong> the segment in the training dataset</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-num_threads</span></code> : change it to match the number of CPUs to run it faster</p></li>
</ul>
<p>&gt;  <a href="#id31"><span class="problematic" id="id32">``</span></a>`` ` bash
&gt; [INFO] Counter vocab from -1 samples.
&gt; [INFO] n_sample=-1: Build vocab on <strong>full</strong> datasets.
&gt; [INFO] * Transform statistics for corpus_1(50.00%):
&gt; * FilterTooLongStats(filtered=2138)
&gt; [INFO] * Transform statistics for corpus_1(50.00%):
&gt; * FilterTooLongStats(filtered=2032)
&gt; [INFO] Counters src:14705
&gt; [INFO] Counters tgt:11884
&gt;  <a href="#id33"><span class="problematic" id="id34">``</span></a>`` `</p>
<aside class="system-message" id="id31">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 229); <em><a href="#id32">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id33">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 229); <em><a href="#id34">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</section>
<section id="check-gpu">
<h1 id="check-gpu">Check GPU<a class="headerlink" href="#check-gpu" title="Link to this heading">¶</a></h1>
<p>&lt;kbd&gt;nvidia-smi -L&lt;/kbd&gt; Check if the GPU is active</p>
<blockquote>
<div><p><a href="#id35"><span class="problematic" id="id36">``</span></a>`` <a href="#id37"><span class="problematic" id="id38">`</span></a>python</p>
<aside class="system-message" id="id35">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 245); <em><a href="#id36">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id37">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 245); <em><a href="#id38">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
</div></blockquote>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 246)</p>
<p>Block quote ends without a blank line; unexpected unindent.</p>
</aside>
<p>import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))</p>
<p>gpu_memory = torch.cuda.mem_get_info(0)
print(“Free GPU memory:”, gpu_memory[0]/1024**2, “out of:”, gpu_memory[1]/1024**2)
&gt;&gt;&gt; True
==================
# &gt;&gt;&gt; Tesla T4
&gt;&gt;&gt; Free GPU memory: 15007.75 out of: 15109.75
==============================================================================================</p>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 257)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p><a href="#id39"><span class="problematic" id="id40">``</span></a>`` `</p>
<aside class="system-message" id="id39">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 257); <em><a href="#id40">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
</section>
<section id="traning">
<h1 id="traning">Traning<a class="headerlink" href="#traning" title="Link to this heading">¶</a></h1>
<ol class="arabic">
<li><p>remove existed model: &lt;kbd&gt;rm -rf drive/MyDrive/nmt/models/&lt;/kbd&gt;</p></li>
<li><dl>
<dt>Train the NMT model: &lt;kbd&gt;onmt_train -config config.yaml&lt;/kbd&gt;</dt><dd><ul>
<li><p>If the traning stopped, and we want to continue it from a specific checkpoint: &lt;kbd&gt;onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt&lt;/kbd&gt;</p>
<blockquote>
<div><p><a href="#id41"><span class="problematic" id="id42">``</span></a>`` <a href="#id43"><span class="problematic" id="id44">`</span></a>yaml title=”config.yml”
train_steps # &gt; train_from 的数字</p>
<aside class="system-message" id="id41">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 266); <em><a href="#id42">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
<aside class="system-message" id="id43">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 266); <em><a href="#id44">backlink</a></em></p>
<p>Inline interpreted text or phrase reference start-string without end-string.</p>
</aside>
<aside class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 268)</p>
<p>Unexpected indentation.</p>
</aside>
<blockquote>
<div><p><a href="#id45"><span class="problematic" id="id46">``</span></a>`` `</p>
<aside class="system-message" id="id45">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 268); <em><a href="#id46">backlink</a></em></p>
<p>Inline literal start-string without end-string.</p>
</aside>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
</ol>
<ul class="simple">
<li><p>debug mode: &lt;kbd&gt;dmesg -T&lt;/kbd&gt;</p></li>
</ul>
</section>
<section id="applying">
<h1 id="applying">Applying<a class="headerlink" href="#applying" title="Link to this heading">¶</a></h1>
<p>Translation Options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-model</span></code> : model(s) used</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-src</span></code> : source file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-output</span></code> : filename to write result</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">-gpu</span></code><span class="classifier">GPU ID</span></dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">=0</span></code> : 1 GPU</p></li>
<li><p>: CPU</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-min_length[optional]</span></code> :  to avoid empty translations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-verbose[optional]</span></code> : if you want to print translations</p></li>
</ul>
<p>&gt; 1. &lt;kbd&gt;onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1&lt;/kbd&gt;
&gt; 2. Check the first line of result &lt;kbd&gt;head -n 1 UN.en.translated&lt;/kbd&gt;
&gt;
&gt;
&gt; &gt; &lt;u&gt; Using sub-word&lt;/u&gt;
&gt; &gt; ▁Recalling ▁its ▁relevant ▁resolutions , ▁including ▁resolution ▁ 5 8 / 2 9 2 ▁of ▁ 6 ▁May ▁ 2 0 0 4 , ▁as ▁well ▁as ▁th ose ▁adopted ▁at ▁its ▁tenth ▁emergency ▁special ▁session ,
&gt; &gt; &lt;u&gt;Using word&lt;/u&gt;
&gt; &gt; Recalling its relevant resolutions, including resolution 58/292 of 6 May 2004, as well as those adopted at its tenth emergency special session,</p>
</section>
<section id="evaluation">
<h1 id="evaluation">Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">¶</a></h1>
</section>
<section id="id47">
<h1 id="id47">model<a class="headerlink" href="#id47" title="Link to this heading">¶</a></h1>
<section id="ensemble-decoding">
<h2 id="ensemble-decoding">Ensemble Decoding<a class="headerlink" href="#ensemble-decoding" title="Link to this heading">¶</a></h2>
<p>During translation, instead of adding one model/checkpoint to the -model argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?</p>
<ul class="simple">
<li><p><strong>Averaging Models</strong></p></li>
</ul>
<aside class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">C:\Users\zxouyang\CodeProjects\PRIVATE_P\io\docs\source\NLP/opennmt.rst</span>, line 307)</p>
<p>Bullet list ends without a blank line; unexpected unindent.</p>
</aside>
<p>&lt;kbd&gt;python3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt&lt;/kbd&gt;
Average multiple models into one model using the average_models.py script, and see how this affects performance.</p>
</section>
<section id="release-model">
<h2 id="release-model">Release Model<a class="headerlink" href="#release-model" title="Link to this heading">¶</a></h2>
<p>see how it reduce the model size.</p>
<p>&lt;kbd&gt;onmt_release_model –model model.pt –output model_released.pt&lt;/kbd&gt;</p>
</section>
</section>
<section id="using-pre-trained-nmt-models">
<h1 id="using-pre-trained-nmt-models">Using pre-trained NMT models<a class="headerlink" href="#using-pre-trained-nmt-models" title="Link to this heading">¶</a></h1>
<p>For low-resource languages(up to 15m), using directly or fine-tuning mBART can give better results.
For high-resource languages, training a baseline model from scratch can outperform mBART.
Then, applying mixed fine-tuning (Chu et al., 2017) on this new baseline using in-house data can even achieve better gains in terms of Machine Translation quality. Check this code snippet if you would like to try mBART. You can also convert M2M-100 model to the CTranslate2 format for better efficiency as explained here.</p>
</section>
<section id="multilingual-neural-machine-translation-mnmt">
<h1 id="multilingual-neural-machine-translation-mnmt">Multilingual Neural Machine Translation，MNMT<a class="headerlink" href="#multilingual-neural-machine-translation-mnmt" title="Link to this heading">¶</a></h1>
<p><strong>Advantages：</strong></p>
<ol class="arabic simple">
<li><p>help a &lt;u&gt;low-resource&lt;/u&gt; language acquire extra knowledge from other languages</p></li>
<li><p>tend to generalize better due to exposure to diverse languages. This particular phenomenon is known as <strong>translation Transfer Learning or Knowledge Transfer</strong> (Dabre et al., 2020).</p></li>
<li><p>Languages that do not share the same alphabet cannot achieve the same linguistic benefits from a multilingual NMT model. Still, researchers investigate approaches like transliteration to increase knowledge transfer between languages that belong to the same language family, but use different alphabets. For example, using this transliteration trick, my Indic-to-English multilingual NMT model can translate from 10 Indic languages to English.</p></li>
</ol>
<p><strong>Notes：</strong></p>
<ol class="arabic simple">
<li><p>&lt;u&gt;shuffle&lt;/u&gt; dataset</p></li>
<li><dl class="simple">
<dt>check banlanced -&gt; &lt;u&gt;over-sampling&lt;/u&gt;</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>giving <strong>weights</strong> to datasets</dt><dd><p>&gt; En: 10 million, zh: 2 million  <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em;"></span></span></span></span></span>  weight of En = 1, weight of zh = 5</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>&lt;u&gt;[Optional]&lt;/u&gt; add a special token to the start of each sentence. In this case, you will have to add these tokens to your SentencePiece model through the option  <code class="docutils literal notranslate"><span class="pre">--user_defined_symbols</span></code> . However, some researchers believe this step is optional.</dt><dd><p>&gt; En:  <code class="docutils literal notranslate"><span class="pre">&lt;en&gt;</span></code> , zh:  <code class="docutils literal notranslate"><span class="pre">&lt;zh&gt;</span></code></p>
</dd>
</dl>
</li>
<li><p>Integrating other data augmentation approaches like Back-Translation can still be useful.</p></li>
<li><dl class="simple">
<dt>pre-trained NMT models</dt><dd><p>For low-resource languages(up to 15m), using directly or fine-tuning mBART can give better results.
For high-resource languages, training a baseline model from scratch can outperform mBART.
Then, applying &lt;u&gt;mixed fine-tuning&lt;/u&gt; (Chu et al., 2017) on this new baseline using in-house data can even achieve better gains in terms of Machine Translation quality.</p>
</dd>
</dl>
</li>
</ol>
<p>[Notes on Multilingual Machine Translation]</p>
<p>You can also convert M2M-100 model to the CTranslate2 format for better efficiency as explained here.</p>
<section id="tensorboard">
<h2 id="tensorboard">tensorboard<a class="headerlink" href="#tensorboard" title="Link to this heading">¶</a></h2>
<p>[Running TensorBoard](<a class="reference external" href="https://forum.opennmt.net/t/running-tensorboard/4242">https://forum.opennmt.net/t/running-tensorboard/4242</a>)</p>
</section>
</section>
<section id="ref">
<h1 id="ref">Ref<a class="headerlink" href="#ref" title="Link to this heading">¶</a></h1>
<blockquote>
<div><p>[神经机器翻译（NMT）的一些重要资源分享](<a class="reference external" href="https://zhuanlan.zhihu.com/p/29338282">https://zhuanlan.zhihu.com/p/29338282</a>)
[OpenNMT-py Tutorial](<a class="reference external" href="https://github.com/ymoslem/OpenNMT-Tutorial/tree/main">https://github.com/ymoslem/OpenNMT-Tutorial/tree/main</a>)</p>
</div></blockquote>
<p>[Notes on Multilingual Machine Translation]: <a class="reference external" href="https://blog.machinetranslation.io/multilingual-nmt/">https://blog.machinetranslation.io/multilingual-nmt/</a></p>
<p>[](<a class="reference external" href="https://leonis.cc/sui-sui-nian/2022-12-16-opennmt-tutorial-quickstart.html">https://leonis.cc/sui-sui-nian/2022-12-16-opennmt-tutorial-quickstart.html</a>)</p>
<p>[Open-Source Toolkit for Neural Machine Translation]:<a class="reference external" href="https://aclanthology.org/P17-4012.pdf">https://aclanthology.org/P17-4012.pdf</a></p>
</section>
<section id="temp">
<h1 id="temp">Temp<a class="headerlink" href="#temp" title="Link to this heading">¶</a></h1>
<p>[How is Accuracy calculated ?](<a class="reference external" href="https://github.com/OpenNMT/OpenNMT-py/issues/1944">https://github.com/OpenNMT/OpenNMT-py/issues/1944</a>)</p>
<p>[Discrepency between training and tranlsation](<a class="reference external" href="https://forum.opennmt.net/t/discrepency-between-training-and-tranlsation/4765">https://forum.opennmt.net/t/discrepency-between-training-and-tranlsation/4765</a>)</p>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, coconut.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>